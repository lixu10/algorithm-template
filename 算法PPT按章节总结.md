算法复习-PPT按照章节总结。一级标题是PPT上的编号。

提示词：

> ```markdown
> 我看不懂英文，帮我详细总结这章PPT讲解的内容，用中文，让我看完你的讲解就能知道这章全部内容，公式请使用latex格式，用$或者$$包裹住公式
> ```

注：部分章节的公式可能不是latex格式，但应该都不太影响理解（大部分我自己都看过一遍）

# a01-1

这章其实**没有教很多具体大算法的细节**，主要是在回答三个问题：

1. 什么是“算法”？
2. 算法能解决哪些典型问题？
3. 同一个问题，不同算法差别有多大，为什么“好算法”这么重要？ 

我按“**提到了哪些算法 / 问题 → 问题是啥 → 常见解法怎么做**”的顺序给你讲一遍，只讲算法思路，不讲复杂度公式。

------

## 一、斐波那契数列：递归算法 vs 循环算法

PPT 一开始用 Fibonacci 数列来说明“什么叫算法”：
 (F(1)=1, F(2)=1, F(n)=F(n-1)+F(n-2))。

### 1. 问题是啥？

给你一个正整数 n，要求出第 n 个 Fibonacci 数。比如：

- n=1 → 1
- n=2 → 1
- n=3 → 2
- n=4 → 3
- n=5 → 5
- n=6 → 8
- …… 前两个是 1，从第三个开始都是“前两个数之和”。

------

### 2. 算法 A1：递归算法（照着数学公式写）

伪代码大概是这样的（PPT 上就是这个意思）：

```c
f(n) {
    if (n <= 2)
        return 1;
    else
        return f(n-1) + f(n-2);
}
```

**思路：**

- 如果要求的是第 1 个或第 2 个数，答案已经明说是 1，直接返回 1；
- 否则，第 n 个数 = 第 n-1 个数 + 第 n-2 个数，
- 所以我只要去“递归地”调用自己，分别算出 f(n-1)、f(n-2)，再相加就行了。

**举个 n=5 的执行过程：**

- f(5) = f(4) + f(3)
- f(4) = f(3) + f(2)
- f(3) = f(2) + f(1)

把这些展开，会发现很多重复计算（比如 f(3) 会被算很多次），这也是老师想说明的——**算法能算出来，但可能很“笨”**。

------

### 3. 算法 A2：非递归（循环）算法

PPT 上给出另一个写法：先保存前两个数，然后用 for 循环往后推。

```c
f1 = 1, f2 = 1;
for (i = 3; i <= n; i++) {
    f = f1 + f2;
    f2 = f1;
    f1 = f;
}
```

**思路：**

- 一开始：`f1` 代表 F(2)，`f2` 代表 F(1)，都是 1；
- 从第 3 个数开始，循环：
  - 当前新数 f = f1 + f2（也就是“前两个数之和”）
  - 然后往前挪：`f2 ← f1`，`f1 ← f`
- 循环结束时，`f1` 就是第 n 个 Fibonacci 数。

**举个 n=5 的过程：**

- 初始：f2=1 (F1), f1=1 (F2)
- i=3：f=1+1=2 → f2=1, f1=2 (F3)
- i=4：f=2+1=3 → f2=2, f1=3 (F4)
- i=5：f=3+2=5 → f2=3, f1=5 (F5)

最后得到 F(5)=5。

> 这一对 A1 / A2 就是在告诉你：**同一个问题可以有很多种算法，有的写法优雅但重复计算多，有的写法更“工程化”、更省事。**

------

## 二、排序算法：冒泡/选择、插入、归并

### 1. 问题：排序问题的形式化描述

PPT 用“排序”来说明“算法是解决计算问题的工具”。

- **输入**：n 个数，写成一个序列，例如 `<31, 41, 59, 26, 41, 58>`
- **输出**：把这 n 个数按“非递减”（从小到大，可以有相等）排好，如
   `<26, 31, 41, 41, 58, 59>`

它没有详细展开所有排序算法，只是举了几个常见的：

- 冒泡（选择）排序作为“可计算过程”的例子；
- 后面又拿**插入排序**和**归并排序**对比，说明不同算法效率差异很大。

------

### 2. 冒泡/选择排序：最直观的排序

PPT 只是提到“冒泡（选择）排序是可计算过程”，没有写细节，我给你用生活话简要描述一下过程。

**冒泡排序（Bubble Sort）核心思想：**

- 不断“比较相邻的两个数，如果顺序错了就交换它们”，
- 一趟一趟往后扫，每一趟都会把一个“最大”的元素送到序列末尾，就像气泡往上冒一样。

例子：排序 `[5, 2, 4, 1]`

- 第 1 趟：
  - 比较 5 和 2 → 交换 → `[2, 5, 4, 1]`
  - 比较 5 和 4 → 交换 → `[2, 4, 5, 1]`
  - 比较 5 和 1 → 交换 → `[2, 4, 1, 5]`（5已经在最后）
- 第 2 趟：
  - 比较 2 和 4 → 不换
  - 比较 4 和 1 → 换 → `[2, 1, 4, 5]`
- 第 3 趟：
  - 比较 2 和 1 → 换 → `[1, 2, 4, 5]`

就排好了。

**选择排序（Selection Sort）核心思想：**

- 每一轮在“剩下的元素”里找一个最小的，放到当前最前面的位置。

同样是 `[5, 2, 4, 1]`：

- 第 1 轮：在全体 `{5,2,4,1}` 中找最小是 1 → 交换到第 1 个 → `[1,2,4,5]`
- 第 2 轮：在后面 `{2,4,5}` 中找最小 2 → 已在正确位置
- 后面继续即可。

这两种算法都很容易理解、很适合入门，但当数据量很大时，效率不太好，所以只是“起点”。

------

### 3. 插入排序（Insertion Sort）：像摸牌一样

PPT 在讲“好算法 vs 差算法”时，用的是“插入排序”对比“归并排序”。

**插入排序的直观类比：**

- 打牌时，你一张一张摸牌，每摸到一张新牌，就把它插入到你手里已有的有序牌堆的正确位置上。
- 序列前半部分始终保持有序，新的元素不断插入进去。

例子：`[5, 2, 4, 1]`

- 已排序部分：`[5]`
- 读到 2，把 2 插到 5 前面 → `[2, 5]`
- 读到 4，把 4 插到 2 和 5 中间 → `[2, 4, 5]`
- 读到 1，把 1 插到最前面 → `[1, 2, 4, 5]`

插入排序在“基本有序”的情况下还挺好用，但数据特别多时，还是会比较慢。

------

### 4. 归并排序（Merge Sort）：分治+合并

**PPT 重点：**它用一个例子说明：

- 在同样数据量下，如果用某种简单算法（比如插入排序），一个快电脑要算很多小时；
- 换成“更聪明”的算法（比如归并排序），就算电脑慢一些，也能很快算完。

**归并排序的基本思路：**

1. **分**：不断把数组一分为二，直到每一块只有 1 个元素（1 个元素本身必然是有序的）。
2. **治**：把两个有序的小数组“合并”成一个更大的有序数组。
3. 最终整体变成有序。

举个简化过程：`[5, 2, 4, 1]`

- 分： `[5,2,4,1]` → `[5,2]` 和 `[4,1]`
- 再分：`[5,2]` → `[5]` `[2]`；`[4,1]` → `[4]` `[1]`
- 合并：
  - `[5]` 和 `[2]` 合并 → `[2,5]`
  - `[4]` 和 `[1]` 合并 → `[1,4]`
- 再合并 `[2,5]` 和 `[1,4]` → `[1,2,4,5]`

**重点不是公式，而是理念：**

> 先把大问题拆成小问题（分治），小问题容易解决，再把解决好的小块拼起来。

------

## 三、路径和最短路问题：地图导航

在“算法能解决什么问题”那里，PPT 列了很多例子：基因组、互联网路由、搜索引擎、电子商务加密、制造业选址等等，其中一个特别明显的是：

> “从一个顶点到另一个顶点的最短路径” & 地图中“finding good routes”。

### 1. 问题是啥？

典型问题：

- 给你一张地图，标出所有路口（点）和公路（边），每条边有一个“长度”或“耗时”；
- 给定起点 A 和终点 B，**找一条总长度（或总时间）最短的路线**。

以及 PPT 中老师贴的两个例子：

- “新配送经理要画出一条最快的配送路径”（其实是多点路线问题）。
- 顺丰快递从“北三环中路 66 号 → 北航”的实际路线比他想象的路线绕了很多圈。

------

### 2. 典型解法：最短路算法（以 Dijkstra 为代表）

PPT 没写算法名字，我用你以后很可能会学到的**Dijkstra 算法**来说明最短路是怎么做的（只讲直观思路）：

1. 把每个路口看成一个“点”，路看成“边”，标上长度（距离或时间）；
2. 一开始只知道起点 A 到自己的距离是 0，别的点都是“未知的，很大”；
3. 每一步：
   - 在“尚未确定”的点中，找出目前看起来距离 A **最近**的那个点 P；
   - 把 P 认为是“已经找到最短路”；
   - 用 P 去尝试更新它的邻居：
     - 如果 “A → P → 邻居 Q” 这条路比之前知道的 A→Q 更短，就用这条新路线替换旧路线；
4. 重复上面的步骤，直到把终点 B“确定”下来，或所有点都处理完。

**直观理解：**

- 起点的“影响力”像水波一样一步步向外扩散；
- 每次都先“固定”现在最近的那个点，然后通过它去“拉近”其他点的距离；
- 最终，起点到每个点的最短路线都被计算出来。

日常导航软件（高德、百度、Google Maps）在路况不太复杂的情况下，大体都是用类似的最短路算法，再加上实时路况等因素。

------

## 四、旅行商问题（TSP）：典型“难问题”

### 1. 问题是啥？

1.1.4 节“Hard problems”里，老师讲了个**新经理**的故事：老板让他为 20 个客户安排一条“最快配送路径”。并写了个 20! ≈ 2.4×10¹⁸ 的数字，说明“拍脑袋枚举所有可能路线是完全不现实的”。

这个问题就是经典的 **Traveling Salesman Problem（旅行商问题，TSP）**：

> 有 n 个城市，一个业务员要从其中一个城市出发，
>  每个城市恰好访问一次，最后回到起点，
>  要求总路程尽量短。
>  问：应该按什么顺序走？

------

### 2. 朴素解法：全部枚举（只适合极小规模）

最直接的想法：

1. 列出所有“访问顺序”的排列（比如 5 个城市时有 5! = 120 种走法）；
2. 对每一种顺序，计算总路线长度；
3. 取总长度最短的那一个。

小规模（比如 n≤10）还能算，但 n=20 时就有 20! ≈ 2.4×10¹⁸ 种可能，哪怕一秒钟算很多条也算不过来。PPT 就是用这个例子告诉你：**有些问题，用最直接的算法根本解决不了现实规模，只能认它为“难问题”。**

------

### 3. 实际中的“近似解法”：贪心/启发式

PPT 没展开讲，只说这种问题属于 NP 完全问题的一类代表，“目前不知道有没有真正高效的解法”。

但现实里物流公司、快递公司总要跑车对吧？常见做法是用各种 **启发式算法**，求一个“足够好但不一定最优”的路线，比如：

**最邻近贪心法：**

1. 从仓库出发；
2. 每一步都去“距离当前最近的、还没送过的客户”；
3. 直到所有客户都送完，再回仓库。

它不保证一定是最短路线，但在很多场景下效果还不错，而且算得飞快，所以现实系统经常用此类思路，再加上一些优化（比如局部交换两条边、重新规划某一段路线等）。

------

## 五、整数分解问题：大的数拆成质因数

在“Hard problems”的列表里还出现了一个：

> Factorization：把一个大的合数 M 分解成两个质数 p、q，使得 M = p × q。

### 1. 问题是啥？

给你一个很大的整数 M（比如几百上千位），已知它是两个质数的乘积：
 (M = p \times q)，问你：p 和 q 是多少？

这个问题和**密码学（RSA 加密）**联系很紧密：

- 乘法很快（给你 p,q，很快能算出 M）；
- 但从 M 反过来找 p,q 却非常难，这就是安全性所在。

------

### 2. 最简单的解法：试除法

**思路：**

1. 从 2 开始尝试去整除 M；
2. 如果 k 能整除 M，就记录下一个因子：p = k，q = M / k；
3. 否则 k 增加 1 继续试；
4. 实际上，只需要试到 (\sqrt{M}) 就够了（因为如果 M = a×b，至少有一个 ≤ √M）。

例如：M = 77

- k=2 → 77 % 2 ≠ 0
- k=3 → 77 % 3 ≠ 0
- k=4,5 → 都不行
- k=7 → 77 % 7 = 0 → 找到 p=7, q=11

小数可以这样玩，但 M 很大时，这种方法会慢到没法用。现实中有各种更加复杂的分解算法，但课件这里只是用它来说明——**有些问题看起来简单，找“好算法”却非常困难。**

------

## 六、矩阵乘法：三重循环的经典算法

在“算法能解决的问题”那里，PPT 还提到了“矩阵相乘（Matrices product）”。

### 1. 问题是啥？

给定两个矩阵 A（m×k）和 B（k×n），求它们的乘积 C（m×n）。C 的每个元素为：

> C[i][j] = 第 i 行（A）和第 j 列（B）对应元素相乘再相加的结果。

### 2. 标准算法：三层循环

用伪代码描述（只讲逻辑）：

```pseudo
for i in 行:
    for j in 列:
        C[i][j] = 0
        for t in 1..k:
            C[i][j] += A[i][t] * B[t][j]
```

也就是：

- 外面两层：枚举 C 的每一个位置 (i,j)；
- 里面一层：把 A 的第 i 行 和 B 的第 j 列 做“点积”（一一相乘再相加），填入 C[i][j]。

这是你在线性代数 + 程序设计里最常遇见的标准解法。

------

## 七、本章想传达的核心观点（和算法相关的那一半）

虽然你说不想听复杂度分析，但**老师用好多例子只是在强调一个思想**：

> - 同一个问题，用不同的算法，耗时差别可以是“几分钟 vs 几十个小时”；
> - 有些问题（TSP、整数分解）目前根本不知道有没有真正快的算法，只能承认它们“很难”；
> - 算法并不是“虚的理论”，而是和地图导航、快递路径、加密解密、编译器、网络路由等现实系统紧密相关。

所以这一章里出现的主要算法 / 问题可以总结为：

1. **Fibonacci 数列**：递归写法 vs 循环写法；
2. **排序问题**
   - 冒泡 / 选择排序：最基础的“比较 + 交换 / 选择”；
   - 插入排序：像摸牌一样插入；
   - 归并排序：分治 + 合并；
3. **最短路 / 地图导航**：以 Dijkstra 类算法为代表的路径规划；
4. **旅行商问题（配送路径）**：全部枚举不现实，只能用启发式（贪心等）求近似解；
5. **整数分解（M = p×q）**：小规模可以用试除法，大规模是著名难题；
6. **矩阵乘法**：三重循环按行列相乘求和。

如果你愿意，我可以下一步**挑一两个你最关心的问题**（比如“最短路”或“旅行商问题”），用更形象的例子，给你画步骤图或写中文伪代码，一步一步带你“跑一遍算法”。



# a01-2

这一章其实就围绕**一个主角算法：插入排序（Insertion Sort）**，用它来示范：

- 怎么用伪代码准确描述一个算法
- 怎么说明“它一定能排对”（正确性，循环不变量）
- 不同输入时，算法会“干多少活”（最好情况、最坏情况）

你要的重点是“讲算法和典型案例”，那我就按这个思路来讲，不细抠复杂度公式。

------

## 一、这一章在讲什么？

PPT 目录：

- **2.1 插入排序：框架——如何描述算法**
- **2.1.2 用循环不变量证明插入排序是对的**
- **2.2 算法分析：以插入排序为例**（最好 / 最坏情况）
- 后面再简要说“增长率”“练习”和“伪代码 → C 代码的对应关系”。

核心就是：

> 用“给一串数排序”这个问题 + 插入排序这个算法，
>  把“描述—证明—分析”三件事串起来。

------

## 二、插入排序到底在做什么？

### 1. 要解决的问题是啥？

问题：

> 把一个长度为 n 的数列排成**从小到大**（非递减）的顺序。

- **输入**：<a₁, a₂, …, aₙ>
- **输出**：<a₁′, a₂′, …, aₙ′>，满足
   a₁′ ≤ a₂′ ≤ … ≤ aₙ′（只是重新排列原来的这些元素）。

### 2. 生活类比：摸扑克牌时的“整理手牌”

右上角那几张图和手里抓牌的图片就是想告诉你：

- 你左手已经有一些牌，并且它们从小到大排好；
- 右手再拿一张新牌，把它插入到左手牌中**合适的位置**；
- 重复这个动作，最后所有牌都排好了。

> 插入排序就是把数组当成“牌堆”，
>  一张一张“摸过来插进去”。

------

## 三、插入排序伪代码 + 逐行解释

PPT 上给出伪代码（已经写成接近 C 的风格）：

```c
INSERTION-SORT(A)
1 for (j = 2; j <= length[A]; j++)
2 {
3     key = A[j]
4     // 把 A[j] 插入到已经排好序的 A[1..j-1] 中
5     i = j - 1
6     while (i > 0 && A[i] > key)
7     {
8         A[i+1] = A[i]   // A[i] 比 key 大，往后挪
9         i = i - 1       // 接着看前一个元素
10    }
11    A[i+1] = key        // 把 key 放到正确的位置
12 }
```

逐行用白话说一遍（假设数组下标从 1 开始）：

1. **外层 for 循环**：

   - `j` 从 2 到 n（`length[A]`）。
   - 意思是：从第二个元素开始，**轮到 A[j] 这张“新牌”要插入**。

2. 第 3 行：`key = A[j]`

   - 把当前要插入的这张“牌”记下来，叫 `key`。

3. 第 5 行：`i = j - 1`

   - 从 `key` 左边那个位置开始往前看（也就是已经处理好的那一段）。

4. 第 6～9 行：while 循环

   - 条件：`i > 0 && A[i] > key`

     - 还没有越界（`i > 0`），
     - 并且当前元素 A[i] **比 key 大**。

   - 循环体做两件事：

     - 把 A[i] 向右挪一格（复制到 A[i+1]），
     - `i--`，继续往前看前一个元素。

   - 这一步就像：

     > 从右往左扫，凡是比新牌大的通通往右挪，为 key 腾出一个合适的位置。

5. 第 11 行：`A[i+1] = key`

   - while 结束时有两种可能：
     1. 走到最前面（`i == 0`），说明 key 是目前最小的；
     2. 遇到一个不大于 key 的元素（`A[i] <= key`），说明 key 应该插在它后面。
   - 所以把 key 放在 `i+1` 就是**刚刚好的位置**。

整个过程配合 PPT 那几幅小数组图（5,2,4,6,1,3）你可以自己模拟几轮，会特别直观：

- 每次 `j` 往右走一步，就多“摸”一张牌；
- 左边那截始终保持有序。

------

## 四、为什么这个算法一定能排好？——循环不变量（直觉版）

这一节的标题是：**Loop invariants and correctness of insertion sort**。

它定义了一个“循环不变量”（loop invariant）：

> 在 for 循环每一轮开始时：
>  **子数组 A[1..j-1] 正好是原数组前 j-1 个元素，但已经是排好序的。**

你可以把它理解成一句要“始终保持为真”的话。

正确性的证明分三步（和数学归纳法特别像）：

### 1. 初始化（Initialization）

- 在第一轮循环前，`j = 2`。
- A[1..j-1] = A[1..1]，只包含第一个元素。
- 一个元素“天然有序”，所以此时“不变量成立”。

### 2. 维持（Maintenance）

假设第 j 轮开始前，不变量是真的：

> A[1..j-1] 已经有序。

那这一轮我们要做的，就是**把 A[j] 插入到这段有序序列中**：

- while 循环只是在“往右挪大于 key 的元素”；
- 其他元素保持不变；
- 最后把 key 插入到中间某个位置。

这样一来，A[1..j] 还是那 j 个原始元素，只是重新排了顺序，而且保持有序。
 所以第 j 轮结束时，**下一轮开始前，A[1..(j+1)-1] 仍然有序**——不变量被保持下来了。

### 3. 终止（Termination）

for 循环结束时，`j = n+1`，循环条件失败跳出。
 此时根据循环不变量：

> A[1..j-1] = A[1..n] 已经排好序。

这就说明：

> 整个算法结束后，输出数组确实是“原来的所有元素，而且有序”。

这部分是“证明它没写错逻辑”的思想，不是新的算法，只是帮你确信：

> **插入排序不但“看着像对的”，而且可以严格说明“必然是对的”。**

------

## 五、插入排序在不同输入上的表现（只讲现象）

接下来 PPT 进入算法分析部分，但我们只看“现象”和“典型输入”，不啃公式。

### 1. 输入规模 n 和“运行时间”概念

- n：输入里一共有多少个数据（排序时就是数组长度）。
- 算法运行时间 ≈ 它一共做了多少次“基本操作”（比如某一行的比较）。

你可以简单理解成：

> 算法在循环里忙活的次数，大概决定了它跑得快不快。

------

### 2. 最好情况：本来就排好了

PPT 说：如果数组已经是**从小到大排好**的：

- 每次轮到 A[j]，只要比较一次就发现：左边最后一个元素 A[i] 已经 ≤ key；
- while 条件失败，循环体一次都不进；
- 所以每个元素只做很少的几步操作，整体工作量**大致和 n 成正比**。

形象点说：

> 已经整齐的书架，再扫描一遍确认一下就行，几乎不用搬书。

------

### 3. 最坏情况：完全逆序

PPT 给的例子是类似 `[10, 7, 5, 4, 2]` 这种**从大到小**的逆序。

这时，每次插入 A[j] 都特别惨：

- 要把它和左边所有元素都比一遍；
- 然后左边一整串元素都往右挪一格；
- 相当于第 2 个元素要挪 1 次，第 3 个挪 2 次，第 4 个挪 3 次……
- 总共挪的次数大概是 1+2+…+(n-1) 这一类数量级，也就是**跟 n² 成比例**。

形象点说：

> 书架上书完全反着摆好，现在要全部翻个方向，
>  你只能一本一本往前搬，动作数量会非常多。

------

### 4. 随机情况 & 平均表现

PPT 还提到：很多时候“平均情况”其实和“最坏情况”差不太多，所以**做算法分析时通常盯着最坏情况就够了**。

但有一句话很重要：

> 插入排序的效率实际非常高！对于“好”的输入，比快排还快很多。

意思是：

- 当数据“几乎有序”时，插入排序只需要少量移动；
- 而像快速排序这类算法，不管你是否已经有序，都要做一整套分割、递归等操作；
- 所以在一些场景（小规模 + 基本有序）下，插入排序反而更香。

------

## 六、伪代码 vs 真代码：C 版本长啥样？

在倒数第二页，老师给了一个 **C 语言实现的插入排序**，和前面的伪代码一一对应，只是数组下标从 0 开始：

```c
void insert_sort(int x[], int n)
{
    int key, i, j;
    for (j = 1; j < n; j++)        // 对应伪代码里 j=2..n
    {
        key = x[j];
        i = j - 1;
        while (i >= 0 && x[i] > key)
        {
            x[i+1] = x[i];
            i--;
        }
        x[++i] = key;
    }
}
```

这部分想说明两件事：

1. **伪代码更抽象、更接近算法本质**，跟具体语言（C/C++/Java）无关；
2. **伪代码写清楚了，翻成真代码就很顺**，反过来如果伪代码都说不清，代码也很难不乱。

------

## 七、小结：这一章关于“算法”的要点（只保留跟算法有关的）

把所有“复杂度公式、符号”都淡化，关于算法本身，这一章想让你记住：

1. **插入排序算法本身在做什么**
   - 像整理手牌一样：左边保持有序，右边一张一张插入。
   - 伪代码结构：外层 for 枚举“新牌”，内层 while 把比它大的牌往右挪。
2. **如何用“循环不变量”来说明一个算法是正确的**
   - 找一句话：例如“每轮开始前，前面那一截已经有序”；
   - 说明它：
     - 一开始就成立（初始化）；
     - 每一轮之后还能继续成立（维持）；
     - 循环结束时，这句话就刚好变成整个算法的结论（终止）。
3. **插入排序在各种典型输入上的表现**
   - 已经排好：几乎只做一轮比对，非常快；
   - 完全逆序：每个元素都得往前“挤”，总动作数大致和 n² 成比例；
   - 随机输入：通常介于两者之间，但往往接近“最坏”这边；
   - 对“几乎有序的小数据”，插入排序在实践里很常用、而且很快。
4. **伪代码和具体语言之间的关系**
   - 伪代码负责把算法思想表达清楚；
   - 真正写 C/C++/Java 时，就是在伪代码基础上加上语法细节。

------

如果你愿意，下一步我可以帮你做两件事选一件：

- **手算演示**：拿一串具体数字（比如 5,2,4,6,1,3），我按“每一行代码”给你模拟插入排序的过程；
- **循环不变量练习**：我们共同为另一个简单小算法（比如线性查找）写一个“循环不变量”，你会更熟悉这个证明思路。



# a02-1

这部分主要在干三件事：

1. 介绍**分治算法思想（divide & conquer）**
2. 用这个思想设计一个具体排序算法：**归并排序（merge sort）**
3. 顺便讲讲：像归并排序这样的分治算法，为啥一般都比插入排序这类“简单循环”快很多（不推公式，只讲直觉）

------

## 一、从排序问题引出：为什么要“分而治之”

开头先回顾：排序问题可以用很多方法来做：

- 冒泡排序：挨个比较相邻元素、交换，大元素慢慢“冒”到末尾
- 插入排序：像摸牌一样，左边保持有序，右边元素一个个插入
- **归并排序**：典型的“分治”：先一劈为二，各自排好再合并
- 快速排序：也是分治，但用“选枢轴 + 划分”来干

PPT 上特别强调：**基于分治策略的排序算法（归并、快排）最坏情况下通常比插入排序快很多**。这一节就是拿归并排序当代表来展开。

------

## 二、分治思想：把大问题拆小，再合起来

在第 3 页，“Divide-and-conquer approach” 的总结可以概括成三句话：

> 1. **Divide**：把原问题拆成若干个规模较小但“长得一样”的子问题；
> 2. **Conquer**：对子问题递归地解决（规模够小的时候就直接算）；
> 3. **Combine**：把子问题的答案合成出原问题的答案。

“递归”就是：函数里又调用自己一遍，用更小的输入去做同样的事。

------

## 三、归并排序：如何用分治来排一个数组？

### 1. 归并排序的大框架

PPT 第 4 页的图是这样的流程（示例序列 2 3 8 1 4 5 7 6）：

1. **Divide**：把长度为 n 的数组 A 分成两半：
   - 左半：前 n/2 个元素
   - 右半：后 n/2 个元素
2. **Conquer**：对左半、右半分别**递归调用“归并排序”**，让它们各自排好序；
3. **Combine**：现在左右两半都是有序的了，用一个专门的子程序 **MERGE** 把这两段“有序子数组”合成一个总的有序数组。

递归什么时候停？

> 当子数组长度变成 1（或者 0）时，已经自然有序，不再分了，这叫“递归到底（bottom out）”。

------

### 2. 用小例子看一次完整过程

以数组 `[2, 3, 8, 1]` 为例（在 14 页的演示里）：

1. 最外层：`[2,3,8,1]`
   - 拆成 `[2,3]` 和 `[8,1]`
2. 对 `[2,3]` 再归并排序：
   - 拆成 `[2]` 和 `[3]`，它们都只有 1 个元素，直接视作有序；
   - 用 MERGE 合并 → `[2,3]`
3. 对 `[8,1]` 再归并排序：
   - 拆成 `[8]` 和 `[1]` → 各自有序；
   - MERGE 合并 → `[1,8]`
4. 现在左右两半分别变成 `[2,3]` 和 `[1,8]`，再用一次 MERGE：
   - 最终得到 `[1,2,3,8]`。

**重点：**整个归并排序真正“做比较 + 移动”的重活，都是在 MERGE 这个合并子程序里完成的；递归那几层更多是负责“把问题拆开”。

------

## 四、MERGE：如何把两个有序段合成一个有序段？

### 1. 问题描述

MERGE(A, p, q, r) 的任务：

- 已知 A[p..q] **已经有序**，A[q+1..r] 也**已经有序**；
- 把它们合成一个总的有序段，覆盖 A[p..r]。

也就是说，MERGE 并不“重新排序所有数据”，它只是**高效地把两个有序表“归并”成一个有序表**。

------

### 2. 直观做法：像玩两摞有序扑克牌

PPT 第 5–6 页用的是扑克牌示意图：左边一摞 9,10,J,Q,K，右边一摞 7,8,9,10,J（都已排好），要把它们合到一起。

操作像这样：

1. 分别在左右两摞顶部放一个指针 i 和 j（指向当前“候选最小牌”）；
2. 比较两摞的顶部：谁小，就把谁“取出”放到结果数组里，然后那一摞的指针往下挪一张；
3. 重复这个过程，直到其中一摞的牌已经用完；
4. 另一摞剩下的牌直接全部顺序接在后面即可（因为那一摞自己已经有序）。

这样就能保证：**每次放入结果数组的都是当前能看到的最小的那张牌**，所以最终数组整体有序。

------

### 3. 程序里的实现细节：L、R 两个辅助数组 + 哨兵 ∞

伪代码在第 7 页：

大致步骤：

1. 计算左半长度 `n1 = q-p+1`，右半长度 `n2 = r-q`；
2. 开两个新数组 L[1..n1+1], R[1..n2+1]；
3. 把 A[p..q] 依次拷贝到 L[1..n1]；
4. 把 A[q+1..r] 依次拷贝到 R[1..n2]；
5. 在 L[n1+1] 和 R[n2+1] 的位置，放一个**特别大的值 ∞（无穷大）** —— 叫“哨兵”；
6. 然后设置 i=1, j=1，从 A[k]（k=p…r）开始，一格一格地填回去：
   - 比较 L[i] 和 R[j]：
     - 若 L[i] ≤ R[j]，就把 L[i] 写到 A[k]，i++；
     - 否则把 R[j] 写到 A[k]，j++。

**哨兵的作用**：

- 当左边所有正常元素用完时，L[i] 会指向 L[n1+1]，也就是这个“特别大”的 ∞；
- 再比较时，R[j] 一定会 ≤ ∞，所以接下来的所有位置都会从右边取；
- 同理，右边用完后就只从左边取；
- **这样我们在循环里面就不用每次都额外判断“L 或 R 是否已经空了”**，逻辑更干净。

可以把哨兵理解成：

> 在牌堆底下压了一张“超大牌”，保证真正的牌永远都先被取完，最后压底的那张不会干扰排序顺序。

------

## 五、为什么 MERGE 一定是对的？（循环不变量直觉版）

第 9 页给了 MERGE 的循环不变量证明，这和之前插入排序那一节类似，只是对象换成了 A[p..r]。

**循环不变量大意：**
 在 for 循环每一轮开始时（循环变量是 k）：

> A[p..k-1] 中，已经包含了从 L 和 R 中取出的 **(k-p) 个最小元素**，而且是有序的；
>  L[i] 和 R[j] 分别是各自数组中“还没有被放回 A 的部分里”的最小元素。

然后三步走：

1. **初始化**：
   - 第一次循环前，k=p，A[p..k-1] 是一个空区间，相当于“已经取出了 0 个最小元素”，说得过去；
   - i=j=1，L[i]、R[j] 都是各自数组的第一个元素，确实是“当前最小的未选元素”。
2. **维持**：
   - 当我们在一轮里选择较小者（比如 L[i]），放进 A[k]，然后 i++：
     - A[p..k] 就包含了多一个最小元素，仍然有序；
     - 此时 L[i]、R[j] 仍然是各自数组“剩余部分”里的最小者；
   - 所以下一轮循环开始时，不变量依然成立。
3. **终止**：
   - 当 k 走到 r+1 时，循环结束；
   - 这时候 A[p..r] 恰好包含了 r-p+1 个元素，就是 L 和 R 里所有正常元素（除去两个哨兵）；
   - 而且是从小到大排好的，因此 A[p..r] 就是合并后的整体有序数组。

这说明：**只要 L 和 R 一开始各自是有序的，MERGE 一定能把它们正确地合并成一个有序段**。

------

## 六、MERGE-SORT 递归函数长什么样？怎么用？

第 12 页给出 MERGE-SORT 的伪代码：

```text
MERGE-SORT(A, p, r)
1 if p < r
2     q ← floor((p + r) / 2)
3     MERGE-SORT(A, p, q)      // 递归排序左半
4     MERGE-SORT(A, q+1, r)    // 递归排序右半
5     MERGE(A, p, q, r)        // 合并两半
```

含义：

- 如果 p>=r，说明这一段长度 ≤1，本身有序，什么也不做；
- 否则算出中点 q，把数组划成 `[p..q]` 和 `[q+1..r]` 两半：
  - 分别递归调用 MERGE-SORT 排好两半；
  - 最后调用 MERGE 把两半合并。

第 13–14 页拿 A = [2,3,8,1] 做了一个完整的递归展开示意，同时在右边给出每一步合并后的结果。老师在 PPT 上写了一句话：

> “在使用的时候：忽略递归过程！函数重在接口！递归重在调用！” 

意思是：

- 真正在“写程序用”这个函数时，**你只关心：给它一个数组，它能帮你排好**；
- 内部到底递归了多少层、怎么分、怎么合，其实可以当成黑箱；
- 这也是递归函数的好处：**把复杂逻辑藏在函数内部，用起来反而简单**。

------

## 七、分治算法的运行时间直觉（为什么归并排序比插入快）

> 你说不太想听复杂度推导，那我只讲**直觉结论**，不写太多公式。

第 15–19 页主要是在做一件事：

> 用一个“递推式”（T(n)）来描述分治算法需要多少时间，然后用图（递归树）来说明大致规模。

### 1. 分治算法的一般“时间公式”长啥样（直觉）

设 T(n) 表示处理规模为 n 的问题需要的时间，大致会长这样：

> T(n) = 子问题花的时间 + 拆分 + 合并花的时间

对很多分治算法，典型形式是：

- 把问题拆成 a 个子问题，每个规模大约是 n/b：→ **子问题部分： a · T(n/b)**
- 拆分本身要做 D(n) 工作，合并要做 C(n) 工作

所以类似：

> T(n) = a·T(n/b) + D(n) + C(n)

这里只是个**描述思想的框架**。

------

### 2. 对归并排序来说，a=2，b=2

对于 MERGE-SORT，情况是：

- 每次把数组一劈两半 → **2 个子问题，每个是原来的一半**
  - 子问题部分：`2 · T(n/2)`
- 划分：算中点 q 很快，算一次就行 → 可以看成“常数级工作”
- 合并：MERGE 需要看一遍所有元素，每个元素只搬一次 → 工作量**跟 n 成正比**

所以可以**口头写成**：

> “归并排序对 n 个数排序，大概需要：
>  先花 2·T(n/2) 时间排两个子数组，然后再花“跟 n 成正比”的时间合并。”

递归树那张图解释的是：

- 每一层“合并”所有子数组时，总共要看 n 个元素，所以每层成本差不多都是 **cn（常数×n）**；
- 每分一次，数组长度减半，所以一共能分大约 log₂ n 层。

于是总时间 ≈ **每层 cn × 层数 ≈ n × log n**。
 PPT 最后一句话就是：

> “显然，归并排序优于插入排序，因为 (n \log n < n^2)。”

**你只需记住：**

- 插入排序最坏情况下要干“跟 n² 差不多多”的活；
- 归并排序最坏情况下只要“n log n 级别”的活；
- 当 n 很大时，n log n 远小于 n²，所以归并排序要快很多。

------

## 八、最后的练习：Fibonacci 的四种算法（只说思路）

最后几页练习让你对比 4 种求 Fibonacci 数列 F(n) 的算法：

1. **最朴素递归**：
   - 照着数学式子：`F(n) = F(n-1) + F(n-2)` 一层层递归；
   - 简单但会有大量重复计算。
2. **循环迭代**：
   - 从 F(1)=1, F(2)=1 开始，用两个变量往后推；
   - 跟你前面那章看到的一样。
3. **矩阵乘法形式**：
   - 把 `[F(n), F(n-1)]` 写成一个 2×2 矩阵乘以前一状态；
   - 把求 F(n) 变成算一个矩阵的 n-2 次方；
4. **利用“快速幂 + 矩阵”**：
   - 用“快速幂”的方式，少做很多次乘法，从而加速。

练习是让你分析这四种方法的时间复杂度，感受“同一个数学定义，可以有效率差异极大的算法”。

------

## 九、整章小结（只聚焦算法 & 案例）

这一份 PPT 的算法主线可以浓缩成：

1. **算法设计策略**
   - 除了之前讲的“模拟、贪心”等，还有一个非常重要的策略：**分治**。
2. **归并排序（Merge Sort）**
   - “分”成两半 → 各自递归排序 → “合”的时候用 MERGE 线性归并两个有序数组；
   - MERGE 利用两个辅助数组 + 哨兵值 ∞，实现逻辑简单的“取两个最小者再写回”的过程。
3. **正确性证明**
   - 用“循环不变量”证明 MERGE 合并得到的一定是排序好的结果；
   - 配合之前插入排序的循环不变量，让你熟悉这种证明套路。
4. **性能直觉**
   - 归并排序每一层都要看一遍所有元素，但一共只分 log n 层；
   - 所以总工作量大约是 n × log n，比插入排序的 n² 更省力（规模大时优势明显）。
5. **Fibonacci 练习**
   - 同一个数学定义，通过朴素递归、循环、矩阵 / 快速幂等多种方式实现，
   - 让你练习“比较不同算法效率”的思维。

如果你愿意，我可以下一步**专门帮你模拟一次 MERGE 的完整过程**（比如合并 `[1,4,7]` 和 `[2,3,6,8]`），一行一行对应伪代码，让你真正“看到” i、j、k 和哨兵是怎样配合工作的。



# a02-2

这一章**没有新算法**，主要是在教你一套“说人话的记号”，用来描述、比较算法运行时间的“快慢等级”。

可以把它理解成：

> 前几章给了你插入排序、归并排序这些“武功招式”，
>  这一章在教你：怎么用统一的**符号语言**来比较“哪门武功更费力”。

下面我分三块讲：

1. 这一章的大主题和跟算法有关的“典型案例”
2. 五种常见记号：Θ, O, Ω, o, ω（只讲直觉，不算证明）
3. 最后几页的函数比较图像和常见函数类型

------

## 一、这章的大主题 + 典型算法案例

开头老师提到一个对比：插入排序 vs 归并排序：

- 插入排序的运行时间：大约是 **Θ(n²)**
- 归并排序的运行时间：大约是 **Θ(n log n)**

又举了个数值例子：

- 当 n=2 时：100·n·log n 还是比 3·n² 大
- 但当 n 越来越大（n→∞）时，**n log n 这一类函数最终会“打败” n²**

所以本章的目的就是：

> 建立一套“函数长势”的记号，
>  用来表达“归并排序比插入排序更高效”这种结论，
>  而不用死盯常数系数和低阶项。

------

## 二、五种渐近记号：它们大概在干嘛？

所有这些记号，都是在描述一个**函数 T(n)（算法运行时间）随着 n 变大时增长的速度**。
 你可以把 T(n) 当成“算法在输入规模为 n 时，大概要做多少步操作”。

------

### 1. Θ(g(n))：长得“差不多一样快”的一类

**Θ（西塔）记号：渐近紧界**。

直觉：

> f(n) = Θ(g(n))
>  就是说：
>  “从某个规模以后，f(n) 跟 g(n) 的增长速度差不多，就是相差一个上下的常数倍。”

你可以脑补成：

- 存在两个常数 c₁, c₂>0 和一个 n₀，使得
  - 对所有 n ≥ n₀，有：
     c₁·g(n) ≤ f(n) ≤ c₂·g(n)

所以：

- **插入排序**最坏运行时间是 Θ(n²)，意思是：
  - 对足够大的 n，它做的操作次数被“上下两个常数×n²”夹在中间。
- **归并排序**是 Θ(n log n)，也是同样的意思，只是 g(n) 换成了 n log n。

PPT 的例子：

- `n²/2 – 3n = Θ(n²)`——把低阶项（-3n）和常数省掉，从整体形状看，它就是“一个 n²”。
- 任意次数为 d 的多项式 `p(n) = a_d n^d + ... + a_0`，只要最高项系数 a_d>0，那么 `p(n) = Θ(n^d)`。

> 也就是说：**只看最高次项、忽略常数系数**，就是“Θ”想表达的感觉。

------

### 2. O(g(n))：上界——“不会比 g(n) 更慢很多”

**O 记号：渐近上界（upper bound）**。

直觉：

> f(n) = O(g(n))
>  意思是：
>  “从某个规模以后，f(n) 的增长速度不超过 g(n) 的常数倍。”

就是只要有一个 c>0 和 n₀，使得对所有 n≥n₀：

- 0 ≤ f(n) ≤ c·g(n)

所以：

- `2n² = O(n³)`：增长速度**不比 n³ 快**，很显然；
- `2n² = O(n²)`：这就更“紧”了，不过 O 记号本身不要求紧。

**和算法的关系：**

- 说“某算法的运行时间是 O(n²)”
   一般表示：**最坏情况下，它不会比 c·n² 更糟糕**。
- 例如，插入排序的最坏情况运行时间 `T(n)` 满足 `T(n) = O(n²)`。

注意：

- `Θ(g(n))` 一定也是 `O(g(n))`，但反过来不一定。
- O 的“范围更大，描述更宽松”，Θ 是“刚好卡在那一档”。

------

### 3. Ω(g(n))：下界——“至少像 g(n) 这么快”

**Ω 记号：渐近下界（lower bound）**。

直觉：

> f(n) = Ω(g(n))
>  就是：
>  “从某个规模以后，f(n) 至少是 g(n) 的常数倍那么大。”

形式上：存在 c>0, n₀，使得所有 n≥n₀ 有

- 0 ≤ c·g(n) ≤ f(n)

和算法的关系：

- 说“算法运行时间是 Ω(g(n))”
   意味着：**无论你给什么输入，只要规模是 n，算法至少要做 c·g(n) 次操作**（n 足够大时）。

PPT 还特别强调：

- 插入排序的运行时间**在某些输入上**可以是 Ω(n)（比如本来就有序）；
- 但“最坏情况”运行时间又是 Ω(n²)，这并不矛盾，因为“最坏情况”是只看最坏那一组输入。

**定理 3.1：**

> f(n) = Θ(g(n))
>  当且仅当
>  f(n) = O(g(n)) 且 f(n) = Ω(g(n))。

也就是说：

> 把“上界”和“下界”都卡在同一档函数 g(n) 上，
>  那这个函数的“数量级”就被锁死在 Θ(g(n))。

------

### 4. o(g(n))：比 g(n) 慢一整个数量级

**小 o 记号：非紧的上界**。

直觉：

> f(n) = o(g(n))
>  表示：
>  “f(n) 比 g(n) 小很多很多，随着 n→∞，f(n)/g(n) → 0”。

PPT 的形式定义是：

- 对**任意**常数 c>0，都可以找到一个 n₀，使得所有 n≥n₀ 时：
   0 ≤ f(n) < c·g(n)。

例子：

- `2n = o(n²)`：因为 n 变大时，`2n / n² = 2/n → 0`
- 但 `2n² ≠ o(n²)`：因为比值永远是 2，不会趋近 0

你可以把 o(g(n)) 当成“长远来看，f 完全可以忽略不计”的那种小角色。

------

### 5. ω(g(n))：比 g(n) 快一整个数量级

**小 ω 记号：非紧的下界**。

它和 Ω 的关系，就像 o 和 O 一样：

> f(n) = ω(g(n))
>  意味着：
>  “f(n) 比 g(n) 大很多很多，f(n)/g(n) → ∞”。

形式定义：

- 对任意 c>0，都有足够大的 n，使得
   0 ≤ c·g(n) < f(n)

例子：

- `n²/2 = ω(n)`：因为 `(n²/2)/n = n/2 → ∞`
- 但 `n²/2 ≠ ω(n²)`：因为比值只是 1/2。

------

## 三、函数比较的“关系表”和图像案例

### 1. 类比“<、≤、=、≥、>”

PPT 做了一个很直观的类比：

| 实数之间 | 函数之间（渐近记号） |
| -------- | -------------------- |
| a < b    | f(n) = o(g(n))       |
| a ≤ b    | f(n) = O(g(n))       |
| a = b    | f(n) = Θ(g(n))       |
| a ≥ b    | f(n) = Ω(g(n))       |
| a > b    | f(n) = ω(g(n))       |

意思就是：

- o 对应 “严格小于”；
- O 对应 “小于等于”；
- Θ 对应 “相等等级”；
- Ω 对应 “大于等于”；
- ω 对应 “严格大于”。

同时也介绍了这些关系的**传递性、自反性**等等（比如如果 f=Θ(g)，g=Θ(h)，那 f=Θ(h)）。

不过有一点跟实数不一样：

> 实数 a, b 总有 a<b, a=b, a>b 三者之一。
>  但函数 f(n), g(n) 有可能**无法比较**，既不是 O，也不是 Ω。

PPT 举了一个带 **sin n** 的振荡函数作为例子，让你看到：
 有的函数时不时大一下、小时候又比另一个小，很难说谁“明显更快”。

------

### 2. 函数增长图：线性、平方、指数谁更猛？

后面几页画了几张图，对比这些函数：

- f(n) = log n
- f(n) = n
- f(n) = n log n
- f(n) = n²
- f(n) = n³
- f(n) = 2ⁿ

纵坐标一开始只画到 250，然后放大到 1000、5000，来展示：

1. 开头 n 比较小（1~5）时，所有曲线挤在一起，看不出太多差异；
2. n 稍大一点时，n²、n³、2ⁿ 明显往上窜；
3. 再放大纵轴，你会发现：
   - log n 和 n log n 增长很慢（曲线很平）；
   - n² 比 n 陡一点；
   - n³ 比 n² 更陡；
   - 2ⁿ 直接“飞到天上去”。

这一组图的目的就是：

> 帮你直观记住不同“复杂度等级”的增长快慢顺序：
>  log n ≺ n ≺ n log n ≺ n² ≺ n³ ≺ 2ⁿ

这样，回到算法世界：

- 如果算法 A 是 O(n log n)，算法 B 是 O(n²)，
   当 n 很大时，A **通常会比 B 快很多**。
- 你就知道：**归并排序、快速排序这类 n log n 的算法，要优先考虑。**

------

## 四、3.2 小节：常见函数类型（和算法的关系）

最后一页列了几个常见数学概念：单调性、向下取整 / 向上取整、模运算、多项式、指数、对数、阶乘等，作为自复习。

这些本身不是算法，只是你在写“算法运行时间 T(n)”时**经常会出现的函数形状**：

- log₂ n：二分查找、分治算法里常见；
- n!：全排列遍历、旅行商问题暴力枚举；
- 2ⁿ：枚举所有子集；
- 多项式 n^k：很多简单循环算法；
- 模运算：哈希、加密、循环数组等地方会用。

------

## 五、和前面几章的联系（总结）

把这三章放在一起看：

1. 第一章：介绍算法、Fibonacci、排序、路径等问题是什么；
2. 第二章：详细讲了插入排序、归并排序这类**具体算法的步骤**；
3. **第三章（这一章）：给你一套“通用记号”去描述它们的运行时间 T(n)**，尤其是：
   - 插入排序：T(n) = Θ(n²)
   - 归并排序：T(n) = Θ(n log n)

以后老师再讲新算法时，常常只会说一句：

> “这个算法的时间是 O(n log n)”，
>  或者 “这个问题至少需要 Ω(n²) 时间”。

你就要靠本章的记号理解：

> 这代表着**它在“数量级”上是哪个等级**，
>  从而判断：在大规模输入下，这个算法到底靠不靠谱。

------

如果你愿意，下一步我可以**挑几个你上课笔记里的具体式子**（比如“插入排序是 Θ(n²)”，“归并排序是 Θ(n log n)”），帮你用中文一句话解释“这句话到底想说明什么”，而不去算证明或推导。



# a02-3

这一章主要是在讲一件事：

> **怎么用“递归式 / 递推式”来描述算法做了多少事，以及怎么用几种方法把这种递归式“解出来”。** 

虽然老师的重点是“分析复杂度”，但你要的是：

- 这一章提到**哪些算法 / 问题**
- 对应的**递归关系（怎么一步一步算）**和**直观解法**

那我就从“**有哪些典型案例 + 各自的递推式和含义**”来给你讲。

------

## 一、几个跟算法直接相关的典型递归式

### 1. 插入排序的“总工作量”——用求和/递推表达

前面几章讲过插入排序，这里只是拿它来当例子：

- 插入排序对 n 个数排序时，外层 for 跑 `j = 2..n`，
- 每一轮里，while 循环要看前面有多少元素比当前 key 大，就移动多少次。

老师在这一页把每一行的“执行次数”记为 c₁, c₂,…，然后得到一个总的式子：

> T(n) = c₁·n + c₂·(n-1) + … + 里面还夹着若干个求和符号 Σ tⱼ （表示第 j 次插入移动了多少次）

你可以把它理解成：

- **T(n)**：插入排序处理 n 个元素，大概要做多少次“比较/赋值”这类基本操作；
- 这就是一个“**递推 / 累加关系**”：由每一轮的工作量加起来得到总工作量。

这里主要是为了引出：

> 很多算法的“总工作量”都可以用**递推式 / 求和式**表示。

------

### 2. 归并排序（Merge Sort）的递推式

这章里再次出现 MERGE-SORT，并给出一个非常典型的递推：

```text
T(n) = 1                 , n = 1 时
T(n) = 2T(n/2) + n       , n > 1 时
```

含义是：

- 如果只有 1 个元素，直接是有序的，工作量记成 1；
- 否则：
  - 把 n 个元素平均劈成两半，各自递归排序 → 左半 T(n/2) + 右半 T(n/2)，一共 `2T(n/2)`；
  - 再用 MERGE 把两个有序段合并，合并时要依次“看一遍所有元素”，做大约 n 次比较/复制 → “+ n”。

虽然老师接下来用这个式子去“算复杂度”，但你只需要记住：

> 对于**分治类算法**，
>
> - “分”出多少个子问题 → 前面有几个 T(·)
> - 子问题规模变成多少 → T(·) 里面 n/2 之类
> - “合并”需要多少额外操作 → 最后那个“+ n”之类

这就是 **分治算法的递推模型**。

------

### 3. Fibonacci（斐波那契数列）的递归算法

这一页又回到 Fibonacci：

```c
f(n) {
    if (n <= 2)
        return 1;
    else
        return f(n-1) + f(n-2);
}
```

对应的“运行时间递推式”写成：

```text
T(n) = 1                   , n <= 2
T(n) = T(n-1) + T(n-2) + 1 , n > 2
```



你可以这么理解：

- 算 f(n) 时，调用一次函数本身就有一点固定开销（判断 if、加法等） → 记成 +1；
- 真正耗时的大头是：
  - 先递归算 f(n-1) → 需要 T(n-1) 的时间
  - 再递归算 f(n-2) → 需要 T(n-2) 的时间

所以总工作量 = T(n-1) + T(n-2) + 1。

这个例子告诉你：

> **递归算法**的运行时间，通常也能写成一个**递归式**，
>  完全照着“函数内部调用自己的结构”写出来。

------

### 4. 计数问题：有多少种不同形态的二叉树？——卡特兰数 h(n)

中间有一块（配了一些二叉树的图）讲的是这样一个问题：

> 用 n 个节点，能组成多少种不同形态的**合法二叉树**？
>  记这个数为 h(n)。

老师先列出了小 n 的情况（比如 n=3 时有 5 种），然后得出了一个经典递推：

```text
h(0) = 1  （约定空树有 1 种）
h(n) = h(0)·h(n-1) + h(1)·h(n-2) + ... + h(n-1)·h(0)
```



为什么会是这个式子？可以这样理解：

- 一个有 n 个节点的二叉树，根节点算 1 个，其余 n-1 个节点分成：
  - 左子树用 i 个节点
  - 右子树用 n-1-i 个节点
- 左子树有 h(i) 种形态，右子树有 h(n-1-i) 种形态；
- 固定 i 时，“左形态 × 右形态”共 h(i)·h(n-1-i) 种；
- i 可以从 0 到 n-1，所以全部加起来。

这就是一个“**组合计数的递推算法**”：你要算 h(n)，只要知道所有 h(0), h(1), …, h(n-1)，就可以按上面这个式子算出来。

老师还给出另一个更简洁的递推：

```text
h(n) = ((4n - 2) / (n + 1)) * h(n-1)
```

以及一个**显式公式**：

```text
h(n) = C(2n, n) / (n+1)
```

这里就不用你推导，只要知道：

> 这串 h(n) 叫**卡特兰数**，出现在很多“括号匹配、路径计数、二叉树形态数”等问题里。

------

## 二、“递推式”这个概念本身 + 几个典型形式

有一页专门给了“递推式（recurrence）”的定义：

> 一个递推式 =
>
> - 若干个**基础情况（base cases）**，比如 T(1)=1；
> - 再加上一条，用**更小的参数**表达 T(n) 的等式或不等式。

然后列了四个典型形式（你可以把它们理解成“不同算法形态对应的数学模型”）：

### 例 (1)：T(n) = T(n-1) + 1

```text
T(1) = 1
T(n) = T(n-1) + 1 , n > 1
```

对应的算法形态：

- 从 1 数到 n，每次做一点固定操作；
- 或者一个简单 for 循环，每轮只做常数工作。

直观解：

> 每次 +1，一共加 (n-1) 次，所以最后 T(n) ≈ n。

------

### 例 (2)：T(n) = 2T(n/2) + n

这就是前面归并排序的那个形式，代表“典型的分治算法”：

- 每次把规模为 n 的问题，拆成 2 个规模 n/2 的子问题（用 T(n/2) 表示）；
- 再加上一个“线性量级”的合并工作（+n）。

------

### 例 (3)：T(n) = T(n-1) + log n

```text
T(2) = 0
T(n) = T(n-1) + log n , n > 2
```

对应的算法形态：

- 有一个循环跑 i=3..n；
- 第 i 轮里需要做大约 log i 次工作（比如用二分查找插到某个有序链里）。

递推式就变成：

> 总工作量 = 之前的工作量 + 本轮的 log n。

------

### 例 (4)：T(n) = T(n/3) + T(2n/3) + n

```text
T(1) = 1
T(n) = T(n/3) + T(2n/3) + n , n > 1
```

这个长得像某些快速排序 / 分治算法：

- 把数据划成两块（一个约 1/3，一个约 2/3）；
- 对两块分别递归处理；
- 再加上一次线性扫描的工作。

直观上：

> 每一层递归都要看一遍所有元素（+n），
>  但层数 ≈ log n，
>  所以总体是“n 乘上 log n 级别”的工作量。

你不用记公式，只要记住：

> 不同的拆分方式（等分 / 1:2 / 9:1 ……）
>
> - 每层合并需要多少工作
>    → 就对应不同形式的递推式。

------

## 三、几种“解递推式”的方法（当成解题套路来理解）

后半章介绍的是：

> 如果你手里有一个递推式，
>  怎么找到它大概会长成什么样子（比如像 n² 还是 n log n）。

这些方法本身也可以当成“**数学上的解题算法**”，只是你说不需要复杂度分析细节，我就用直观语言带你看一下**它们在做什么**，不算证明。

### 1. 置换法（Substitution method）

核心步骤：

1. **猜一个答案形状**：比如看到 `T(n) = 2T(n/2) + n`，就凭经验猜“它可能长得像 n log n”；
2. 用**数学归纳法**验证这个猜想：
   - 假设对所有小于 n 的规模都成立；
   - 把这个假设代回递推式，看看能否推出对 n 也成立。

你可以把它看成：

> 先大胆猜，再用归纳法把猜想“打勾确认”。

对算法来说，这相当于：

> 你看到一个递归程序，凭直觉猜“应该是 n log n”，
>  再用一点数学工具检查这个直觉是否靠谱。

------

### 2. 迭代法（Iteration method）

做的事情是：

- 一遍一遍“展开”递推式：

  比如 `T(n) = 3T(3n/4) + n`，
   再把 `T(3n/4)` 按同样公式展开，下去几层，就会变成：

  ```text
  T(n) = 3T(3n/4) + n
       = 3(3T(9n/16) + 3n/4) + n
       = 9T(9n/16) + 3n + n
       ...
  ```

- 展开到最后，剩下一长串加号构成的“和”（级数）；

- 再用求和的方式看这个和大概是什么数量级。

直观理解：

> 迭代法就是把“递推”一步一步摊开，
>  看每一层做了多少事情，再把所有层加起来。

------

### 3. 递归树（Recursion-tree）方法

递归树就是把“递归调用关系”画成一棵树：

- 根节点代表原问题 T(n)，标上这一层需要干多少工作（比如 n）；
- 它有若干个子结点，对应子问题 T(n/2)、T(2n/3) 等；
- 再往下画，直到规模很小为止。

然后你数：

- **每一层所有节点的工作量总和是多少**；
- **一共多少层**；
- 把每一层的总工作量加起来，就是 T(n) 的数量级。

这特别适合**分治算法**，因为它们的递归结构本来就像树。

------

### 4. Master（主方法）——专门针对 T(n) = aT(n/b) + f(n)

这一节只是提到，有一个“万能模板”专门应对这类递推：

```text
T(n) = a T(n/b) + f(n)
```

只要：

- 子问题个数 = a；
- 每个子问题规模 = 原来的 1/b；
- 每层额外的工作量 = f(n)；

你就可以对比 **$f(n)$** 和 **$n^{log_b a}$** 的大小，直接套一个结论。

你现在只要知道：

> 很多分治算法，比如归并排序、快速排序、一些树形算法，
>  对应的 T(n) 都能写成 aT(n/b)+f(n) 的形式，
>  Master 方法就是“查表式”的快捷解法。

------

## 四、几个“数学小技巧”——对算法只是辅助

最后几页提到一些技巧：

- **忽略边界条件 / 只在 n 很大的时候关心**：
   例如 MERGE-SORT 的 T(1) 是 1 还是 100，其实不影响“大致长成 n log n 还是 n²”。
- **把取整符号（floor/ceil）先当掉**：
   比如 T(⌊n/2⌋) 当成 T(n/2)，这样方便计算。
- **变化变量（change of variable）**：
   比如把 `n = 2^m` 换成用 m 表示，使得递推式变简单。

这些都属于“数学上的简化技巧”，
 对理解**算法本身**不是必须的，只是为了算式子更好看。

------

## 五、整章总结（只从“算法和案例”的角度）

这一章没有教你新“代码级”的算法，但通过几个熟悉的算法/问题，教你：

1. **分治 / 递归 / 计数类问题，怎么写递推式**
   - 插入排序：用求和式表示每趟插入工作量的累加
   - 归并排序：`T(n) = 2T(n/2) + n`
   - Fibonacci 递归算法：`T(n) = T(n-1)+T(n-2)+1`
   - 二叉树形态数 h(n)（卡特兰数）：
      `h(n) = Σ h(i)·h(n-1-i)`
2. **常见递推形式背后，对应什么样的算法结构**
   - `T(n) = T(n-1) + 1` → 简单线性遍历
   - `T(n) = 2T(n/2) + n` → 典型二分分治（如归并排序）
   - `T(n) = T(n/3) + T(2n/3) + n` → 不均匀分治（类似快速排序）
3. **有了递推式之后，有几套“解题套路”可以求它的数量级**
   - 猜答案 + 归纳检查（置换法）
   - 一层层展开成求和（迭代法）
   - 画递归树看每层工作量（递归树法）
   - 对 aT(n/b)+f(n) 这类用 Master 模板。

如果你愿意，下一步我可以：

- **拿一个具体的递推式**（比如 `T(n) = 2T(n/2) + n` 或 Fibonacci 那个），
- 用完全中文、配上图/树，带你“手算一遍”它是怎么一步一步展开、最后长成 n log n 级别的。



# a03-1

这一章其实没有讲新的“程序算法”，而是讲**一套专门用来处理分治算法的“数学小工具”——主方法（Master method）**。它主要用在形如

> T(n) = a·T(n/b) + f(n)

这种递归式上。

你可以把它理解成：

> 给你一个“分治类算法”的**时间递推式**，主方法就是一个**快速判别规则**，帮你看：
>  “到底是递归部分占主导，还是每一层额外做的工作占主导”。

下面我还是按你习惯的结构来讲：

1. 这一章到底在说什么
2. 这些递推式背后藏着什么样的算法模式
3. 主方法的直观含义：“谁大，解就是谁”
4. PPT 里的典型例子 + 对应的“算法故事”
5. 最后一页的二叉树计数（卡特兰数）小案例

------

## 一、这一章的主角：T(n) = aT(n/b) + f(n)

第 2 页一上来就点题：本章关心的就是

> **T(n) = aT(n/b)+f(n)** 这样的递推式，a≥1，b>1，f(n) 正的。

这正是绝大多数**分治算法**的“工作量模型”：

- 把规模为 n 的问题**拆成 a 个子问题**，每个子问题规模变成大约 n/b；
- 对每个子问题**递归调用**同一个算法 → a·T(n/b)；
- 再加上“拆分 + 合并”这些额外操作要做的工作 → f(n)。

典型例子（你之前的 PPT 里见过）：

- 归并排序：`T(n) = 2T(n/2) + cn` —— 分两半 + 一次线性合并；
- 某些图像算法：把图像分成 9 块，每块递归处理，再做一点整体处理 → 形如 `T(n)=9T(n/3)+f(n)`。

这一章就是：

> 在**不算细节**的前提下，用一个统一套路，看出 T(n) 大致会长成哪一类函数（像 n？像 n log n？像 n²？）。

------

## 二、这些递推式背后，大概是什么算法模式？

虽然 PPT 重点是数学推导，但每个式子都对应着一种很典型的“算法结构”。

### 1. T(n) = aT(n/b) + cn —— 标准分治 + 线性合并

这类形式对应：

- **分成 a 个子问题**：每次递归调用 a 次；
- **缩小比例 1/b**：每个子问题的输入规模是原来的 1/b；
- **合并工作线性**：每一层额外只需要看一遍所有元素，做 O(n) 级别的工作。

典型算法：

- 归并排序（Merge Sort）
- 一些分块统计、分块求和之类的算法

PPT 前几页用展开法把它写成一条**几何级数**，然后才概括出主定理。

------

### 2. T(n) = T(2n/3) + 1 —— 每次扔掉一部分数据

背后的算法模式：

- 每次递归只保留**2/3 的数据**，另外 1/3 直接丢掉；
- 每一层只做一点点固定工作（+1）。

这就像：

- 改版的“二分查找”：一次不是一刀劈成两半，而是砍掉 1/3，只保留 2/3 再搜；
- 或者某种“缩小规模搜索”问题：每次排除掉一部分候选，只在剩下的继续找。

------

### 3. T(n) = 3T(n/4) + n log n —— 分成 3 块 + 一个重度合并

算法结构：

- 把问题分成 **3 个子块**，每块只有原来的 1/4；
- 对每块递归处理 → 3T(n/4)；
- 再在当前层对所有 n 个数据做“比较费时”的操作，比如排序、建堆等 → n log n。

可以想象为：

> “分成几部分分别算一遍，然后再做一次稍微复杂点的全局操作”。

------

### 4. T(n) = 2T(n/2) + n log n —— 主方法搞不定的边界情况

这个式子看起来很像 归并排序，只是把“合并”的 cn 换成了“更重一点”的 n log n：

- 分两半 → 2T(n/2)
- 每一层合并时做了一个类似“排序”的操作 → n log n

直觉上：

> 每一层都做“差不多像排序那样难”的事，
>  再加上递归本身，
>  所以整体会比普通的归并排序更费劲。

PPT 特地说明：这个式子落在主定理的“夹缝”里（gap），三种情况都不满足，所以**必须用别的办法**（比如代入法、递归树）才能分析。这是提醒你：

> 主方法虽然好用，但也有**用不了的时候**。

------

## 三、主方法本身在说什么？——“谁大，解就是谁”

第 9–10 页给出了形式严谨的**定理 4.1（主定理）**，有三个情况。你可以完全把复杂的数学字样忘掉，只留下老师写的那句中文：

> **“谁大，解就是谁！”**

具体直觉是这样：

- 把递推式写成 `T(n) = aT(n/b) + f(n)`，

- 定义一个“递归部分对应的函数”

  > g(n) = n^{log_b a}

  这个 g(n) 描述的是：**所有子问题在一起，大致要做多少事**。

- 接下来比较 **f(n)** 和 **g(n)** 谁长得快：

  1. **如果 f(n) 明显比 g(n) 小一截**（多项式意义上更小）：
     - 递归子问题那部分占主导 → T(n) 跟 g(n) 一个等级。
  2. **如果 f(n) 跟 g(n) 一个等级**：
     - 两者势均力敌 → T(n) 会是“g(n) 再乘一个 log 的因子”。
  3. **如果 f(n) 比 g(n) 明显大一截**：
     - 每一层额外做的 f(n) 占主导 → T(n) 跟 f(n) 一个等级。

PPT 上的口号就是：

> 比较 f(n) 和 n^{log_b a} 的大小，
>  **递归树叶子那边大就叶子当老大，
>  每层合并那边大就合并当老大，
>  一样大就再乘个 log。**

你不需要记住正式的 O/Ω/Θ 条件，只要知道：

> 主方法就是替你“快速判断谁是老大”。

------

## 四、PPT 里的典型例子：对应什么样的“算法故事”？

### 例 1：T(n) = 9T(n/3) + n

- **算法结构**：每次把问题拆成 9 份，每份只有原来的 1/3 大小，再做一次线性遍历。

可以想象的场景：

- 图像处理：把一个 n×n 的图片分成 3×3 = 9 个小块，每块处理一次；
- 地图四分树 / 九宫格网格：每个格子递归细分，再汇总信息。

主方法的结果（只说结论大白话）：

> “九个子任务累积起来的工作，比每一层那点线性工作大多了”，
>  所以整体工作量**主要由递归子问题决定**，大致像 `n^2` 这种级别。

------

### 例 2：T(n) = T(2n/3) + 1

- **算法结构**：每次只剩下 2/3 的数据继续递归，其余直接丢，不需要再管；
- 每一层只做常数级操作（比较、判断、更新几个变量）。

可以联想到的例子：

- 一个“偏心”的二分查找：每次检查一个位置，然后只在某一边的 2/3 部分继续查。
- 某些游戏 / 搜索问题：每一步只保留最有希望的 2/3 情况，其他情况剪枝掉。

直觉结果：

> 每次规模都乘 2/3，一共递归不了几层，
>  所以算法总体工作量差不多和 **“递归深度”** 一个级别。

------

### 例 3：T(n) = 3T(n/4) + n log n

这一页 PPT 专门完整用主方法推了一次：a=3, b=4, f(n)=n log n。

- **算法结构**：
  - 每次分成 3 个规模为 n/4 的子问题；
  - 再对 n 个元素做一次像“排序 / 建堆 / 复杂合并”那样的 n log n 操作。

可以想象的场景：

- 把数据分成几块，各自递归做一点处理；
- 然后把所有块的数据合并成一个有序结构（比如堆 / 平衡树），就需要 n log n。

主方法判断的结论（直觉版）：

> 这次，每一层的“合并部分”f(n) 增长得比递归子问题 g(n) 还猛，
>  所以 T(n) 基本由 f(n) 决定：
>  **“合并操作的复杂度”就是整个算法的主导复杂度。**

------

### 例 4：T(n) = 2T(n/2) + n log n —— gap 例子

这里 a=2, b=2，所以 g(n)=n^{log_2 2}=n。

- f(n)=n log n
- g(n)=n

f(n) 比 g(n) 大一些，但**又没大到“多项式级别”**（“大一截”的意思是像 n¹ vs n² 这种），而只是多了一个 log。

PPT 解释说：

> 这种“只差一个 log 的情况”刚好落在**主定理 3 个条件的夹缝里**，
>  所以主方法**不适用**。

这并不是说递推式解不出来，只是：

- 主方法解决不了，就得换回上一章讲的
  - 置换法（猜答案 + 归纳）
  - 递归树法 等。

对你来说，记住一点就行：

> 主方法非常好用，但不是万能；
>  遇到这种“f(n)=n log n & g(n)=n”的边缘情况，就要换别的工具。

------

## 五、最后一页：二叉树计数 h(n) 的递归 & 闭式公式

最后一页又提到上次的二叉树问题：

> 用 n 个节点，可以构成多少种不同形态的二叉树？ 记为 h(n)。

给出两个递归式：

1. 通用形式（卡特兰数递推）：

   ```text
   h(n) = h(0)·h(n-1) + h(1)·h(n-2) + ... + h(n-1)·h(0)
   ```

   对应的算法思路：

   - 以根节点为界，左子树用 i 个节点，右子树用 n-1-i 个节点；
   - 左子树有 h(i) 种，右子树有 h(n-1-i) 种；
   - 固定 i 时，组合数是乘积 h(i)·h(n-1-i)；
   - i 从 0 到 n-1，全部加起来就是 h(n)。

   **这是典型的“DP + 组合计数”算法**：
    先算 h(0), h(1), …, h(n-1)，再按这个式子算 h(n)。

2. 另一种简洁递推（n≥2 时）：

   ```text
   h(n) = ((4n - 2) / (n + 1)) * h(n-1)
   ```

   这个式子是从上面的递推 + 一些组合恒等式推出来的，
    算法上意味着：

   > 要算 h(n)，只需知道 h(n-1)，乘上一个简单的分数即可。

PPT 最右侧还写了**闭式公式**：

```text
h(n) = C(2n, n) / (n+1)
```

并且说：“这个就不好猜！练习：证明该公式。”

你可以只把它当成结论记下：

> 这串 h(n)（1,1,2,5,14,42,……）叫**卡特兰数**，
>  在二叉树数量、括号合法表达式、栈出栈序列等问题里都会出现。

算法上最直接的用法就是：

- 用递归式或 DP 计算 h(n)，
- 在很多“数一数有多少种合法结构”的题里直接套用。

------

## 六、整章小结（只从“算法 / 案例”的角度）

这一章没有再教新的排序、搜索之类的代码，而是：

1. **把大量“分治算法”的工作量模型抽象成**
    `T(n) = aT(n/b) + f(n)` 这样的递推式；
2. 给出一个“**主方法 / 主定理**”：
   - 看 f(n) 和 `n^{log_b a}` 谁长得快；
   - 谁大，解大致就跟谁一个等级；
   - 一样大就再多一个 log；
   - 有些边界情况（比如 n log n vs n）主方法解决不了，要用其他工具；
3. 用几个**典型递推**展示不同算法结构：
   - `9T(n/3)+n`：超多子问题 + 轻量合并；
   - `T(2n/3)+1`：每次丢掉一部分数据的“缩小规模搜索”；
   - `3T(n/4)+n log n`：多子块 + 重度合并（合并成本比递归更大）；
   - `2T(n/2)+n log n`：主方法不适用的边缘例子；
4. 再次提到**二叉树计数 h(n)**，用递归和 DP 的思路来解决“有多少种二叉树形态”这类组合问题。

如果你愿意，下一步我可以帮你**选一两个具体递推式**（比如 `9T(n/3)+n` 或 h(n) 那个），
 一步一步用中文把它展开成“递归树图”或者“DP 代码思路”，
 这样你会更直观地看到“递推式 ↔ 算法”的对应关系。



# a03-2

这份 PPT 是在**复习第 1～4 章里跟“分治”有关的典型算法**，按顺序主要讲了：

1. 整数相乘的几种算法
2. 矩阵相乘的几种算法
3. 同时找最大值和最小值
4. 多数元素（majority）问题
5. 树枝生长 → 斐波那契数列 & 几种求 Fibonacci 的算法 

我就严格按 PPT 的顺序，一段一段用中文给你讲“有什么算法 + 典型例子 + 解法思路”，**不讲复杂度推导**。

------

## 0. 开头：复习内容总览

第 2 页先回顾之前学过的内容：如何描述算法、证明正确性、效率分析、分治、渐近分析、递推式等等。然后列出接下来要复习的几个例子：

1. 归并排序（之前讲过，这里只是列个名字）
2. 整数相乘
3. 矩阵相乘
4. 找最大值和最小值
5. 多数问题
6. 树枝生长 

下面从 **Exam2** 开始，每个“Exam”就是一个典型问题。

------

## 1. Exam2：整数相乘（Multiplication of two integers）

### 1.1 问题描述（第 4 页）

两 个 n 位十进制整数：

- X = xₙ₋₁ xₙ₋₂ … x₀
- Y = yₙ₋₁ yₙ₋₂ … y₀

每一位是 0~9 之间的数字。要求**设计一个算法求 X×Y**。

下面给了三个算法。

------

### 1.2 算法 1：我们从小就会的“竖式乘法”（Naive pencil-and-paper）

PPT 用 31415962 × 27182818 做示例，就是你小学学的竖式：

**思路：**

1. 让 Y 的每一位依次乘以 X（从最低位开始）：
   - 比如 27182818 的最后一位和 31415962 相乘，得到第一行；
   - 下一位乘以 31415962，再在结果后面加一个 0（相当于 *10）；
   - 依此类推。
2. 把所有这些“部分积”一行一行加起来，就得到最终结果。

**算法结构：**

- 外层循环：枚举 Y 的每一位（从个位到最高位）；
- 内层循环：用这一位去乘 X 的每一位，得到一行部分积；
- 最后做一次整体相加。

就是**完全照着人手算的流程写成程序**。

------

### 1.3 算法 2：分治乘法（Divide and Conquer）

从这一页开始就用上“分治思想”了。

把 X、Y 拆成“高位一半 + 低位一半”：

- X ≈ a·10ᵐ + b
- Y ≈ c·10ᵐ + d

其中 a,b,c,d 都是大约 n/2 位的数，m ≈ n/2。
 代入展开：

> XY = (10ᵐa + b)(10ᵐc + d)
>  = 10²ᵐ·ac + 10ᵐ·(ad + bc) + bd

也就是说，只要知道 ac、ad、bc、bd 四个积，就能把 XY 拼出来。

**算法流程（Multiply）：**

1. 如果 n=1（只有一位），那就直接用普通一位数乘法返回。
2. 否则：
   - 取中间位置 m，把 X 分成 a,b；Y 分成 c,d；
   - 递归地计算：
     - e = Multiply(a,c)      （高位×高位）
     - f = Multiply(b,d)      （低位×低位）
     - g = Multiply(b,c)
     - h = Multiply(a,d)
   - 最后按公式组合：
     - XY = 10²ᵐ·e + 10ᵐ·(g+h) + f

**本质：**

> 把一个“n 位数相乘”的大问题
>  拆成四个“n/2 位数相乘”的小问题，再做一些加法和移位。

------

### 1.4 算法 3：Karatsuba 分治乘法（更快一点的分治）

这部分在上一种分治基础上做了个小技巧。

还是把 X,Y 拆成 a,b,c,d，并用：

> XY = 10²ᵐ·ac + 10ᵐ·(ad + bc) + bd

但是注意到：

> ad + bc = ac + bd − (a − b)(c − d)

于是我们只需要算三个乘法：

- e = a·c
- f = b·d
- g = (a − b)(c − d)

再由 e,f,g 算出 ad+bc：

> ad+bc = e + f − g

**Karatsuba 算法流程（FastMultiply）：**

1. n=1 时直接相乘返回；
2. 否则
   - 拆成 a,b,c,d；
   - 递归计算：
     - e = FastMultiply(a,c)
     - f = FastMultiply(b,d)
     - g = FastMultiply(a−b, c−d)
   - 结果：
     - XY = 10²ᵐ·e + 10ᵐ·(e+f−g) + f

**直观理解：**

- 普通分治要做 **4 个子乘法**（ac,ad,bc,bd）；
- Karatsuba 只做 **3 个子乘法**，用加减法来“省掉一个乘法”；
- 对非常大的整数，这样的“少一个乘法”能带来明显加速。

PPT 还提到：n 很大时可以用 FFT、中国剩余定理等更高级算法；这是进一步优化，大概了解即可。

------

## 2. Exam3：矩阵相乘（Multiplication of two matrices）

### 2.1 问题描述（第 11 页）

给定两个 n×n 的实矩阵 A 和 B，要求计算 C = A×B。

公式：

> C[i,j] = Σₖ A[i,k]·B[k,j] 

下面同样给出几种算法。

------

### 2.2 算法 1：三重循环的标准算法

伪代码（第 12 页）：

```pseudo
for i = 1..n
  for j = 1..n
    C[i,j] = 0
    for k = 1..n
      C[i,j] += A[i,k] * B[k,j]
```

**意思：**

- 外层两个循环：枚举 C 的每个位置 (i,j)；
- 内层循环：对 k 从 1 到 n，累加 A[i,k]·B[k,j]，也就是一行一列的点积。

这是最常用、也是最直观的矩阵乘法实现。

------

### 2.3 算法 2：分治矩阵乘法（按块分成 2×2）

第 13 页把 n×n 的矩阵写成 4 个 n/2×n/2 的小块：

> A = [[A11 A12], [A21 A22]]
>  B = [[B11 B12], [B21 B22]]
>  C = A·B = [[C11 C12], [C21 C22]]

块之间的关系：

- C11 = A11B11 + A12B21
- C12 = A11B12 + A12B22
- C21 = A21B11 + A22B21
- C22 = A21B12 + A22B22

**分治思路：**

1. 把大矩阵拆成四块；
2. 递归计算上面 8 个矩阵乘积（A11B11, A12B21, …）；
3. 再把它们加起来拼成 C11..C22。

这是**把矩阵乘法看成“更小矩阵的乘法 + 加法”**的一种分治写法。

------

### 2.4 算法 3：Strassen 算法（用 7 次乘法完成）

第 15 页介绍 Strassen 算法。

关键点：

- 仍然把 A,B 分成 4 个小块；

- 但不是直接做 8 次小矩阵乘法，而是构造 7 个中间矩阵 P1..P7：

  ```text
  P1 = (A11+A22)(B11+B22)
  P2 = (A11+A22)B11
  P3 = A11(B11−B22)
  P4 = A22(−B11+B22)
  P5 = (A11+A12)B22
  P6 = (−A11+A21)(B11+B12)
  P7 = (A12−A22)(B21+B22)
  ```

- 然后用加减法拼出 C：

  ```text
  C11 = P1 + P4 − P5 + P7
  C12 = P3 + P5
  C21 = P2 + P4
  C22 = P1 + P3 − P2 + P6
  ```

**直观理解：**

> 用巧妙的代数变化，用 **7 次矩阵乘法 + 一堆加减法**
>  代替原来的 **8 次矩阵乘法**。

和 Karatsuba 一样，思想是：

- “乘法”比“加法”贵；
- 宁可多做加法、少做乘法。

后面一页还提到 Coppersmith–Winograd 等更快的矩阵乘法算法，说明：

> **一个问题的“标准解法”往往不是极限，还有空间可以继续优化。**

------

## 3. Exam4：同时找最小值和最大值（Finding Minimum and Maximum，MaxMin）

### 3.1 问题描述（第 17 页）

给一个有 n 个元素的数组，用“天平”只能比较两个元素的重量，
 要求：**找出最轻的和最重的元素**，并且尽量少比较次数。

（可以想象成一堆苹果、或者挑运动员体重。）

------

### 3.2 算法 1：直接法（Obvious method）

第 18 页思路：

1. 先扫描一遍找最大值：
   - 假设 maxIndex = 0；
   - 从 i=1 到 n−1：
     - 如果 w[i] > w[maxIndex]，更新 maxIndex。
2. 再扫描一遍找最小值：
   - 可以再次从头到尾扫一遍（或者略微跳过最大值位置）。

这样总共需要大约“两次遍历”。

**优点：**简单好写。
 **缺点：**比较次数偏多（老师后面用公式算了，但你可以理解为：大约要比较接近 2n 次）。

------

### 3.3 算法 2：分治找 MaxMin

从第 19～22 页，是用分治做同一件事。

**思路：**

1. 把数组拆成两半 L1 和 L2；
2. 分别在 L1 和 L2 中递归地求出 (min1, max1)、(min2, max2)；
3. 最终答案 =
   - 全局最小 = min(min1, min2)
   - 全局最大 = max(max1, max2)

伪代码（第 22 页）：

```pseudo
MaxMin(L):
  if 长度为 1 或 2：
      直接比较，最多一次比较就能得到 min 和 max
  else:
      把 L 分成 L1, L2
      (min1, max1) = MaxMin(L1)
      (min2, max2) = MaxMin(L2)
      return (min(min1,min2), max(max1,max2))
```

**直观理解：**

> 先在每一半里选出“最轻”和“最重”，
>  再在左、右两个候选里各比较一次，就得到全局的最轻和最重。

这样做的比较次数比“完全两次线性扫描”要少一些（PPT 后续给了对比表）。

------

## 4. Exam5：多数问题（Majority Problem）

### 4.1 问题描述（第 24 页）

给定一个长度为 n 的数组 A，只能用“==”比较（判断相等），
 要找出“**出现次数超过 n/2 的元素**”，如果存在的话。

比如 A = (2, 3, 2, 1, 3, 2, 2)，2 出现了 4 次，4 > 7/2，所以 2 是 majority。

------

### 4.2 算法 1：最直接的暴力法

伪代码：

```pseudo
for i = 1..n:
  M = 1
  for j = 1..n:
    if i != j 且 A[i] == A[j]:
      M++
  if M > n/2:
     返回“A[i] 是多数元素”
返回 “没有多数元素”
```

**思路：**

- 对每个候选 A[i]，再扫一遍数组数一数它出现几次；
- 任何一个的计数 > n/2 就是答案。

写起来简单，但要两层循环，效率不高。

------

### 4.3 算法 2：分治法找多数元素

第 25 页给了分治版本 Majority(A[1..n])：

**关键观察：**

> 如果整个数组有一个多数元素 x，
>  那么在左半 A[1..n/2] 或右半 A[n/2+1..n] 中，
>  至少有一半会把 x 视为“各自的多数元素”（或者是候选）。

**分治算法：**

```pseudo
Majority(A[1..n]):
  if n == 1:
    return A[1]

  m1 = Majority(A[1..n/2])
  m2 = Majority(A[n/2+1..n])

  在整个 A[1..n] 里统计 m1、m2 的出现次数
  如果其中某个出现次数 > n/2：
       返回它是多数
  否则：
       返回“没有多数”
```

**直观解释：**

1. 左半递归求一个候选 m1，右半求一个候选 m2；
2. 真正的多数元素如果存在，很可能就是 m1 或 m2；
3. 最后只要再扫一遍数组，验证这两个候选的真实出现次数就行。

------

### 4.4 算法 3：计数（Counting）——线性时间解法

第 26 页给了一个更快的算法（前提：数组元素的值不超过 n，便于开计数数组）。

**思路：**

1. 开一个数组 counting[...]，都初始化为 0；
2. 扫一遍 A，把每个元素 A[i] 的计数加一：`counting[A[i]]++`；
3. 扫一遍 counting，找出里面最大的计数 M；
4. 如果 M > n/2，对应的元素就是多数元素；否则没有。

对示例 A = (2,1,3,2,1,5,4,2,5,2)，最后可能得到：

- counting[1] = 2
- counting[2] = 4
- counting[3] = 1
- counting[4] = 1
- counting[5] = 2

最大是 4，对应元素 2，就是多数元素。
 （PPT 还问你“空间复杂度是多少”，因为要开一个大小跟元素范围相关的数组。）

### 4.5 小结：分治不一定是最优

第 27 页老师总结：

1. 暴力：O(n²)
2. 分治：O(n log n)
3. 计数：O(n)

然后写了句 **“寓意：分治并不总是最优解！(Divide and conquer may not always give you the best solution.)”**

意思是：

> 尽管分治很强大，但有些问题用更直接的思路（比如计数）能做得更好。

------

## 5. Exam6：树枝生长（Branch growth）→ 斐波那契

### 5.1 问题描述（第 28–30 页）

设想：

> 一棵树枝，2 年后会分叉长出新树枝。问：第 n 年时，树顶一共有多少根树枝？

图上画了从第 0 年的一粒种子开始，一年年长枝条的样子。

把每年的“顶端枝条数”记为 F(1),F(2),...,F(n)，
 PPT 上列了一行：

> 1, 1, 2, 3, 5, 8, …

这其实就是 **斐波那契数列 Fibonacci**：

> F(1)=1, F(2)=1,
>  F(n) = F(n-1) + F(n-2)

因为：

- 到第 n 年的顶端枝条，要么是前一年就存在的（没分叉），要么是前两年某枝条在今年分叉出来的；
- 这样分析，得到的递推关系就是 F(n)=F(n-1)+F(n-2)。

------

### 5.2 递归解法 + 时间递推

PPT 写了两行：

> 递归解：F(n) = F(n-1) + F(n-2)
>  计算时间：T(n) = T(n-1) + T(n-2)

也就是说：

- 算法：

  ```pseudo
  F(n):
    if n <= 2: return 1
    else: return F(n-1) + F(n-2)
  ```

- 时间：因为它要递归算 F(n-1)、F(n-2)，
   算法执行时间也满足一个类似的递推式。

这和之前讲的 Fibonacci 递归完全一样。

------

### 5.3 多种求 Fibonacci 的算法对比（第 31 页）

标题：**Fibonacci Number**。PPT 罗列了 6 种思路：

> F(1) F(2) F(3) F(4) F(5) F(6)…
>  1   1   2   3   5   8  …

下面是几种算法的简要说明（我只讲“它们怎么算”）：

1. **直接递归**

   - 就是前面说的：`F(n) = F(n-1) + F(n-2)`，
   - 写起来最简单，但会重复计算很多次。

2. **用数组迭代**

   PPT 上写：

   ```text
   F[2] = F[1] = 1;
   for(i: 3 → n)
       F[i] = F[i-1] + F[i-2];
   ```

   思路：

   - 把所有 F(1..n) 存进一个数组，从前往后推；
   - 每次只用到前两个数来更新下一个。

3. **用两个变量迭代（也就是你在前面章节见过的写法）**

   PPT 上写：

   ```text
   f1 = 1; f2 = 1;
   for(i: 1 → n) {
       f = f1 + f2;
       f2 = f1;
       f1 = f;
   }
   ```

   思路：

   - 只用 `f1` 表示当前 F(k)，`f2` 表示上一项 F(k-1)；
   - 每次更新这两个变量就行，不用数组。

4. **通项公式（黄金分割公式）**

   PPT 写了一个数学公式：

   ```text
   F(n) = (1/√5) * ( ((1+√5)/2)^n − ((1−√5)/2)^n )
   ```

   这个可以直接算出 F(n)，但需要浮点数和幂运算，一般是数学上用得多。

5. **矩阵法**

   PPT 写了一个矩阵关系：

   ```text
   [F(n+1) F(n)  ]ᵀ = [[1 1],[1 0]] · [F(n) F(n−1)]ᵀ
   ```

   也就是：

   ```text
   |F(n+1)|   |1 1| |F(n)  |
   |F(n)  | = |1 0| |F(n-1)|
   ```

   所以把这个矩阵提升到 n 次方，就可以通过 **快速矩阵幂** 在 log n 步内求到 F(n)。

6. **“其他方法”**

   PPT 只做简单提及，比如还有一些特殊递推、组合计数的做法，说明 Fibonacci 在数学和算法中出现极多。

整体来说，这一页是在告诉你：

> **同一个 Fibonacci 问题，可以有好多算法，效率差别巨大。**

这也是整个课程的主要思想之一。

------

### 5.4 更复杂的 Fibonacci 类问题：母牛数量（第 32 页）

最后一页提到“母牛的数量”问题（SkyLee 的艾露猫 F 题），本质上是一个更复杂的“出生 + 生育 + 死亡”的模型：

- T(k)：第 k 年母牛总数
- N(k)：第 k 年新出生母牛数

给出两个递推：

1. T(k) = 最近 p 年内新生母牛数之和
2. N(k) = 某一段年份内的新生母牛数量之和

具体文字是：

> “一头 x 年出生的母牛从 x+m 年到 x+n 年间每年生出一头母牛，并在 x+p 年被淘汰……计算第 k 年有多少头母牛。”

这属于 **“高阶线性递推”问题**：

- 比 Fibonacci 多了“m,n,p”这些参数；
- 但依然可以用“递推 / 动态规划 / 矩阵快速幂”去解决。

PPT 说“详细分析见另一份 PPT a03-1”，这里只是告诉你：

> 递归与分治在真实问题中可能变得非常复杂，但思路仍是从“某一年”推到“下一年”。

------

## 6. 最后：习题提示（第 33–34 页）

最后两页给了两个练习：

1. **用天平找第二重的元素**：
   - 问你能不能设计一个算法（最好用分治），只比较两个元素谁重，找出“第二重”的那个。
2. **实现两种矩阵乘法（标准 & Strassen）并做对比**：
   - 编程实现两种算法，给若干输入，比一比“基本操作的步数”，体会它们的差别。

这些是留给你思考和实践的题，不是 PPT 里详细讲的算法。

------

## 总结一下整章的“算法线索”

按 PPT 顺序，我们见到的主要算法和典型案例是：

1. **整数相乘：**
   - 竖式乘法
   - 分治乘法（拆成高低位 a,b,c,d）
   - Karatsuba 分治乘法（3 次子乘法 + 加减法）
2. **矩阵相乘：**
   - 三重循环标准算法
   - 分块分治乘法（4 块 8 次乘法）
   - Strassen 算法（7 次乘法 + 额外加减法）
3. **同时找最大值和最小值：**
   - 直接两遍扫描
   - 分治：左右两半各求 MaxMin，再合并成全局 Min/Max
4. **多数元素问题：**
   - 暴力：对每个元素数一数出现次数
   - 分治：左半、右半递归求候选，再在全局验证
   - 计数：用辅助数组统计每个值的次数，线性时间解决
5. **树枝生长 / Fibonacci：**
   - 把树枝问题抽象成 Fibonacci 递推 F(n)=F(n-1)+F(n-2)
   - 多种求 Fibonacci 的算法：递归、数组 DP、两个变量迭代、通项公式、矩阵法等
   - 更复杂的“母牛”问题：高阶递推、可以用类似思想处理

整份 PPT 的目的，是用这些例子帮你**从宏观上回顾“分治思想 + 递推建模 + 多种算法比较”**。

如果你想，我可以帮你**把某一个具体算法（比如 Karatsuba、Strassen、分治找 MaxMin、或者多数元素计数法）单独挑出来，用举例 + 图示的方式，带你“手工跑一遍”它的执行过程**，这样会更直观。



# a03-3

这章主要是在讲两件事：

1. **概率分析（probabilistic analysis）**：输入是“随机的”时，我们怎么分析一个确定性算法的“平均表现”。
2. **随机算法（randomized algorithms）**：算法自己掷骰子／用随机数，把“坏输入”打乱，变成对任何输入都“通常表现不错”的算法。

PPT 用几个具体的故事来说明这些概念，我按 PPT 顺序给你讲一遍，重点放在“算法和例子”，不讲复杂的数学推导和复杂度。

------

## 5.1 助手招聘问题（The hiring problem）

### 场景（P3–P5）

- 你要在一个月里招聘一位**办公室助理**。
- 有一家猎头公司每天给你推荐 1 个候选人：第 1 天候选人 1，第 2 天候选人 2，…，一直到候选人 n。
- 你对当天来的候选人进行**面试**，然后必须立即决定：
  - 要不要录用他作为当前助理？
  - 如果录用，就要把原来的助理开掉。
- 费用设定：
  - 面试一次：付给猎头公司 1K
  - 录用一次：付 10K（包含开掉原来助理 + 招聘费用）

**目标问题：**

> 采用下面这个“看见更好就立刻换”的策略，一共要花多少钱？（尤其是要雇用几个人）

------

### 算法：HIRE-ASSISTANT（P6–P7）

伪代码逻辑：

```text
best ← 0       // 0 号是假人，能力最差
for i ← 1 to n:
    面试候选人 i
    如果 i 比 best 更好:
        best ← i
        雇用 i 作为新助理（开掉原来的）
```

用白话讲：

1. 先假装有个“能力为 0 的助理”。
2. 依次面试 1..n 号候选人：
   - 如果当前人比现在的助理好，就换成他。
   - 否则就只面试、不录用。
3. 最后留下来的就是这 n 个人中最优秀的那一个。

**这是一个确定性算法**：同一批候选人按同样顺序来，结果永远一样。

------

### 最坏情况／最好情况的直觉（P8–P9）

- **最坏情况：**

  - 候选人的能力是严格递增的：1 号最弱，2 号更强… n 号最强。
  - 每来一个都比当前好，所以 1..n 都会被录用一次。
  - 这样“招聘次数 = n”，钱花得最多。

- **最好情况：**

  - 一开始就遇到最强的，比如 1 号就是最强。
  - 那么只会在第一天录用一次，以后再也不用换。
  - “招聘次数 = 1”。

- 我们更关心的是“平均情况”：

  > 在候选人顺序是**随机排列**的前提下，平均会雇多少次新助理？

这就引出了**概率分析**。

------

## 5.1.2 概率分析（P10–P11）

做概率分析时，课件假设：

- 这 n 个候选人本身有一个真实的“排名” rank(·)（第几优秀）；
- 他们到来的顺序是对 1..n 的**一个随机排列**，每种排列出现的概率都相同（均匀随机）。

**概率分析的本质：**

> 我们假设输入是随机的（例如候选人顺序是随机），
>  然后对算法的表现（比如雇多少次）求“期望值 = 平均次数”。

这一节不要求你把期望算出来，只要理解：

- 为了做平均分析，需要了解或假设“输入的分布”。
- 第 5.2 节会教你一个小技巧：**指示随机变量**，帮忙算这个“平均雇人次数”。

------

## 5.1.3 随机算法思想（Randomized algorithms，P12–P15）

问题来了：现实中，**输入未必真是随机的**，甚至有些“对手”会故意给你最坏的输入。

于是我们换个思路：

> 与其指望“输入本身是随机的”，
>  不如在**算法内部自己做随机化处理**，把输入打乱成随机顺序。

在招聘问题里：

- 猎头给你一份候选人列表（顺序可能很糟糕）。
- 你自己先用随机数**打乱这个列表的顺序**；
- 然后按这个随机顺序调用同样的 HIRE-ASSISTANT 算法。

这样得到一个新的算法：**RANDOMIZED-HIRE-ASSISTANT**

```text
RANDOMIZED-HIRE-ASSISTANT(n):
    随机打乱候选人数组
    调用 HIRE-ASSISTANT(n)
```

特点：

- 算法每次运行都会用随机数打乱顺序，所以**同一个输入，多次运行结果不一定一样**（雇的人次不同）。
- 但是：**不存在某个固定输入一定导致最坏情况**，因为我们每次都重新洗牌。
- 只有当随机数很“倒霉”（比如正好洗出递增排列）时才会花很多钱，但这种情况概率很低。

------

### 随机数产生器的小算法（P15）

PPT 顺便介绍了一个简单的“伪随机数生成算法”（线性同余）：

```text
X0 = 给定种子
Xi+1 = (a * Xi) mod M
```

每次把上一次的结果乘以 a 再对 M 取模，就得到一个新整数。
 虽然这本质上是**完全确定性的**，但只要参数选得好，生成的数字序列看起来“足够随机”，可以用来写 RANDOM(a,b) 之类的函数。

------

## 5.2 指示随机变量（Indicator random variables，P17–P21）

这一节是一个**概率分析的小工具**，主要用在抛硬币、招聘问题这类场景里。

### 定义（P17）

对某个随机事件 A，定义一个随机变量：

```text
I(A) = 1  如果事件 A 发生
I(A) = 0  如果 A 不发生
```

叫做**指示随机变量**。

比如：

- A = “硬币朝上是正面”
- 那么 I(A) = 1（正面），0（反面）

**重要性质：**

> 这个随机变量的期望值 E[I(A)] = 事件 A 发生的概率 Pr(A)。

（你只要记住这点结论，不用推证明）

------

### 简单例子：投硬币（P18–P21）

1. **投一次硬币，正面次数的期望**

   - 事件 H = “正面朝上”
   - 指示变量 X = I(H)，X 要么是 0，要么是 1
   - E[X] = Pr(H) = 1/2

   所以“投一次硬币，正面次数的平均值”就是 1/2。

2. **投 n 次硬币，正面次数的期望**

   - 对第 i 次投掷，定义 Xi = I{第 i 次结果是正面}
   - 总正面次数 X = X1+…+Xn
   - 每个 E[Xi] = 1/2
   - 所以 E[X] = E[X1+…+Xn] = E[X1]+…+E[Xn] = n·(1/2) = n/2

**直观理解：**

> **指示变量 = “发生就算 1，不发生算 0”**，
>  把 n 次实验中代表同一类事情的指示变量加起来，就等于“发生的总次数”；
>  它们的期望很好算，因为 E[I(A)] 就是事件的概率。

下面就用这个招数回到招聘问题。

------

## 把指示变量用在招聘问题上（P22–P27）

### 第一步：把“雇用次数”写成若干个 0/1 相加（P22–P23）

- 定义 Xi = I{第 i 个候选人被录用}
  - 如果第 i 号候选人被你录用了，则 Xi = 1
  - 否则 Xi = 0
- 总录用次数 X = X1 + X2 + … + Xn

于是：

> E[X] = E[X1] + … + E[Xn]
>  = Pr(1 被录用) + Pr(2 被录用) + … + Pr(n 被录用)

所以关键变成：

> **求：第 i 个候选人被录用的概率是多少？**

------

### 第二步：第 i 个会被录用的条件（P24–P25）

注意到第 i 个人被录用的充要条件：

> 他必须比前面所有 1..i-1 号都优秀（即“目前为止最好的人”）。

如果候选人顺序是**完全随机打乱的**，那么在前 i 个候选人中：

- 每个人都有**同样的概率**成为这 i 个里的最强者；
- 所以“第 i 个是前 i 个里最强者”的概率就是 1/i。

因此：

> E[Xi] = Pr{i 被录用} = 1/i

最后就可以把 E[X] 写成：

> E[X] = 1 + 1/2 + 1/3 + … + 1/n ≈ ln n

老师举的例子（n=30）：

- 如果完全最坏，每天都换人：要雇 30 个，成本很高。
- 但在随机顺序下，**平均只会雇 ~ln 30 ≈ 3.4 次**，
- 也就是“平均只多换 3～4 次人”，相对便宜很多。

你不必记住具体数字，只要知道：

> 利用指示变量，把“总录用次数”拆成很多个 0/1 的和，
>  然后求每个人被录用的概率，就能算出平均录用人数。

------

## 5.3 随机算法（Randomized algorithms，P28–P30）

PPT 在这里强调：

- 原始的 HIRE-ASSISTANT 算法是**确定性的**：
  - 对于一个固定的候选人顺序，录用次数是固定的。
  - 有些特别糟糕的顺序必然会导致“雇很多次”，这是算法本身的弱点。
- 如果我们把**随机化写进算法**（比如先随机打乱候选人数组），则：
  - 输入本身不再重要（甭管对手怎么给）
  - 每次运行的实际花费取决于“随机数是否走运”；
  - 坏情况只在随机数很“倒霉”时发生，概率通常比较小。

**随机招聘算法：**

```text
RANDOMIZED-HIRE-ASSISTANT(n):
    调用 RANDOMLY-PERMUTE 把候选人顺序随机打乱
    再调用 HIRE-ASSISTANT(n)
```

PPT 说明：

> 打乱之后的情况，和前面做“输入是随机排列”的概率分析完全一样，
>  所以它的“期望雇用次数”仍然是 ~ln n。

------

## 5.3.2 随机打乱数组（Randomly permuting an array，P31–P41）

这一部分讲的是**两个具体的随机打乱算法**，应用很多：洗牌、随机测试数据、OJ 上的防卡输入等等。

### 目标（P31–P32）

给一个数组 A = ⟨1,2,…,n⟩，我们想要一个算法：

> 把它随机打乱成某个排列，
>  并且 **n! 个排列中每一个出现的概率都一样**（均匀随机打乱）。

------

### 算法一：PERMUTE-BY-SORTING（P33）

**思路：**

1. 给数组 A 中的每个元素 A[i] 生成一个随机“优先级” P[i]。
   - 例如 A = ⟨1,2,3,4⟩，P 随机生成为 ⟨36,3,97,19⟩。
2. 按照 P[i] 的大小对 A 的元素排序（相当于给每个元素抽了一张“随机权重”牌，然后按权重从小到大排）。
3. 得到排序后的新数组 B，比如 ⟨2,4,1,3⟩。

伪代码：

```text
PERMUTE-BY-SORTING(A):
    n = length(A)
    for i = 1..n:
        P[i] = RANDOM(1, n^3)    // 随机优先级
    按照 P 的大小对 A 做排序
    返回 A（现在已经是打乱后的）
```

- 用 1..n³ 的范围，是为了让 P[i] “很大概率”都不相同（避免优先级重复）。
- 缺点是：
  - 需要排序（时间略慢）
  - 需要额外的优先级数组 P。

老师说：**这个方法可以实现随机打乱，但不是最优雅的。**

------

### 算法二：RANDOMIZE-IN-PLACE（Fisher–Yates 洗牌，P34）

这个是实际中非常常用的“在原数组中就地洗牌”算法。

伪代码：

```text
RANDOMIZE-IN-PLACE(A, n):
    for i = 1..n:
        j = RANDOM(i, n)        // 在 [i..n] 中随机选一个下标
        swap(A[i], A[j])        // 交换 A[i] 和 A[j]
```

**直观过程：**

- 第 1 轮（i=1）：
  - 从所有 n 个位置中随机挑一个 j，把 A[1] 和 A[j] 交换。
  - 于是 A[1] 上放的是随机选的一张牌。
- 第 2 轮（i=2）：
  - 只在 A[2..n] 里随机挑一个 j，交换 A[2] 和 A[j]。
  - 这样在第二轮之后，A[1],A[2] 都是“固定”的，不再改变。
- …
- 第 i 轮：
  - 从 A[i..n] 中随机挑一个位置跟 A[i] 换。
  - 之后不会再动 A[i]。

直到 i 走到 n，整个数组就被“洗牌”完成。

**优点：**

- 不需要排序，所以代码简单、速度快。
- 不需要额外数组，只在原地交换。
- 每一种排列出现的概率都相同（PPT 后续用循环不变量证明了这一点，你知道存在这个证明即可）。

**应用：**

- 洗纸牌 / 洗扑克牌的程序。
- 做算法题时，先把输入数组随机打乱一下，可以避开某些对你算法特别不利的“卡数据”。
- 构造随机测试用例。

------

## 5.4 课后思考题（P42）

最后一页列了几道练习，都是围绕“随机数和招聘问题”的：

- 已知一个偏置随机数生成器 RAND-p()（生成 0 的概率是 p，生成 1 的概率是 1-p），
   构造一个新的 RAND-01()，让它生成 0/1 的概率都为 1/2。
- 招聘问题中：
  - 恰好只雇 1 个人的概率是多少？
  - 恰好雇了 n 个人的概率是多少？
  - 恰好雇了 2 个人的概率是多少？

这些需要一些概率推导和编程实验，不在你“只关心算法思路”的范围内，就不展开了。

------

## 全章小结（只从“算法和例子”的角度）

按 PPT 顺序，这章主要通过几个例子说明：

1. **助手招聘问题（确定性算法 HIRE-ASSISTANT）**
   - 顺序给人，看到更优秀就换掉原来的。
   - 可以做最坏情况、平均情况分析。
2. **概率分析 & 指示随机变量**
   - 把“总发生次数”写成很多个 0/1 变量之和，简化期望的计算。
   - 用它求“平均雇用次数”。
3. **随机算法的思想**
   - 当输入分布未知时，在算法里自己做随机化。
   - 在招聘问题中：先随机打乱候选人顺序，再用同样的确定性策略，可避免固定的最坏输入。
4. **随机打乱数组的两个具体算法**
   - PERMUTE-BY-SORTING：给每个元素随机优先级，再按优先级排序。
   - RANDOMIZE-IN-PLACE（洗牌算法）：从前往后，位置 i 随机和后面任意位置交换。

整个章节就是：

> 一边用“招聘”和“洗牌”的例子，演示随机算法；
>  一边用硬币、指示随机变量，演示怎么对这些算法做概率上的平均分析。

如果你愿意，下一步我可以**帮你手动画一两个小例子**，比如：

- 给一组具体候选人能力，演示 HIRE-ASSISTANT 和 RANDOMIZED-HIRE-ASSISTANT 如何雇人；
- 或者拿一个长度为 5 的数组，完整跑一遍 RANDOMIZE-IN-PLACE，看每一步怎么交换。



# a04

这份 PPT 把**排序和顺序统计量**这一块做了一个大集合，按顺序讲了：

- 第 6 章：堆排序 & 优先队列
- 第 7 章：快速排序（包括随机快排）
- 第 8 章：线性时间排序 & 比较排序的下界
- 第 9 章：中位数和顺序统计量（含随机选择算法）

你要的是“讲算法 + 典型案例和解法，不细讲复杂度推导”，我就按 PPT 顺序，把所有出现的算法和典型问题用中文串起来，复杂度只报结论。

------

## 一、第 6 章 Heapsort（堆排序）

### 6.1 堆（heap）的概念（P3–P6）

**1. 堆是什么？**

- PPT 上说的 **二叉堆（binary heap）** 是一种**完全二叉树**：
  - 除了最后一层外，其它层都是满的，
  - 最后一层的节点从左往右连续摆放。
- 用数组 $A[1..n]$ 就能表示这棵树：
  - 下标 $i$ 的左孩子是 $2i$，右孩子是 $2i+1$，父亲是 $\lfloor i/2 \rfloor$（在图 6.1 上有示意）。

**2. 最大堆 / 最小堆**

- **最大堆（max-heap）**：对每个非根节点 $i$，都有
   $$A[\text{PARENT}(i)] \ge A[i]$$
   也就是说，父亲的值永远不比孩子小，因此**根节点一定是整个堆里最大的元素**。
- **最小堆（min-heap）**：反过来，父亲结点的值不大于孩子结点，根是最小的元素。

PPT 之后所有的代码都以“最大堆”为例。

------

### 6.2 维护堆性质：`MAX-HEAPIFY`（P7–P8）

**问题：**
 已经有一棵“几乎是最大堆”的树：左右子树都是最大堆，但是根结点 $i$ 可能比某个孩子小，打破了最大堆性质。怎么“修好”它？

**算法 `MAX-HEAPIFY(A, i)`：**

1. 找出下标 $i$、左孩子 $\text{LEFT}(i)$、右孩子 $\text{RIGHT}(i)$ 中值最大的那个位置，把它记作 `largest`。
2. 如果 `largest` 不是 $i$（说明 $i$ 比某个孩子小）：
   - 交换 $A[i]$ 和 $A[\text{largest}]$；
   - 然后在 `largest` 的位置继续递归调用 `MAX-HEAPIFY`。

直观理解（P7、P8 上的堆图）：

> 把 $A[i]$ 这颗“值比较小”的结点往下“沉”，一路和更大的孩子交换，
>  直到它不再小于孩子，整棵树重新满足最大堆性质。

这个操作的时间和树的高度成正比，堆高 $\lg n$，所以一次 `MAX-HEAPIFY` 的时间是 $O(\lg n)$。

------

### 6.3 建堆：`BUILD-MAX-HEAP`（P3–P4, P9–P10）

**目标：**
 给定一段无序数组 $A[1..n]$，一次性把它“整理”成一个最大堆。

**关键观察（P9）：**

- 数组的后半段 $A[\lfloor n/2 \rfloor + 1 .. n]$ 全是 **叶子节点**，每个叶子自己就是一个最大堆。
- 只要从最后一个非叶子开始，往前一个一个调用 `MAX-HEAPIFY`，就能自底向上把整棵树修成最大堆。

**算法 `BUILD-MAX-HEAP(A)`：**

1. 把 `A.heap-size` 设置成 `A.length`（堆大小 = 元素总数）。
2. 从 $\lfloor n/2 \rfloor$ 到 $1$ 递减：
   - 对每个 $i$ 调用 `MAX-HEAPIFY(A, i)`。

直观图（P4、P9）：

- 象棋盘上从下往上一层一层“收拾”孩子，保证每个父节点比两个孩子都大，最后整棵树就是最大堆。

时间结论：建堆的总时间是 $O(n)$（不是 $O(n\lg n)$）。PPT 用“高度分布”的图解释：大部分节点高度很小。

------

### 6.4 堆排序算法 HEAPSORT（P3–P5, P11）

堆排序的框架非常简单（P3、P4 页的大图）：

1. `BUILD-MAX-HEAP(A)`：把数组变成最大堆。
2. 循环从尾巴往前走：
   - 把堆顶（最大元素）$A[1]$ 和当前堆尾 $A[i]$ 交换——此时 $A[i]$ 已经放在了最终位置；
   - `heap-size` 减一，把那个最大元素从堆里“删掉”；
   - 对新的根结点调用 `MAX-HEAPIFY(A, 1)`，恢复堆性质。

重复直到堆只剩一个元素，整个数组就从小到大排好序了（P11 底部的例子：输入是任意序，输出为 $1,2,3,4,7,8,9,10,14,16$）。

**复杂度结论：**
 堆排序总时间 $\Theta(n\lg n)$，原地排序，不需要额外大数组。

------

### 6.5 堆实现优先队列（P12–P16）

PPT 的图片举了排队购票、飞机起降、道路指示牌等场景来说明：堆最常见的应用是 **优先队列（priority queue）**——谁优先级高谁先处理。

对 **最大优先队列**，我们需要四个操作：

1. `HEAP-MAXIMUM(A)`：
   - 直接返回 $A[1]$（根）——最大元素。
   - 时间 $\Theta(1)$。
2. `HEAP-EXTRACT-MAX(A)`： (P14)
   - 取出堆顶最大值 `max = A[1]`；
   - 把堆尾元素挪到根上，`heap-size--`；
   - 对根调用 `MAX-HEAPIFY(A,1)`。
   - 既**返回最大值**，又把它从集合中删去。
   - 时间 $O(\lg n)$。
3. `HEAP-INCREASE-KEY(A, i, key)`： (P15)
   - 把第 $i$ 个元素的关键字从旧值增大到 `key`；
   - 新值不能比旧值小（否则报错）；
   - 然后像“气球上浮”一样，只要父亲节点比它小，就交换它和父亲，一直往上冒。
   - 时间 $O(\lg n)$。
4. `MAX-HEAP-INSERT(A, key)`： (P16)
   - `heap-size++`，在堆尾放一个 $-\infty$ 的临时值；
   - 再调用 `HEAP-INCREASE-KEY` 把它升到真正的 `key`，并自动上浮到合适位置。
   - 时间 $O(\lg n)$。

**总结：**
 堆除了能排序，还能支撑高效的优先队列操作，在后面学**贪心算法、最小生成树、最短路**时都会用到（P5 也提到这些章节）。

------

## 二、第 7 章 Quicksort（快速排序）

PPT 一开头就总结：

- 最坏时间：$\Theta(n^2)$
- 期望时间：$\Theta(n\lg n)$
- 实际上在大多数情况下，快排是最实用的排序算法之一。

### 7.1 快排的基本过程：`PARTITION` + 递归（P19）

**核心：分区过程 `PARTITION(A,p,r)`（P19）：**

1. 选取最后一个元素 $A[r]$ 作为 **枢轴（pivot）**。
2. 设 $i = p-1$；然后从 $j=p$ 到 $r-1$ 扫描数组：
   - 如果 $A[j] \le A[r]$：
     - 先让 $i$ 自增一位，
     - 然后交换 $A[i]$ 和 $A[j]$。
3. 扫描结束后，交换 $A[i+1]$ 和 $A[r]$。

这样就保证：

- $A[p..i]$ 里所有元素 $\le$ 枢轴；
- $A[i+1]$ 就是枢轴最终位置；
- $A[i+2..r]$ 里所有元素 $>$ 枢轴。

图 7.1 上那一列小方格（a→f）就是展示 i、j 的移动和每步交换的效果。

**快速排序 `QUICKSORT(A,p,r)`：**

1. 若 $p < r$：
   - $q = \text{PARTITION}(A,p,r)$：把 $A[p..r]$ 分成“小于等于”一侧和“大于”一侧；
   - 递归排左半：`QUICKSORT(A, p, q-1)`；
   - 递归排右半：`QUICKSORT(A, q+1, r)`。

每一次分区都把**一个元素放到了最终正确位置**（图中星号标出来），然后对左右剩下的部分递归排序。

------

### 7.2 “分区形状”与时间复杂度（P20–P24）

这里 PPT 用几种 $T(n)$ 递推式来讨论不同分区效果（只报结论）：

1. **最坏分区（极端不平衡）**：
   - 每次分区都把枢轴放到最左或最右，只剩下 $n-1$ 个元素继续递归：
   - $$T(n) = T(n-1) + \Theta(n) \Rightarrow \Theta(n^2)$$
   - 比如：数组已经有序，而你总拿第一个/最后一个当枢轴。
2. **最好分区（完全平衡）**：
   - 每次都刚好一分为二：
   - $$T(n) = 2T(n/2) + \Theta(n) \Rightarrow \Theta(n\lg n)$$
3. **一般的“较平衡分区”**：
   - 例如 $T(n) = T(9n/10) + T(n/10) + \Theta(n)$ 等弱不均匀的情况（P23 那棵树说明最小/最大高度）；
   - 依然可以证明时间是 $\Theta(n\lg n)$。
4. **平均情况：**
   - PPT 上画了一个直观解释：假设**好分区和坏分区交替出现**，
   - 那么递归树高度大约 $2\lg n$，每层做 $\Theta(n)$ 量级的工作 → 总体仍是 $O(n\lg n)$（P24）。

------

### 7.3 随机快排 `RANDOMIZED-QUICKSORT`（P25）

为了避免“总是碰到最坏分区”的坏运气，PPT 引入了**随机化快排**：

- 把 `PARTITION` 替换为 `RANDOMIZED-PARTITION`：
  - 先在区间 $[p,r]$ 内**随机选一个下标 $i$**；
  - 交换 $A[i]$ 和 $A[r]$，然后再调用普通 `PARTITION(A,p,r)`。

这样，每次选的枢轴都是随机的：

> 任意一个元素都有同样的机会当枢轴，
>  所以“总碰上最坏情况”的概率非常低，
>  期望运行时间是 $\Theta(n\lg n)$。

------

### 7.4 随机快排的期望分析（P26–P32）

这一大段 PPT 其实是在用**指示随机变量**非常细致地推 $\mathbb{E}[T(n)]$，你只用抓住两个核心直觉：

1. **最坏情况仍然是 $\Theta(n^2)$**，因为如果随机数特别倒霉，总能构造出接近单边分区的递归。
2. **但平均每一层“被比较到的元素对”不多，总体的比较次数是 $O(n\lg n)$。**

P28～P31 页的思路是：

- 把所有元素记为 $z_1,\dots,z_n$，
- 定义指示变量 $X_{ij} = I{z_i \text{ 和 } z_j \text{ 在某一次分区中被比较}}$；
- 快排总比较次数 $X = \sum_{i<j} X_{ij}$；
- 利用“任意两个元素最多比较一次”的事实（P30 的图），算出 $\mathbb{E}[X_{ij}]$ 较小；
- 最后得到 $\mathbb{E}[X] = O(n\lg n)$。

P32 又给了一个更形象的说法：

> 第 1 次分区，确定 1 个元素位置；
>  第 2 次分区，确定 2 个元素位置；
>  第 3 次分区，确定 4 个元素位置；
>  ……
>  第 $k$ 次分区后，约有 $2^k - 1 \approx n$ 个元素位置确定，$k \approx \lg n$；
>  每次分区最多做 $n-1$ 次比较，所以总比较次数是 $O(n\lg n)$。

------

## 三、第 8 章 线性时间排序 & 比较排序下界

### 8.1 线性时间排序的例子（P34–P35）

这一页先强调一个重要区别：

- **比较排序（comparison sort）**：
  - 只通过元素之间的比较（如 $a_i \le a_j$）来确定顺序，比如冒泡、选择、插入、归并、堆、快排等；
  - 对比较排序，有一个下界：任何算法最坏都要至少 $\Omega(n\lg n)$ 次比较，才能保证对任意输入排序成功（最好情况除外）。
- **非比较排序（线性时间排序）**：
  - 利用元素的“数值结构”（比如是小整数），使用数组计数、按位处理等方法；
  - 可以做到线性时间 $O(n)$，不受 $\Omega(n\lg n)$ 下界限制。

PPT 只列出了几个名字（之后课程会展开）：

1. **计数排序（counting sort）**
2. **基数排序（radix sort）**
3. **桶排序（bucket sort）**

它们都利用“元素是在某个有限范围内的整数／均匀分布”等假设来加速。

------

### 8.2 决策树模型 & 比较排序下界（P36–P40）

为了说明“任何比较排序的最坏时间 $\ge \Omega(n\lg n)$”，PPT 引入了**决策树模型**。

**1. 决策树是什么？**

- 对某个固定的比较排序算法，用一棵**满二叉树**表示它在所有可能输入上的比较过程：
  - 每个内部节点代表一次比较“$a_i \le a_j$？”；
  - 左右子树代表不同的比较结果；
  - 每个叶子代表一种排列（排序结果）。
- PPT 在 P36–P37 给了插入排序对 3 个元素的决策树示例：
  - 根比较 “1 和 2”；
  - 接着比较“2 和 3”或“1 和 3”；
  - 最终叶子对应 6 种排列之一。

**2. 排序算法与决策树的关系**

- 对 $n$ 个不同元素，有 $n!$ 种排列。
- 一个正确的比较排序算法必须能在不同输入下给出这 $n!$ 种结果中的任意一种，所以：
  - 决策树的**叶子数 $l$ 至少是 $n!$**。
- 高度为 $h$ 的二叉树，叶子数最多 $2^h$，所以
   $$n! \le l \le 2^h \Rightarrow h \ge \lg(n!)$$
- $\lg(n!)$ 的量级是 $ \Omega(n\lg n)$
   （P40 直接给出结论），
   所以任何比较排序的最坏比较次数都至少是 $\Omega(n\lg n)$。

**结论：**

> 不管你怎么设计比较排序，都不可能在**最坏情况**下比 $n\lg n$ 更好（只看比较次数）。

这也说明了：

- 像堆排、归并排序、平均情况下的快排等都已经是“数量级最优”的比较排序了。

------

## 四、第 9 章 Medians and Order Statistics（中位数与顺序统计量）

### 9.1 顺序统计量 & 找最小值（P41–P43）

**1. 顺序统计量定义（P41）**

- 有一组 $n$ 个数，按从小到大排序后：
  - 第 $1$ 小的是**最小值**：第 1 阶顺序统计量；
  - 第 $n$ 小的是**最大值**：第 $n$ 阶顺序统计量；
  - 中间位置（比如第 $\lceil n/2 \rceil$ 小）的数就是一种**中位数（median）**。

例子（P41 图）：序列 $6, 12, 5, 9, 2, 10, 8, 7$。

- 最小值：$2$
- 最大值：$12$

**2. 一般顺序统计问题：**

> 给一组 $n$ 个不同的数，要找出**第 $i$ 小的数**。

最直接的方法：先排序，再取输出数组的第 $i$ 个，时间 $O(n\lg n)$。
 PPT 问：“能不能做到更快？”——后面给出线性期望时间的算法。

**3. 找最小值的简单算法（P43）**

`MINIMUM(A)` 伪代码：

```text
min = A[1]
for i = 2..n:
    if min > A[i]:
        min = A[i]
return min
```

- 一路扫描，顺便维护当前最小值。
- 比较次数是 $n-1$，时间是 $\Theta(n)$。

------

### 9.2 随机选择：`RANDOMIZED-SELECT`（P44–P46）

这是本章最重要的算法之一：

> 在**期望时间 $\Theta(n)$** 内，从 $n$ 个数中找出第 $i$ 小的元素。

它的思想和快速排序非常像：用**随机分区**来定位目标元素所在的子区间。

**算法 `RANDOMIZED-SELECT(A,p,r,i)`（P44）：**

输入：子数组 $A[p..r]$，要找其中第 $i$ 小的元素。

1. 如果 $p==r$（区间里只有一个元素），直接返回 $A[p]$。
2. 否则做一次随机分区：`q = RANDOMIZED-PARTITION(A,p,r)`：
   - 枢轴 $A[q]$ 会被放在它在整个 $A[p..r]$ 中的**正确排序位置**；
   - 它左边有 $k-1$ 个比它小的元素，其中
      $$k = q - p + 1$$
      所以 $A[q]$ 恰好是“这段区间里的第 $k$ 小”。
3. 比较 $i$ 和 $k$：
   - 若 $i == k$：说明要找的就是枢轴 $A[q]$，直接返回；
   - 若 $i < k$：目标在左半边 $A[p..q-1]$，递归调用
      $$\text{RANDOMIZED-SELECT}(A,p,q-1,i)$$
   - 若 $i > k$：目标在右半边 $A[q+1..r]$，但右边都是比 $A[q]$ 大的数，所以在右半边它是“第 $i-k$ 小”的：
      $$\text{RANDOMIZED-SELECT}(A,q+1,r,i-k)$$

图（P44）画了一条数轴：左边比枢轴小，右边比枢轴大，枢轴在位置 $k$。我们只在其中一边继续递归，因此**每次递归都把问题规模减小了一大块**。

**时间结论（P45–P46）：**

- 最坏情况：每次分区都特别不平衡，会退化到
   $$T(n) = T(n-1) + O(n) = \Theta(n^2)$$
- 但在随机分区下，平均每次分割都比较平衡，
   利用指示随机变量和类似快排的分析，可以证明：
   **期望时间 $\Theta(n)$**。

直观理解（P46）：和随机快排一样，

> 把若干次运行摞在一起考虑，大部分时候分区都不会太糟，
>  在数学期望上只需要常数倍的 $n$ 级工作量。

------

## 五：本章所有算法的复杂度总结（只列结论）

按 PPT 涉及的主要算法，把**时间复杂度结论**帮你整理一下（不推导）：

- **堆排序 HEAPSORT**：最坏时间 $\Theta(n\lg n)$
- **建最大堆 BUILD-MAX-HEAP**：时间 $\Theta(n)$
- **最大优先队列操作**：
  - `HEAP-MAXIMUM`：$\Theta(1)$
  - `HEAP-EXTRACT-MAX`：$O(\lg n)$
  - `HEAP-INCREASE-KEY`：$O(\lg n)$
  - `MAX-HEAP-INSERT`：$O(\lg n)$
- **快速排序 QUICKSORT**：
  - 最坏：$\Theta(n^2)$
  - 最好 / 期望（随机快排）：$\Theta(n\lg n)$
- **比较排序下界**：任何比较排序最坏都至少要 $\Omega(n\lg n)$ 次比较。
- **找最小值 MINIMUM**：$\Theta(n)$
- **随机选择 RANDOMIZED-SELECT**：
  - 最坏：$\Theta(n^2)$
  - 期望：$\Theta(n)$

------

如果你愿意，下一步我可以：

- 单独挑一个你最想搞懂的算法（比如堆排序、快排、随机选择），
- 用一小组具体数字，按“每一步数组长什么样”的方式，带你**手工走完一遍整个算法**，你会更有直观感觉。



# a05-1

这一章主要用**“装配线调度（Assembly-line Scheduling, ALS）”**这个案例，教你什么是**动态规划（DP）**，以及一套设计 DP 算法的固定步骤。

我会按 PPT 顺序来讲：

1. 开头：动态规划在整个课程里的位置
2. 动态规划的一般思想（四个步骤）
3. 典型案例：装配线调度问题
   - 问题模型 & 记号
   - 暴力穷举为什么不行
   - “最优子结构”怎么找
   - 递归方程（转移方程）
   - 自底向上的 DP 算法 `FASTEST-WAY`
   - 如何回溯出一条真正的最优路线 `PRINT-STATIONS`
4. 这一章对动态规划方法论的总结

------

## 一、动态规划在整个课程里的位置（P1–P2）

前面几部分你已经见过很多“设计算法的套路”：穷举、搜索、回溯、分治、随机化……这一章开始的是 **Part IV：高级设计和分析技巧**，第一个主角就是**动态规划（Dynamic Programming）**。

PPT 上有几句重要的话：

- 动态规划**通常用来解决“最优化问题”**：有很多可行方案，要找使代价最小或收益最大的那个。
- 它**不是一个固定算法**，而是一种**通用方法**。
- “programming” 在这里不是写代码，而是“列表法”：用表保存子问题的解。
- 同时对比了一下**贪心算法**：贪心是“每一步做局部最优选择”；而 DP 要考虑全局结构，列很多子问题的解再综合选择。

------

## 二、动态规划的一般四步法（P3–4）

这四步是之后所有 DP 的“母版”，PPT 在 P3、P32 又重复了一次：

1. **刻画最优解的结构**
   - 把“一个最优解”拆成若干子问题的解，看看它们之间有什么关系。
2. **写出最优值的递归定义（转移函数）**
   - 用子问题最优值来表达原问题最优值，写成递归式，例如
      $$f[i] = \min(f[i-1]+\cdots,\ f[i-2]+\cdots)$$
3. **自底向上计算最优值**
   - 按子问题规模从小到大填表，把递归式当成“转移方程”来算。
4. **（可选）从表里反推一个最优解**
   - 如果不仅要最优值，还要“具体怎么做”，就要在填表时记一些路径信息，最后从后往前回溯出一条具体的最优方案。

这一章只用一个例子——**装配线调度 ALS**——把这四步完整走一遍。

------

## 三、典型案例：装配线调度 ALS（P5–7）

### 3.1 问题模型和记号（P5–6）

场景：一辆车的底盘从工厂入口进入，有 **两条装配流水线**：

- 线 1：站点 $S_{1,1}, S_{1,2}, \dots, S_{1,n}$
- 线 2：站点 $S_{2,1}, S_{2,2}, \dots, S_{2,n}$

每个 station 都会对车做一些操作，会花掉一定时间。你可以在中间某些站点**从一条线“跳”到另一条线**，但切换要额外时间。目标是：

> 让这辆车从入口走到出口，**总时间最少**。

PPT 用这些符号描述（见 P5 图）：

- $a_{i,j}$：在装配线 $i$ 上的第 $j$ 个站点 $S_{i,j}$ 处理所需时间。
- $t_{i,j}$：从线 $i$ 的站点 $j$ **切换到另一条线的站点 $j+1$** 的时间。
- $e_i$：从入口进入装配线 $i$ 的时间。
- $x_i$：在装配线 $i$ 走完最后一个站点后，到出口的时间。

车的路径必须满足：

- 从入口先选择进入 line 1 或 line 2；
- 每一列（每个工位序号 $j$）恰好经过一个 station：$S_{1,j}$ 或 $S_{2,j}$；
- 可能在列与列之间切换线。

图上右边有一条红色路径，就是某个“最快方案”的示例（P6）。

------

### 3.2 暴力穷举为什么不行（P7）

如果你想用最笨的方法：

- 每一列你要么选 line1，要么选 line2；
- 一共有 $n$ 列，所以可能的方案数是 $2^n$。

每一种方案，如果已知“在哪条线上用哪个 station”，再把每个 $a_{i,j}$ 和切换时间 $t_{i,j}$ 加起来，**算出总时间要 $\Theta(n)$**。

所以穷举算法总体时间是 $\Omega(2^n)$，$n$ 稍大就根本算不完。

P8 页还举了软件工程的例子：

- 写 2048 游戏，假设有 3 个程序员（3 条 line），要实现 40 个函数（40 个 station），如果每个 function 都可以选择任何一个人写，想要暴力试出最优分配，组合数是 $3^{40}$，同样是天文数字。

> 这说明：**直接穷举所有方案不可行，必须利用问题结构。**

------

## 四、Step 1：最优解的结构（P9–16）

### 4.1 定义子问题：到某个 station 的最快时间（P9–11）

为了刻画结构，先关注一个子问题：

> “从入口到达 station $S_{1,j}$ 的最快时间是多少？”

同理还有到 $S_{2,j}$ 的最快时间。

- 当 $j=1$ 时，情况很简单：
  - 只能从入口直接到 $S_{1,1}$ 或 $S_{2,1}$，没有更早的 station。
- 当 $j\ge2$ 时，到 $S_{1,j}$ 有两种可能（见 P11 图）：
  1. 上一步在 **$S_{1,j-1}$**，然后直接走到 $S_{1,j}$（不切换线）；
  2. 上一步在 **$S_{2,j-1}$**，再切换到 $S_{1,j}$，要多加 $t_{2,j-1}$ 的切换时间。

> 到 $S_{1,j}$ 的所有路径，最后一步只可能是从 $S_{1,j-1}$ 或 $S_{2,j-1}$ 过来。

对 $S_{2,j}$ 同理：要么从 $S_{2,j-1}$ 过来，要么从 $S_{1,j-1}$ 切过来。

------

### 4.2 “最优子结构”的含义（P12–16）

P12–14 页用图画了一个关键概念：**最优子结构（Optimal Substructure）**。

结论可以用一句话概括：

> 如果一条路径 $p_j$ 是“从入口到 $S_{1,j}$ 的最优路径”，
>  那么它去掉最后一步之后得到的那条路径 $p_{j-1}$，
>  必然是“从入口到它的前一个 station（$S_{1,j-1}$ 或 $S_{2,j-1}$）的最优路径”。

如果不是最优的，就会跟另一条更快的子路径拼接得到更快的整条路径，从而矛盾。

这就是**动态规划能用的前提**之一：

> **原问题的最优解里面包含子问题的最优解。**

装配线调度就满足这个特点，所以适合 DP。

------

## 五、Step 2：递归方程 / 转移函数（P17–21）

### 5.1 定义状态 $f_i[j]$ 和最终目标 $f^*$（P17–18）

设

- $f_i[j]$：从入口出发，**以最快方式到达 station $S_{i,j}$ 的总时间**。
  - $i=1,2$ 表示哪条线；
  - $j=1,2,\dots,n$ 表示第几站。

那么我们真正想要的是：车走完整个装配线后到出口的最短时间 $f^*$，显然就是

$$
 f^* = \min\bigl(f_1[n] + x_1,\ f_2[n] + x_2\bigr) \tag{15.1}
$$

因为最后一步必然是在某条线的第 $n$ 个 station 上完成，然后花 $x_i$ 时间到出口。

------

### 5.2 基本情况：初始站点 $j=1$（P19–20）

从入口直接进入某条线的第一个 station：

- $$f_1[1] = e_1 + a_{1,1} \tag{15.2}$$
- $$f_2[1] = e_2 + a_{2,1} \tag{15.3}$$

因为还没有切换线的可能，只是入口时间 + 这站的加工时间。

------

### 5.3 递推关系：$j\ge2$ 的情况（P19–20）

对于 $j\ge2$：

1. 到 $S_{1,j}$ 的最快时间 $f_1[j]$ 有两种候选：

   - 不切换线：从 $S_{1,j-1}$ 直接来
      $$ f_1[j] = f_1[j-1] + a_{1,j} $$

   - 从线 2 切过来：先走到 $S_{2,j-1}$，再切换到 $S_{1,j}$
      $$ f_1[j] = f_2[j-1] + t_{2,j-1} + a_{1,j} $$

   - 取较小者：
     $$
      f_1[j] = \min\bigl(f_1[j-1] + a_{1,j},\ f_2[j-1] + t_{2,j-1} + a_{1,j}\bigr) \tag{15.4}
     $$

2. 到 $S_{2,j}$ 的 $f_2[j]$ 同理：
   $$
    f_2[j] = \min\bigl(f_2[j-1] + a_{2,j},\ f_1[j-1] + t_{1,j-1} + a_{2,j}\bigr) \tag{15.5}
   $$

把 $j=1$ 的基本情况和 $j\ge2$ 的递推情况合并起来，就是：
$$
f_1[j] =
 \begin{cases}
 e_1 + a_{1,1}, & j=1 \\
 \min\bigl(f_1[j-1] + a_{1,j},\ f_2[j-1] + t_{2,j-1} + a_{1,j}\bigr), & j\ge2
 \end{cases}\tag{15.6}
$$

$$
f_2[j] =
 \begin{cases}
 e_2 + a_{2,1}, & j=1\\
 \min\bigl(f_2[j-1] + a_{2,j},\ f_1[j-1] + t_{1,j-1} + a_{2,j}\bigr), & j\ge2
 \end{cases}\tag{15.7}
$$

这两个式子就是这个 DP 的**转移函数（Transition Function）**。

P21 页给了一个完整的数值例子，把 $f_1[j],f_2[j]$ 一列一列算出来，最后得到 $f^* = 38$。

------

### 5.4 除了最优值，还想要“最优路线”怎么走？（P22–23）

上面算出来的 $f_i[j]$ 只是“最快时间”，但图上那条**红色路径**（选了哪些 station）也很重要。为此 PPT 又定义了一组辅助数组 $l_i[j]$：

- $l_i[j]$：在**最快路径**中，到达 $S_{i,j}$ 之前，上一站是**哪条线**的 station $j-1$。
  - 比如 $l_1[5] = 2$ 表示：
     在最快路径里到 $S_{1,5}$ 之前，是从线 2 的 $S_{2,4}$ 切过来的。
- $2\le j\le n$ 时才有 $l_i[j]$，因为 station 1 之前没有 station 0。
- $l^*$：最终使用第 $n$ 个 station 的那条线的编号（1 或 2）。

有了 $l^*$ 和所有 $l_i[j]$，就可以从最后一个 station 往前追溯：

- 先看 $l^*$，知道最后一站是在 line 1 还是 line 2；
- 再根据 $l_i[n]$ 找到前一站是哪个 line 的 $S_{\cdot,n-1}$；
- 一直追到 $j=1$ 为止，就拿到一条完整的最优路线。

P23 页用示例填了 $l_1[j], l_2[j]$ 的表，并写出回溯顺序：

> $l^*=1$ → 用 $S_{1,6}$
>  $l_1[6]=2$ → 之前是 $S_{2,5}$
>  $l_2[5]=2$ → 再之前是 $S_{2,4}$
>  ……
>  $l_2[2]=1$ → 最开始是 $S_{1,1}$

------

## 六、Step 3：算法设计 —— 递归版 vs 动规版（P24–29）

### 6.1 直接按递归方程写的“纯递归算法”（P24–25）

如果你直接按照 (15.1)、(15.6)、(15.7) 写一个递归函数，每次求 $f_i[j]$ 都递归调用 $f_1[j-1]$ 和 $f_2[j-1]$，会发生什么？

PPT 分析了每个 $f_i[j]$ 被调用的次数 $r_i(j)$，得到：

- $r_1(n) = r_2(n) = 1$；
- $r_1(j) = r_2(j) = r_1(j+1) + r_2(j+1)$；
- 所以对 $j=1,2,\dots,n-1$ 有 $r_i(j) = 2^{,n-j}$；
  - 光是 $f_1[1]$ 就会被算 $2^{n-1}$ 次！

总调用次数为 $\Theta(2^n)$，即使每次计算很快，总时间也是**指数级 $O(2^n)$**，还是不现实。

> 这说明：仅仅写出递归式还不够，要换一种计算顺序——这就是动态规划的核心。

------

### 6.2 动态规划思想：自底向上填表（P26–29）

关键观察（P26）：

> 对于 $j\ge2$，$f_i[j]$ 只依赖于 $f_1[j-1]$ 和 $f_2[j-1]$（上一列）。

所以我们可以按 **station 序号 $j$ 从小到大**来算：

1. 先利用基本式 (15.2)、(15.3) 算出 $f_1[1],f_2[1]$；
2. 对 $j=2,3,\dots,n$：
   - 用 (15.6)、(15.7) 计算 $f_1[j],f_2[j]$；
   - 同时，根据“哪个分支更小”来记录 $l_1[j],l_2[j]$（1 表示从本线来，2 表示从另一条线切换来）；
3. 最后比较 $f_1[n]+x_1$ 和 $f_2[n]+x_2$ 得到 $f^*$ 和 $l^*$。

这就是 PPT 上的 `FASTEST-WAY(a,t,e,x,n)` 算法（P27–29）。伪代码大致是：

```text
FASTEST-WAY(a, t, e, x, n)
    f1[1] = e1 + a1,1
    f2[1] = e2 + a2,1
    for j = 2..n:
        // 计算 f1[j], l1[j]
        if f1[j-1] + a1,j <= f2[j-1] + t2,j-1 + a1,j:
            f1[j] = f1[j-1] + a1,j
            l1[j] = 1
        else:
            f1[j] = f2[j-1] + t2,j-1 + a1,j
            l1[j] = 2

        // 计算 f2[j], l2[j]
        if f2[j-1] + a2,j <= f1[j-1] + t1,j-1 + a2,j:
            f2[j] = f2[j-1] + a2,j
            l2[j] = 2
        else:
            f2[j] = f1[j-1] + t1,j-1 + a2,j
            l2[j] = 1

    if f1[n] + x1 <= f2[n] + x2:
        f* = f1[n] + x1
        l* = 1
    else:
        f* = f2[n] + x2
        l* = 2
```

**核心特点：**

- 每个 $f_i[j]$ 只算一次，结果存表里；
- 后面用到时直接查表，而不是再次递归计算；
- 整个算法的时间是 $\Theta(n)$（对每一列只做常数次操作）。

这就是典型的 **动态规划：自底向上填表 + 每一步做“最优选择”**。

------

## 七、Step 4：从表中回溯一条最优路径（P30）

`FASTEST-WAY` 算完后，我们已经有：

- 所有 $f_i[j]$：每个 station 的最快到达时间；
- 所有 $l_i[j]$：最快路径中每一步来自哪条线；
- $f^*$：整体最短时间；
- $l^*$：最后一站在第几条线。

现在只缺一件事：输出**具体要走的站点序列**。

P30 提供了一个过程 `PRINT-STATIONS(l,n)`：

```text
PRINT-STATIONS(l, n)
    i = l*
    print "station", n, "line", i
    for j = n down to 2:
        i = l_i[j]
        print "station", j-1, "line", i
```

- 先打印最后一站 $(n, l^*)$；
- 然后从 $j=n$ 递减到 $2$，每次用 $l_i[j]$ 找到前一站在第几条线；
- 这样输出的顺序是 **从后往前**：$n, n-1, \dots, 1$。

P36 的练习 3 还问：如果想**按 station 递增顺序**输出（从 1 到 n），怎么改？

- 典型做法是先把这些 station 存到一个数组 / 栈里，再反向输出。

------

## 八、“最优值” vs “最优解”（P31）

P31 专门强调一个概念：

- **最优值（optimal value）**：目标函数的最好数值，比如这题中 $f^* = 38$。
- **一个最优解（an optimal solution）**：达到这个最优值的一条具体路线（可能不止一条）。

在装配线问题里：

- 表格里的 $f_1[j], f_2[j], f^*$ 给出了**最优时间**；
- 图中的那条红线（由 $l_i[j], l^*$ 回溯出来）是“众多最优解中的一个”。

有时实际应用只关心最优值（比如最小成本是多少），那可以只做前 3 步，不用做“回溯”。

------

## 九、动态规划方法论总结（P32）

PPT 最后再把动态规划的四步法写了一遍，并说明：

1. 先找 **最优解的结构**（有无最优子结构）；
2. 写出 **递归方程 / 转移函数**；
3. 再设计一个 **自底向上的算法** 按这个方程填表；
4. 如果需要，还要设计一个 **回溯方案的过程**，从表格信息中恢复一条最优路径。

对这章的 ALS 问题来说：

- **算法名字**：
  - 动态规划算法 `FASTEST-WAY`（求最优值 + 辅助数组）
  - 回溯输出算法 `PRINT-STATIONS`（给出一个最优调度方案）
- **结果复杂度**：
  - 递归直接算：指数级 $\Theta(2^n)$（不可行）
  - 动态规划：$\Theta(n)$ 时间、$O(n)$ 额外空间

------

## 十、整章小结（从“讲了什么算法”角度）

按 PPT 顺序，这一章做了这些事：

1. 说明动态规划是一种通用 **“列子问题表 + 每一步选择最优转移”** 的方法。
2. 用 **汽车装配线调度 ALS** 作为完整案例：
   - 暴力穷举有 $2^n$ 种路径，不可行；
   - 发现最优解具有**最优子结构**，所以适合 DP；
   - 定义状态 $f_i[j]$（到 $S_{i,j}$ 的最短时间）；
   - 写出递归方程 $(15.2)–(15.7)$；
   - 给出自底向上的 DP 算法 `FASTEST-WAY`，时间 $\Theta(n)$；
   - 定义 $l_i[j], l^*$ 并设计 `PRINT-STATIONS`，从表中恢复具体最优路线。
3. 借此把 **动态规划四步法**（结构 → 递归式 → 填表 → 回溯解）讲清楚。

如果你愿意，下一步我可以帮你**拿 PPT 里的那组具体数字**（那张写了很多 7,9,8,5… 的图），一步一步用表格形式带你**手算出 $f_i[j]$ 和 $l_i[j]$，最后画出完整最优路径**，你就能完全“看懂表格和红线是怎么来的”。



# a05-2

这一章是在用“**钢管（钢条）切割**”这个例子，继续练动态规划：
怎么从一个**指数级的递归算法**，一步步改成**高效的 DP 算法**，还能把**最优切割方案**打印出来。

我按 PPT 顺序讲一遍。

---

## 1. 问题背景：钢管切割（P2–5）

假设有一根长度为 (n) 米的钢管，要按市场价切成若干段卖。
给出一张“长度–价格表”，例如（P2–3 的表）：

| 长度 (i)（米）   | 1 | 2 | 3 | 4 | 5  | 6  | 7  | 8  | 9  | 10 |
| ----------- | - | - | - | - | -- | -- | -- | -- | -- | -- |
| 单价 (p_i)（元） | 1 | 5 | 8 | 9 | 10 | 17 | 17 | 20 | 24 | 30 |

问题：

> 钢管长 (n) 米，**怎么切**，才能让总收入最大？

直觉上的例子：

* 钢管长 10 米：**不切**，整根卖 30 元 最划算。
* 钢管长 9 米：可以切成 (3+6) 米，收入 (p_3 + p_6 = 8 + 17 = 25) 元，比整根卖 24 元好。

你可以把这个问题想成：

> “把一个长度为 (n) 的整数分拆成若干个整数，每个长度 (i) 有一个价格 (p_i)，
> 问如何分拆才能让 (\sum p_{i_k}) 最大。”

---

## 2. Step 1：最优子结构（P6–7）

先做“数学建模”。

### 2.1 定义最优收益 (r_n)

* 记 (r_n)：**长度为 (n) 的钢管所能得到的最大收益**。

* 一种切法：把 (n) 切成 (k) 段$n = i_1 + i_2 + \dots + i_k,\quad 1 \le i_j \le n$

  这时得到的收益是$p_{i_1} + p_{i_2} + \dots + p_{i_k}$

在所有切法中取最大，就得到：

$$ r_n = \max\bigl(p_{i_1} + p_{i_2} + \dots + p_{i_k}\bigr) $$

PPT 进一步把它写成一个**更好用的形式**（看 P6–8 的树图会更直观）：

> 只看“**第一刀切多长**”这件事。

假设第一刀切出长度为 (i) 的一段（卖出价 (p_i)），剩下的长度是 (n-i)，对剩下那一段继续“最优切割”，收益就是 (r_{n-i})。

于是：

$$ r_n = \max\bigl(p_1 + r_{n-1},\ p_2 + r_{n-2},\ \dots,\ p_{n-1} + r_1,\ p_n + r_0\bigr) $$

再写紧一点就是经典式子（P9 的式 (15.2)）：

$$ r_n = \max_{1 \le i \le n} \bigl(p_i + r_{n-i}\bigr),\quad r_0 = 0 $$

这就是后面整个算法的“核心公式”。

---

### 2.2 为什么有“最优子结构”？（P7 的例子）

PPT 举了一个例子说明：

* 对长度 7 的钢管，某个**最优切法**是：
  $$ 7 = 2 + 2 + 3,\quad r_7 = p_2 + p_2 + p_3 = 18 $$
* 注意前两段 (2+2) 是一个对“长度 4”的切割。
* 如果 (2+2) 不是“长度为 4 的最优切法”，那么一定存在一个更好的切法（比如 (4) 或 (1+3)），把那种更好的切法替换上去，就能得到比 18 还大的收益，**和“7 的切法已经最优”矛盾**。

所以：

> **一个最优切法里面，前面那一截也必须是该长度的最优切法。**

这就是“最优子结构”的含义——非常适合用动态规划。

---

## 3. Step 2：递归解（P8–15）

### 3.1 直接照公式写递归（P8–12）

根据刚才的递推式，可以写一个最直观的递归定义：

$$
r_n = \begin{cases}
0, & n = 0 
\displaystyle \max_{1\le i\le n} (p_i + r_{n-i}), & n \ge 1
\end{cases}
$$

伪代码（P12）大致是：

```text
CUT-ROD(p, n):
    if n == 0: return 0
    q = -∞
    for i = 1..n:
        q = max(q, p[i] + CUT-ROD(p, n - i))
    return q
```

含义很简单：

* 钢管长 (n) 米，枚举第一刀切 (i) 米：

  * 这刀赚 (p_i)，
  * 剩下 (n-i) 米递归算最优收益 (r_{n-i})。
* 在所有 (i) 中取最大。

### 3.2 问题：递归树里“子问题大量重复”（P10–13）

PPT 在 P10–13 画了递归树：你会看到很多子问题反复出现，比如：

* 计算 (r_4) 时，会递归算 (r_3, r_2, r_1)；
* 计算 (r_3) 时又会递归算 (r_2, r_1)；
* 同一个 (r_2) 被算了好多遍。

结果就是：

> 子问题数量指数级，
> 递归调用次数也指数级，
> 整体时间复杂度是 (T(n) = \Theta(2^n))。

（P13–15 用置换法进行了证明，但你不需要细节，只记住这个结果即可。）

---

## 4. Step 3：用动态规划高效计算最优值（P16–18）

这一部分展示两种 DP 写法：

1. **自顶向下 + 备忘录（memoization）**
2. **自底向上填表（bottom-up）**

### 4.1 写法一：自顶向下 + 备忘录（P16）

思路：

> 还是按照递归的思路“往下拆”，
> 但**每个子问题 (r_n) 只算一次，算完就存起来**，
> 以后再遇到直接查表，不再递归。

关键做法：

* 准备一个数组 (r[0..n])：

  * 初始时全部设为“尚未计算”（比如 (-\infty) 或 (-1)）；

* 改造递归函数：

  ```text
  MEMOIZED-CUT-ROD-AUX(p, n, r):
      if r[n] >= 0:         // 已算过，直接返回
          return r[n]
      if n == 0:
          q = 0
      else:
          q = -∞
          for i = 1..n:
              q = max(q, p[i] + MEMOIZED-CUT-ROD-AUX(p, n-i, r))
      r[n] = q              // 把结果存进表
      return q
  ```

* 主程序 `MEMOIZED-CUT-ROD(p, n)`：

  * 申请数组 `r[0..n]`，全部设为 -∞；
  * 调用 `MEMOIZED-CUT-ROD-AUX(p, n, r)`。

这样，每个 (r_j) 最多被真正递归计算一次，后面都直接返回。
PPT 给出的时间复杂度结论：**(\Theta(n^2))**。

---

### 4.2 写法二：自底向上 fill table（P17）

第二种写法是完全不递归，直接“从小到大”把所有 (r_0, r_1, \dots, r_n) 算出来：

```text
BOTTOM-UP-CUT-ROD(p, n):
    let r[0..n] be a new array
    r[0] = 0
    for j = 1..n:
        q = -∞
        for i = 1..j:
            q = max(q, p[i] + r[j-i])
        r[j] = q
    return r[n]
```

理解方式：

* 外层循环：按长度 (j = 1,2,\dots,n) 递增；
* 计算 (r_j) 时，用前面已经算好的 (r_0, r_1, \dots, r_{j-1}) 来套公式
  $$
  r_j = \max_{1\le i\le j} (p_i + r_{j-i})
  $$
* 每个 (r_j) 都只算一次。

这样总体的计算量也是**(\Theta(n^2))**。

P17 上还给了几个具体例子：

* (r_1 = p_1)
* (r_2 = \max(p_1 + r_1,\ p_2))
* (r_3 = \max(p_1 + r_2,\ p_2 + r_1,\ p_3))

你会发现：

> 这就是把原来的递归“翻过来”，按长度从小到大填表。

---

### 4.3 为什么“普通递归”慢、“DP”快？（P18）

PPT 用“子问题图”（subproblem graph）解释了三种情况：

1. **单纯递归**：

   * 图里每个结点是一个子问题 (r_j)；
   * 每条有向边代表“从 (r_n) 递归调用 (r_{n-i})”；
   * 一个结点会被多次访问，多次递归计算，浪费巨大。
2. **递归 + 备忘录**：

   * 结点还是会被多次访问，但**只第一次真的往下递归算，之后直接返回**。
   * 每个子问题仅算一次。
3. **自底向上**：
   * 把图的边方向反过来，相当于按“拓扑逆序”从底算到顶；
   * 每个子问题只算一次，每条边只被访问一次。

所以，动态规划的本质就是：

> 把“带重复子问题的递归”改造成
> “**无重复子问题的表格计算**”。

---

## 5. Step 4：恢复一个最优切割方案（P19–20）

到目前为止，我们只能得到“最大收益 (r_n)”这个数，但不知道**怎么切**。
PPT 的最后一步教你：在 DP 时顺便记录“第一刀切多长”，然后回溯出一整个方案。

### 5.1 扩展 DP：额外记录数组 (s[j])（P19）

在 `BOTTOM-UP-CUT-ROD` 的基础上，加一个数组 (s[1..n])：

* (s[j])：当钢管长度为 (j) 时，**使收益 (r_j) 达到最优的那一刀的长度**（也就是第一刀切多长）。

扩展版伪代码（P19 的 `EXTENDED-BOTTOM-UP-CUT-ROD`）：

```text
EXTENDED-BOTTOM-UP-CUT-ROD(p, n):
    let r[0..n], s[0..n] be new arrays
    r[0] = 0
    for j = 1..n:
        q = -∞
        for i = 1..j:
            if q < p[i] + r[j - i]:
                q = p[i] + r[j - i]
                s[j] = i        // 记录第一刀切的长度
        r[j] = q
    return (r, s)
```

区别就在这一行：

$$
\text{if } q < p[i] + r[j-i] \text{ then } s[j] = i
$$

——当更新最大收益时，同时把“贡献这个最大收益的第一刀长度”记下来。

### 5.2 打印切割方案（P20）

有了 (s[1..n])，打印最优切割方案就很简单了：

`PRINT-CUT-ROD-SOLUTION(p, n)`：

1. 先调用 `EXTENDED-BOTTOM-UP-CUT-ROD(p, n)` 得到 (r, s)；
2. 然后：

```text
while n > 0:
    print s[n]       // 第一刀切 s[n] 米
    n = n - s[n]     // 剩下的一段继续切
```

比如（P20 的输出示例）：

* 长度 10：

  * 不切，(s[10] = 10)，收益 30 元。
* 长度 9：

  * (s[9] = 3)，说明第一刀切 3 米，剩 6 米；
  * 再看 (s[6] = 6)，第二刀切 6 米，结束；
  * 切法 (9 = 3 + 6)，总收益 25 元。
* 长度 8：

  * (s[8] = 2)，剩 6；
  * (s[6] = 6)；
  * 故 (8 = 2 + 6)，收益 22 元。

---

## 6. 练习题简要说明（P21–24）

PPT 最后给了几个小练习，也能帮助你巩固思路。

### 6.1 练习 1：给定另一张价格表（P21–22）

价格表变成（上半表格）：

$$
i:\ 1,2,3,4,5,6,7 
p_i:\ 2,5,10,11,13,15,20
$$

经过 DP 计算，下面的表给出了结果（P22 已填好）：

* (r[i] = 0,2,5,10,12,15,20,22)
* (s[i] = 0,1,2,3,1,2,3,1)

问题：长度 7 怎么切？

根据 (s[7]=1)：第一刀切 1 米，剩 6；
再看 (s[6]=3)：第二刀切 3 米，剩 3；
再看 (s[3]=3)：第三刀切 3 米，剩 0。

所以：

* **最优值**：(r_7 = 22)
* **最优解**：(7 = 1+3+3)

### 6.2 练习 2：按式 (15.1) 写递归 & 它的运行时间（P23）

式 (15.1) 是更“原始”的写法，把所有切法都写出来：

$$
r_n = \max(p_1 + r_{n-1},\ r_1 + r_{n-1},\ r_2 + r_{n-2},\ \dots)
$$

如果按它写递归，得到的还是一颗巨大的递归树，
运行时间同样是 (T(n)=\Theta(2^n))。

P23 右侧给的 `CUT-ROD(p, n)` 伪代码就是前面讲过的递归版本。

### 6.3 练习 3：用自底向上 DP 时，(15.1) 和 (15.2) 哪个更快？（P24）

问题本质是：

* 用 (15.1) 直接写 DP，会涉及更多子问题组合（重复考虑对称情况）；
* 用简化后的 (15.2) 写 DP，只需要考虑 (p_i + r_{n-i}) 这一种形式，子问题更少，效率更高。

PPT 右边给出的 `EXTENDED-BOTTOM-UP-CUT-ROD` 就是基于 (15.2) 的写法。

---

## 7. 本章所有算法 & 复杂度结论小结

按“算法”列一个清单（不含推导，只列结果）：

1. **暴力递归 `CUT-ROD`**（自顶向下，不存结果）

   * 对应公式：(r_n = \max_{1\le i\le n}(p_i + r_{n-i}))
   * 子问题大量重复
   * 时间复杂度：(\Theta(2^n))
2. **DP：自顶向下 + 备忘录 `MEMOIZED-CUT-ROD`**

   * 思路：递归 + 数组记忆中间结果 (r[n])
   * 每个子问题至多计算一次
   * 时间复杂度：(\Theta(n^2))
3. **DP：自底向上填表 `BOTTOM-UP-CUT-ROD`**

   * 从 (r_0) 算到 (r_n)，每个 (r_j) 做 (j) 次比较
   * 时间复杂度：(\Theta(n^2))
4. **扩展 DP：`EXTENDED-BOTTOM-UP-CUT-ROD` + `PRINT-CUT-ROD-SOLUTION`**

   * 在填表同时记录 (s[j])：长度为 (j) 时第一刀切多少
   * `PRINT` 用 (s) 数组回溯出一条最优切割方案
   * 时间仍然是 (\Theta(n^2))，多了一个 (O(n)) 的回溯输出。

---

## 8. 再总结一下这一章在讲什么

用一段话收个尾：

* 这章用“**钢管切割**”这个很直观的例子，完整走了一遍动态规划的四步：

  1. 找到**最优子结构**，定义最优值 (r_n)；
  2. 写出递推式 (r_n = \max_{1\le i\le n}(p_i + r_{n-i}))；
  3. 先用“纯递归”说明会退化成 (2^n) 时间，再引入

     * **备忘录递归**
     * **自底向上 DP**
       把时间压到 (\Theta(n^2))；
  4. 最后扩展 DP，记下第一刀的位置 (s[n])，再回溯得到**具体的最优切割方案**。

如果你愿意，下一步我可以帮你**拿一个具体长度（比如 7 米或 10 米）**，
按照 `EXTENDED-BOTTOM-UP-CUT-ROD` 的表格，一行一行算出 (r[i]) 和 (s[i])，
再画出“如何切”的完整过程，让你对这个 DP 的运行更有画面感。



# a06

这章 PPT 主要讲了**两个典型的动态规划算法**：

1. **矩阵链乘（Matrix-Chain Multiplication, MCM）** —— 给一串矩阵，怎么决定加括号顺序，使得标量乘法次数最少。
2. **最优二叉搜索树（Optimal Binary Search Tree, OBST）** —— 已知每个关键字被查到的频率，怎么构造一棵 BST，使得**期望查找代价最小**。

我会按 PPT 的顺序来讲，每个问题都用“**问题 → 最优子结构 → 递归式 → DP 算法 → 怎样恢复最优解**”的思路来走一遍，复杂度只给结论。

---

## 一、15.2 矩阵链乘（Matrix-chain Multiplication, MCM）

### 1.1 单个矩阵相乘回顾（P3–P8）

PPT 先回顾了**两个矩阵相乘**的标准算法：

* 矩阵 $A$ 为 $p\times q$，矩阵 $B$ 为 $q\times r$，才能相乘。
* 结果矩阵 $C = A\times B$ 的大小是 $p\times r$。
* 标量乘法次数是 $pqr$（真正耗时的是这部分）。

伪代码大概是：

```text
MATRIX-MULTIPLY(A,B)
  for i = 1..rows[A]        // p
    for j = 1..cols[B]      // r
      C[i,j] = 0
      for k = 1..cols[A]    // q
        C[i,j] = C[i,j] + A[i,k] * B[k,j]
  return C
```

---

### 1.2 问题：矩阵链乘 —— 括号怎么加才省事？（P3–P9）

有一串矩阵：

$$
\langle A_1, A_2, \dots, A_n\rangle
$$

想计算它们的乘积：

$$
A_1A_2\cdots A_n
$$

由于矩阵乘法满足**结合律**，比如 $A_1(A_2A_3) = (A_1A_2)A_3$，所以**无论怎么加括号，结果矩阵一样**。但不同的括号方式需要的**标量乘法次数差异巨大**。

PPT 举了 3 个矩阵的例子（P8）：

* $A_1: 10\times100$
* $A_2: 100\times5$
* $A_3: 5\times50$

两种加括号：

1. $((A_1A_2)A_3)$

   * 先算 $A_1A_2$：$10\cdot100\cdot5 = 5000$ 次乘法，结果是 $10\times5$；
   * 再算 $(A_1A_2)A_3$：$10\cdot5\cdot50 = 2500$ 次乘法；
   * 总计：$7500$ 次。
2. $(A_1(A_2A_3))$

   * 先算 $A_2A_3$：$100\cdot5\cdot50 = 25000$ 次；
   * 再算 $A_1(A_2A_3)$：$10\cdot100\cdot50 = 50000$ 次；
   * 总计：$75000$ 次。

**同样的结果，计算量差了 10 倍！**

> **矩阵链乘问题（MCM）**：
> 给定矩阵链 $\langle A_1,\dots,A_n\rangle$，
> 其中 $A_i$ 的维度是 $p_{i-1}\times p_i$，
> 选择一种加括号方式（乘法顺序），使得**标量乘法总次数最少**。

---

### 1.3 暴力穷举：全括号数 $P(n)$ 很快爆炸（P10–11）

* $n$ 个矩阵连乘可以有很多种“**全括号方式**”（每一种对应一种运算顺序）。
* 设 $P(n)$ 为 $n$ 个矩阵的全括号数。

  * $P(1)=1$（只剩一个矩阵，没有乘法）。
  * $n\ge2$ 时任意一种括号形式，都可以看作在某个 $k$ 把链切成两半
    $(A_1\cdots A_k)(A_{k+1}\cdots A_n)$，于是有递推：

$$
P(n) = \sum_{k=1}^{n-1} P(k),P(n-k)
$$

这个递推的解是**指数级**，$P(n)$ 大致是 $\Omega(2^n)$ 级别，所以**枚举所有加括号方式完全不可行**。

---

### 1.4 Step 1：最优子结构（P12–16）

定义子问题：

$$
A_{i..j} = A_iA_{i+1}\cdots A_j,\quad i\le j
$$

看任意一个非平凡子链 $A_{i..j}$ 的全括号（$i<j$），不管怎么加括号，最外层肯定在某个 $k$ 把它分成两块：

$$
(A_{i..k})(A_{k+1..j}),\quad i\le k<j
$$

那么这次乘法的总成本是：

* 左边子链 $A_{i..k}$ 的成本：$cost(A_{i..k})$；
* 右边子链 $A_{k+1..j}$ 的成本：$cost(A_{k+1..j})$；
* 然后把这两个结果相乘：如果 $A_i$ 是 $p_{i-1}\times p_i$，
  那么 $A_{i..k}$ 是 $p_{i-1}\times p_k$，
  $A_{k+1..j}$ 是 $p_k\times p_j$，
  这一步要 $p_{i-1}p_kp_j$ 次乘法。

所以：

$$ cost(A_{i..j}) = cost(A_{i..k}) + cost(A_{k+1..j}) + p_{i-1}p_kp_j $$

**最优子结构结论：**

> 如果 $A_{i..j}$ 的一组**最优加括号方式**在 $k$ 处分割，
> 那么左边的 $A_{i..k}$ 和右边的 $A_{k+1..j}$
> 必须分别是它们各自子问题的最优加括号方式。

否则，用更优的子括号替换，就能得到更好的整体结果，与“整体最优”矛盾。

---

### 1.5 Step 2：递归式（P17–21）

设：

$$
m[i,j] = \text{矩阵链 }A_{i..j}\text{ 的**最少标量乘法次数**。}
$$

* 原问题的答案是 $m[1,n]$。

边界：

* 一个矩阵不用乘：
  $$
  m[i,i] = 0,\quad 1\le i\le n
  $$

递推（$i<j$）：枚举所有分割点 $k$，取最小值：

$$
m[i,j] = \min_{i\le k<j}\Bigl(m[i,k] + m[k+1,j] + p_{i-1}p_kp_j\Bigr)
$$

加上边界可以写成：

$$
m[i,j] =
\begin{cases}
0, & i = j[4pt]
\displaystyle \min_{i\le k<j}\left(m[i,k] + m[k+1,j] + p_{i-1}p_kp_j\right), & i<j
\end{cases}
$$

为了以后能恢复具体加括号方式，还要定义一个**辅助数组**：

* $s[i,j]$：在求 $m[i,j]$ 时**取得最小值的分割点** $k$。

---

### 1.6 直接递归算法（很慢，只是概念）（P22–24）

按上面递推式可以写一个递归函数 `RE-MCM(p,i,j)`：

```text
RE-MCM(p, i, j):
  if i == j: return 0
  m[i,j] = +∞
  for k = i..j-1:
    q = RE-MCM(p, i, k) +
        RE-MCM(p, k+1, j) +
        p[i-1] * p[k] * p[j]
    m[i,j] = min(m[i,j], q)
  return m[i,j]
```

问题：**同一个子问题 $(i,j)$ 会被在不同递归分支中重复算很多次**，导致总时间接近指数级，非常慢，实际不能用。

---

### 1.7 Step 3：自底向上的 DP 算法 `MCM-DP`（P25–31）

PPT 给出的真正实用的算法是**表格法 + 自底向上**：

**输入：**

* 维度数组 $p[0..n]$，矩阵 $A_i$ 的维度是 $p_{i-1}\times p_i$。

**输出：**

* 表 `m[i,j]`：最优乘法次数；
* 表 `s[i,j]$：分割点（用于重建括号）。

**算法思想：**

* 所有子问题是区间 $[i,j]$（$1\le i\le j\le n$），一共 $\Theta(n^2)$ 个。
* 小区间的结果会被大区间使用，所以按“**区间长度从小到大**”计算。

伪代码（省略细节符号）：

```text
MCM-DP(p):
  n = length(p) - 1
  for i = 1..n:
    m[i,i] = 0
  for l = 2..n:              // l 为链长
    for i = 1..n-l+1:
      j = i + l - 1
      m[i,j] = +∞
      for k = i..j-1:        // 枚举分割点
        q = m[i,k] + m[k+1,j] + p[i-1]*p[k]*p[j]
        if q < m[i,j]:
          m[i,j] = q
          s[i,j] = k
  return m, s
```

* 外层 `l` 从 2 到 $n$，先算长度 2 的子链，再算长度 3 ……
* 每个 $(i,j)$ 只被计算一次。
* 时间复杂度结论：$T(n) = O(n^3)$；空间 $O(n^2)$。

P27–29 页画的那个“上三角 DP 表”和“旋转 45° 的菱形表”，就是 `m[i,j]` 和 `s[i,j]` 的可视化示意。

---

### 1.8 Step 4：打印一个最优加括号方式（P32–35）

`MCM-DP` 得到了 `s[i,j]`，但还没把括号打印出来。
PPT 给了递归打印函数 `PRINT-OPTIMAL-PARENS(s,i,j)`：

```text
PRINT-OPTIMAL-PARENS(s, i, j):
  if i == j:
    print "A_i"
  else:
    print "("
    PRINT-OPTIMAL-PARENS(s, i, s[i,j])
    PRINT-OPTIMAL-PARENS(s, s[i,j]+1, j)
    print ")"
```

用法：调用 `PRINT-OPTIMAL-PARENS(s, 1, n)`。

* 如果只有一个矩阵 $A_i$，就直接输出 $A_i$。
* 否则，根据 `s[i,j]` 把区间 $[i,j]$ 拆成左 `[i,s[i,j]]$ 和右 $[s[i,j]+1,j]$，递归输出左右，再外面加括号。

在例子中（矩阵维度为 $30\times35$, $35\times15$, $15\times5$, $5\times10$, $10\times20$, $20\times25$，见 P27），最终输出括号形如：

$$
((A_1(A_2A_3))((A_4A_5)A_6))
$$

这就是乘法顺序的一个**最优方案**。

---

### 1.9 小结：MCM 这一部分讲了什么算法？

1. **问题**：矩阵链乘，决定括号顺序，使标量乘法次数最少。
2. **暴力**：全括号数 $P(n)$ 指数级，暴力枚举不可行。
3. **最优子结构**：

   * $A_{i..j}$ 的最优加括号一定是某个 $k$ 的分割
     $(A_{i..k})(A_{k+1..j})$，且左右部分必须各自最优。
4. **递归式**：
   $$
   m[i,i]=0,\quad
   m[i,j]=\min_{i\le k<j}{m[i,k]+m[k+1,j]+p_{i-1}p_kp_j}
   $$
5. **DP 算法 `MCM-DP`**：

   * 自底向上按链长填表 $m,s$，时间 $O(n^3)$。
6. **恢复最优解**：

   * 用 `PRINT-OPTIMAL-PARENS(s,1,n)` 输出一组最优括号。

---

## 二、15.5 最优二叉搜索树（Optimal Binary Search Trees, OBST）

这一节讲的是：**已知每个关键字被查的概率，怎样构造一棵 BST，使得“平均查找代价”最小**。

### 2.1 应用背景：词典 / 翻译软件 / 输入法（P37–46）

PPT 用了“**翻译软件 / 输入法的词库**”作背景：

* 有一份英文单词 → 中文解释的词典，要不断查找单词。
* 如果按顺序存在**线性表**里，每次查找平均需要 $O(n)$ 时间。
* 换成**二叉搜索树（BST）**，查一个词的最坏时间是 $O(\log n)$（如果平衡）。

但实际有个关键问题：

> 不同单词出现频率差别很大。
> 高频词（比如 `algorithm`）
> 低频词（比如 `mycophagist`）。

我们希望：**高频词离根更近**，低频词可以离根远一点，这样整体平均访问次数更少。

PPT 通过几个具体例子（示例句子是 *Accepted Coding programming is rightly to program a right program to run a right program*）计算了不同 BST 的期望搜索代价，发现：

* 不同 BST 的费用不同，比如 $38/15$、$39/15$、$44/15$ 等；
* “平衡树”并不一定是最优的；
* “把频率最高的 key 放根上”也不一定得到全局最优。

于是，需要一个**系统的方法**来找“期望代价最小”的 BST，这就是 OBST 问题。

---

### 2.2 问题模型与概率（P50–52）

设有 $n$ 个不同的关键字，已经按大小排序：

$$
K = \langle k_1, k_2, \dots, k_n\rangle,\quad k_1 < k_2 < \dots < k_n
$$

* 查找关键字 $k_i$ 的概率为 $p_i$（比如在所有查找请求中，$k_i$ 占 $p_i$ 的比例）。
* 有些查找是**失败的**（要查的词不在词典里）。这些失败查找分成 $n+1$ 类：

  * 小于 $k_1$ 的所有词 —— 记为 dummy key $d_0$；
  * 在 $k_i$ 和 $k_{i+1}$ 之间的词 —— dummy key $d_i$；
  * 大于 $k_n$ 的词 —— dummy key $d_n$。
* 查到 dummy key $d_i$ 的概率为 $q_i$。

于是：

$$
\sum_{i=1}^n p_i + \sum_{i=0}^n q_i = 1
$$

在 BST 中：

* 所有真正的关键字 $k_i$ 是**内部结点**；
* 所有 dummy key $d_i$ 是**叶子**（或空子树的代表）。

令 $\text{depth}_T(x)$ 表示结点 $x$ 在树 $T$ 中的深度（根深度为 0）。
对一棵给定的 BST $T$，其**期望搜索代价**为（公式 (15.16)）：

$$
E[\text{cost in }T]
= \sum_{i=1}^n (\text{depth}_T(k_i)+1)p_i

* \sum_{i=0}^n (\text{depth}_T(d_i)+1)q_i
$$

也可以看作：每个结点的“深度+1”乘以它被访问的概率，求总和。

> **OBST 问题**：
> 已知 $p_1,\dots,p_n$ 和 $q_0,\dots,q_n$，
> 构造一棵 BST，使得 $E[\text{cost in }T]$ 最小。

---

### 2.3 Step 1：最优子结构（P57–62）

考虑任意一棵候选 BST $T$，从中取一棵**子树** $T'$，其中包含一段连续的关键字：

$$
k_i, k_{i+1}, \dots, k_j,\quad 1\le i\le j\le n
$$

以及对应的 dummy keys：$d_{i-1},\dots,d_j$。

这棵子树本身也是一棵合法的 BST，解决的是一个“**子问题**”：只考虑这段键值和这几类失败查找。

这棵子树本身也是一棵合法的 BST，解决的是一个“**子问题**”：只考虑这段键值和这几类失败查找。

**最优子结构结论：**

> 如果 $T$ 是全局最优 BST，
> 那么对任意区间 $[i,j]$，
> 其对应的子树 $T'$ 也必须是
> 这个子问题的最优 BST。

证明思路是**“剪切—粘贴”**（cut-and-paste，P59）：

* 假设子树 $T'$ 不是子问题的最优解，存在另一棵更好的子树 $T''$；
* 把 $T'$ 从 $T$ 中“剪掉”，换成 $T''$，得到一棵新树 $T^{*}$；
* 新树的期望代价比原来的 $T$ 小，与“$T$ 已经最优”矛盾。

所以可以放心地**把大问题拆成连续区间的子树问题**，用 DP。

---

### 2.4 Step 2：递归式与状态定义（P62–66）

定义子问题的最优值：

* $e[i,j]$：只考虑关键字 $k_i,\dots,k_j$
  和 dummy keys $d_{i-1},\dots,d_j$ 时，
  所能构造出的**最优 BST 的期望代价**。

特别地，当 $j = i-1$ 时，没有真正的 key 只有一个 dummy key $d_{i-1}$（对应空子树），其代价就是它被访问一次的概率：

$$
e[i,i-1] = q_{i-1}
$$

原问题就是 $e[1,n]$。

再定义一个辅助量：

$$
w[i,j] = \sum_{l=i}^j p_l + \sum_{l=i-1}^j q_l
$$

可以理解为**该子树内所有节点（key + dummy）的概率总和**。

* 特别地，$w[i,i-1]=q_{i-1}$。
* 有一个递推式（式 (15.20)）：
  $$
  w[i,j]=w[i,j-1]+p_j+q_j
  $$
  所以每次只需 $O(1)$ 时间更新。

---

#### 2.4.1 选择一个根 $k_r$ 的代价（P64–66）

假设在子问题 $(i,j)$ 中，我们选 $k_r$（$i\le r\le j$）作为根：

* 它的左子树是区间 $[i,r-1]$ 的最优 BST，代价为 $e[i,r-1]$；
* 右子树是区间 $[r+1,j]$ 的最优 BST，代价为 $e[r+1,j]$。

当这两棵子树挂到根之下时，它们内部每个节点的深度都 **+1**，
因此总期望代价要多加一遍各自的概率和，
也就是 $w[i,r-1]$ 和 $w[r+1,j]$。

整理以后，代价可以写成一个更简洁的形式（式 (15.18)）：

$$
e[i,j] = e[i,r-1] + e[r+1,j] + w[i,j]
$$

于是，对于子问题 $(i,j)$，我们要在所有可能的根 $k_r$ 中选最优的那个：

$$
e[i,j] = \min_{i\le r\le j}{e[i,r-1] + e[r+1,j] + w[i,j]}
$$

连同基例，就得到完整递推（式 (15.19)）：

$$
e[i,j] =
\begin{cases}
q_{i-1}, & j = i-1[4pt]
\displaystyle
\min_{i\le r\le j}{e[i,r-1] + e[r+1,j] + w[i,j]},
& j\ge i
\end{cases}
$$

同样，为以后恢复树结构，还要记一个数组：

* $\text{root}[i,j]=r$：表示在区间 $[i,j]$ 的最优 BST 中，$k_r$ 是子树的根。

---

### 2.5 Step 3：自底向上的 DP 算法 `OBST`（P67–74）

DP 的思路和 MCM 很像：

* 所有子问题是 $(i,j)$，满足 $1\le i\le j\le n$。
* 还有“空区间” $(i,i-1)$。
* 总共也是 $\Theta(n^2)$ 个子问题。

PPT 给出了算法 `OBST(p,q,n)`：

```text
OBST(p, q, n):
  // 初始化空区间
  for i = 1..n+1:
    e[i, i-1] = q[i-1]
    w[i, i-1] = q[i-1]

  // l 表示 key 的个数（子树大小）
  for l = 1..n:
    for i = 1..n-l+1:
      j = i + l - 1
      e[i, j] = +∞
      // 用递推式 O(1) 算 w[i,j]
      w[i, j] = w[i, j-1] + p[j] + q[j]
      // 尝试每一个可能的根 k_r
      for r = i..j:
        t = e[i, r-1] + e[r+1, j] + w[i, j]
        if t < e[i, j]:
          e[i, j] = t
          root[i, j] = r

  return e, root
```

解释：

* 第一层 `l` 从 1 到 $n$，表示子树中 key 的数量，从小到大算。
* 对于每个 $(i,j)$：

  * 先用式 (15.20) 更新 `w[i,j]`；
  * 再用式 (15.19) 在所有可能的根 $r$ 中取最小。

得到的：

* $e[1,n]$ 就是整个词典的**最小期望搜索代价**；
* $\text{root}[i,j]$ 表示每个子树的根。

结论：

* 时间复杂度 $O(n^3)$（三重循环）。
* 空间复杂度 $O(n^2)$（存 $e,w,\text{root}$ 三个表）。

---

### 2.6 Step 4：从 `root` 表构造一棵最优 BST（P80）

PPT 把“如何根据 `root` 输出树结构”放在了习题 15.5-1。大致的构造方式是：

定义一个递归过程 `CONSTRUCT-OBST(root, i, j)`：

* 若 $j < i$：只剩一个 dummy key $d_{i-1}$，对应一个空子树 / 叶子。
* 若 $i\le j$：

  1. 取 $r = \text{root}[i,j]$，建立结点 $k_r$；
  2. 递归构造左子树：`CONSTRUCT-OBST(root, i, r-1)`；
  3. 递归构造右子树：`CONSTRUCT-OBST(root, r+1, j)`。

构造完成后，你就得到了一棵“按给定频率 $p_i,q_i$ 意义下的最优二叉搜索树”。

---

### 2.7 OBST 这一部分讲了什么算法？

1. **问题**：

   * 已知有序关键字 $k_1,\dots,k_n$ 的搜索概率 $p_i$，
   * 以及失败查找对应的 dummy keys $d_0,\dots,d_n$ 的概率 $q_i$；
   * 要构造 BST，使**期望搜索代价最小**。
2. **强行暴力枚举所有 BST** —— 数量为 $\Omega(4^n/n^{3/2})$，不可行（P56）。
3. **最优子结构**：

   * 全局最优 BST 的任意子树，对应的那段 key 也必须是子问题的最优 BST。
4. **递归式**：

   * 子问题最优值 $e[i,j]$；权重 $w[i,j]$；
   * 基例：$e[i,i-1]=q_{i-1},\ w[i,i-1]=q_{i-1}$；
   * 递推：
     $$
     w[i,j]=w[i,j-1]+p_j+q_j
     $$
     $$
     e[i,j]=\min_{i\le r\le j}{e[i,r-1]+e[r+1,j]+w[i,j]}
     $$
5. **DP 算法 `OBST`**：

   * 按 key 数量从小到大填表 $e,w,\text{root}$，时间 $O(n^3)$，空间 $O(n^2)$。
6. **恢复最优解**：

   * 使用 `root[i,j]` 递归构造整棵树，每个子区间找对应的根 $k_r$。

---

## 三、整章小结（从“算法 + 案例 + 解法”的角度）

这一份 PPT 主要围绕“**动态规划**”继续展开了两个重要案例：

1. **矩阵链乘（MCM）**

   * 案例：一串矩阵 $A_1\cdots A_n$，结果相同，但加括号不同，计算量差别巨大。
   * 算法：

     * 定义 $m[i,j]$ 为 $A_{i..j}$ 的最小乘法次数。
     * 递推：$m[i,j] = \min_{i\le k<j}{m[i,k]+m[k+1,j]+p_{i-1}p_kp_j}$。
     * 用 `MCM-DP` 自底向上填表，$O(n^3)$ 时间求出最小乘法次数；
     * 用 `s[i,j]` 和 `PRINT-OPTIMAL-PARENS` 输出一个最优括号方案。
2. **最优二叉搜索树（OBST）**

   * 案例：翻译软件 / 词典，已知词频，如何构造 BST 使平均查找代价最小。
   * 算法：

     * 定义 $e[i,j]$ 为区间 $[i,j]$ 上最优 BST 的期望代价，$w[i,j]$ 为该区间的概率和。
     * 递推：
       $$
       w[i,j]=w[i,j-1]+p_j+q_j,\quad
       e[i,j]=\min_{i\le r\le j}{e[i,r-1]+e[r+1,j]+w[i,j]}
       $$
     * 用 `OBST` 算法表格法求出 $e[1,n]$ 和 `root[1,n]`，时间 $O(n^3)$；
     * 根据 `root` 递归构造出一棵最优 BST。

这两部分和你前面学过的 **装配线调度** 和 **钢条切割** 一样，完整走了一遍 DP 的套路：

> **最优子结构 → 递归式 → 自底向上 DP 求最优值 → 用额外表恢复最优解。**

如果你愿意，我可以下一步用 PPT 里的某个具体例子（比如那 6 个矩阵，或者 $n=5$ 的 OBST 概率表），带你**手算一张 DP 表**，一步一步填出 $m[i,j]/e[i,j]$ 和 $s[i,j]/root[i,j]$，你会对这两种 DP 的细节更有感觉。

# a06-x

这章 PPT 主要讲**动态规划里最经典的一个问题：最长公共子序列（LCS）**，并且一步步从直观想法 → 递归 → DP 表格算法 → 如何恢复一个最长公共子序列，再顺带讲了怎么做一些**代码和空间优化**。

我按 PPT 的顺序给你讲一遍，最后再整体总结一次。

---

## 1. 应用背景：DNA、字符串相似度（P2–4）

* 一条 DNA 可以看成是由碱基组成的串，每个位置只能是集合 ${A,C,G,T}$ 里的一个。
* 比如两个生物的 DNA：

  * $S_1 = \text{ACCGGTCGAGTGCGCGGAAGCCGGCCGAA}$
  * $S_2 = \text{GTCGTTCGGAATGCCGTTGCTCTGTAAA}$

问题：**这两个 DNA 有多相似？**（P2 的几张图说明可以用来分析亲缘关系、转基因、疾病等）

PPT 给了几种“相似”的定义方式（P3–4）：

1. 如果一个串是另一个串的**子串**（连续的一段），就说相似。
2. 如果把一个串改造成另一个串需要的“修改步数”很少，也可以说相似（编辑距离）。
3. 本章选择的方式：找一个第三个串 $S_3$，它是 $S_1$ 和 $S_2$ 的**公共子序列**，而且越长越好。$S_3$ 越长，两者越相似（P4 图里的 $S_1,S_2,S_3$ 例子）。

于是我们把“相似度”形式化成一个算法问题：

> **最长公共子序列（Longest Common Subsequence, LCS）问题。**

---

## 2. 基本概念：子序列 & 公共子序列（P5–7）

### 2.1 子序列（Subsequence，P5）

给一个序列
$$
X = \langle x_1,x_2,\dots,x_m\rangle
$$

另一个序列
$$
Z = \langle z_1,z_2,\dots,z_k\rangle
$$

如果存在一串严格递增的下标
$$
\langle i_1,i_2,\dots,i_k\rangle,\quad 1\le i_1<\dots<i_k\le m
$$
使得对所有 $j$ 都有
$$
z_j = x_{i_j},
$$
那么说 $Z$ 是 $X$ 的一个**子序列**。

> 也就是：**从 $X$ 里按顺序“抽几个元素出来”，中间可以跳过，不要求连续**。

例子（P5）：

* $X = \langle A,B,C,B,D,A,B\rangle$
* $Z=\langle B,C,D,B\rangle$
* 对应下标序列是 $\langle 2,3,5,7\rangle$，所以 $Z$ 是 $X$ 的子序列。

---

### 2.2 公共子序列 & 最长公共子序列（P6–7）

* 如果 $Z$ 既是 $X$ 的子序列，也是 $Y$ 的子序列，就叫做 $X$ 和 $Y$ 的**公共子序列**。
* 在所有公共子序列中，**长度最大的那些**就叫 **LCS**（可能不止一个）。

PPT 的经典例子（P6）：

* $X = \langle A,B,C,B,D,A,B\rangle$
* $Y = \langle B,D,C,A,B,A\rangle$
* $Z_1 = \langle B,C,A\rangle$ 是公共子序列，但不是最长的
* $Z_2 = \langle B,C,B,A\rangle$ 是一个 LCS（$\langle B,D,A,B\rangle$ 也是）。

**LCS 问题的正式描述（P7）：**

> 给定两个序列
> $X = \langle x_1,\dots,x_m\rangle,\ Y = \langle y_1,\dots,y_n\rangle$，
> 求一个长度最大的序列 $Z$，它是 $X$ 和 $Y$ 的公共子序列。

PPT 说明：这个问题非常适合用**动态规划**来做。

---

## 3. 暴力解法：枚举所有子序列 → 不可行（P8）

一个最直接的想法（P8）：

1. 枚举 $X$ 的所有子序列（共有 $2^m$ 个，因为每个元素要么选要么不选）；
2. 对于每个子序列，检查它是不是 $Y$ 的子序列；
3. 找出其中最长的。

显然，当 $m$ 稍微大一点时，$2^m$ 就非常恐怖，**指数级时间**，完全不现实。

> 所以我们要用动态规划，利用这个问题的“结构”。

---

## 4. Step 1：刻画 LCS 的最优子结构（P9–14）

### 4.1 用“前缀”做子问题（P9）

定义前缀：

* $X_i = \langle x_1,\dots,x_i\rangle$，是 $X$ 的前 $i$ 个元素（$i=0,1,\dots,m$，$X_0$ 是空序列）；
* $Y_j = \langle y_1,\dots,y_j\rangle$，是 $Y$ 的前 $j$ 个元素（$j=0,1,\dots,n$）。

接下来我们考虑：**“$X$ 和 $Y$ 的一个 LCS，跟这些前缀有什么关系？”**

---

### 4.2 定理 15.1：LCS 的最优子结构（P10–14）

设

* $Z = \langle z_1,\dots,z_k\rangle$ 是 $X$ 和 $Y$ 的**某个 LCS**，
* $X = \langle x_1,\dots,x_{m-1},x_m\rangle$，
* $Y = \langle y_1,\dots,y_{n-1},y_n\rangle$。

定理 15.1 分三种情况讨论（P10–13）：

1. **如果 $x_m = y_n$**：

   * 那么 LCS 的最后一个字符一定是这个：$z_k = x_m = y_n$；
   * 而前面的 $\langle z_1,\dots,z_{k-1}\rangle$ 就是 $X_{m-1}$ 和 $Y_{n-1}$ 的一个 LCS。

   也就是说：

   > 当两个串的最后一个字符相同时，**LCS 也以这个字符结尾**，
   > 剩下的问题就是求前缀 $X_{m-1}$ 和 $Y_{n-1}$ 的 LCS。

2. **如果 $x_m \ne y_n$，并且 $z_k \ne x_m$**：

   * 那么 $Z$ 其实是 $X_{m-1}$ 和 $Y$ 的 LCS。

3. **如果 $x_m \ne y_n$，并且 $z_k \ne y_n$**：

   * 那么 $Z$ 是 $X$ 和 $Y_{n-1}$ 的 LCS。

图 14 页把这几种情况画成了示意图：

* 要么往左上看（去掉两个串的最后一个字符）；
* 要么只去掉上边序列的最后一个字符；
* 要么只去掉下边序列的最后一个字符。

**结论：**

> LCS 问题有很好的**最优子结构**：
> 一个 LCS 的前缀部分本身就是某两个前缀子问题的 LCS。

这为动态规划打开了大门。

---

## 5. Step 2：递归形式（P15–18）

### 5.1 定义状态 $c[i,j]$（P15）

设

* $c[i,j]$ 表示：**$X_i$ 和 $Y_j$ 的 LCS 的长度**。

也就是：

* $c[i,j] = \text{LCS}(X_1..i, Y_1..j)$ 的长度。

有了定理 15.1，可以得出递推公式（P16 式 (15.14)）：

$$
c[i,j] =
\begin{cases}
0, & i=0 \text{ 或 } j=0;[4pt] \\
c[i-1,j-1] + 1, & i>0, j>0 \text{ 且 } x_i = y_j;[4pt] \\
\max{c[i-1,j], c[i,j-1]}, & i>0, j>0 \text{ 且 } x_i \ne y_j.
\end{cases}
$$

解释一下三种情况：

1. 有一个是空串（长度为 0）：

   * LCS 肯定也是空串，所以长度 0。
2. 最后一个字符相同：$x_i = y_j$

   * LCS 一定以这个字符结尾，长度等于“前面 LCS 的长度 + 1”，即 $c[i-1,j-1]+1$。
3. 最后一个字符不同：$x_i \ne y_j$

   * 看哪种选择更好：

     * 要么丢掉 $x_i$，用 $c[i-1,j]$；
     * 要么丢掉 $y_j$，用 $c[i,j-1]$；
   * 取较大者。

### 5.2 直接按公式写递归，会怎样？（P18–19）

如果直接照这个式子写递归函数：

```text
LCS-REC(i, j):
  if i==0 or j==0: return 0
  if x_i == y_j:
      return LCS-REC(i-1, j-1) + 1
  else:
      return max( LCS-REC(i-1, j),
                  LCS-REC(i, j-1) )
```

因为有大量**重叠子问题**（许多 $(i,j)$ 会被反复计算），递归树的规模会爆炸，运行时间是**指数级**。

> 所以我们需要把它改成“记忆化”或“自底向上”的 DP。

---

## 6. Step 3：用 DP 表格计算 LCS 长度（P19–22）

### 6.1 DP 算法 LCS-LENGTH（P20–21）

PPT 给的标准 DP 算法叫 `LCS-LENGTH(X,Y)`：

* 建一个 $(m+1)\times(n+1)$ 的表 $c[i,j]$ 存 LCS 长度；
* 再建一个同样大小的表 $b[i,j]$ 存“方向”，用于之后回溯 LCS；
* 按**行优先**顺序填表（从 $i=1$ 到 $m$，每行从 $j=1$ 到 $n$）。

核心步骤就是把刚才的递推式翻译成代码（伪代码第 7–16 行）：

* 若 $x_i = y_j$：

  * $c[i,j] = c[i-1,j-1] + 1$
  * $b[i,j] = \text{“↖”}$（表示来自左上）
* 若 $x_i \ne y_j$：

  * 如果 $c[i-1,j] \ge c[i,j-1]$：

    * $c[i,j] = c[i-1,j]$
    * $b[i,j] = \text{“↑”}$（表示来自上边）
  * 否则：

    * $c[i,j] = c[i,j-1]$
    * $b[i,j] = \text{“←”}$（表示来自左边）

初始条件：

* $c[i,0]=0$（任意前缀对空串的 LCS 长度都是 0）；
* $c[0,j]=0$ 也一样。

P21、P22 页画了典型例子：

* $X=\text{ABCBDAB}$
* $Y=\text{BDCABA}$

表格左边是 $c$ 值，格子里还有箭头（就是 $b$ 表）。根据递推式从上到下、从左到右填完，最后右下角 $c[m,n]$ 就是**LCS 的长度**（这里是 4）。

**复杂度结论（P21–22）：**

* 时间：$T(m,n)=O(mn)$，因为每个格子 $c[i,j]$ 只花常数时间。
* 空间：$O(mn)$ 存表 $c$ 和 $b$。

---

## 7. Step 4：用 b 表恢复一个 LCS（P23）

现在我们有了：

* $c[m,n]$：LCS 的长度；
* $b[i,j]$：每个格子的“来自方向”。

要恢复 LCS 的具体内容，只需按照 $b$ 的箭头从右下角“倒着走”到左上角，这就是 `PRINT-LCS(b,X,i,j)`（P23）：

```text
PRINT-LCS(b, X, i, j):
  if i==0 or j==0: return
  if b[i,j] == "↖":
       PRINT-LCS(b, X, i-1, j-1)
       print x_i
  else if b[i,j] == "↑":
       PRINT-LCS(b, X, i-1, j)
  else: // "←"
       PRINT-LCS(b, X, i, j-1)
```

* 初始调用：`PRINT-LCS(b, X, m, n)`。
* 每遇到“↖”，说明 $x_i=y_j$ 是 LCS 的一个字符：

  * 先递归到更小的子问题 `(i-1,j-1)`，
  * 再输出 $x_i$，保证字符顺序从左到右。
* 箭头“↑”表示当前格子的值来自 $c[i-1,j]$，说明 $x_i$ 不在当前考虑的某个 LCS 里，只需往上走；
* 箭头“←”同理。

例子（P23 图）：

* 对 $X=\text{ABCBDAB}$, $Y=\text{BDCABA}$，沿着箭头走一遍，输出的是一个 LCS，比如 $\text{BCBA}$。

**这个过程的时间是 $O(m+n)$**（每次递归至少让 $i$ 或 $j$ 减一）。

---

## 8. 代码改进：空间优化（P24–25, 29–31）

这一部分讲的是**在不改变算法本质的前提下，怎么省空间**。

### 8.1 不用 b 表，只用 c 表也能回溯 LCS（P24, 29）

观察：$c[i,j]$ 的值只可能来自三处之一：

* 若 $x_i = y_j$，则 $c[i,j] = c[i-1,j-1]+1$；
* 若 $x_i\ne y_j$ 且 $c[i-1,j] \ge c[i,j-1]$，则来自上边；
* 否则来自左边。

给定 $c$ 表，我们可以在 $O(1)$ 时间判断是哪一种情况，从而知道应该走哪条边。这样就可以写一个 `PRINT-LCS` 版本**只用 c 表，不需要 b 表**（P29 已经给了一个解答版本）。

* 好处：少一个 $m\times n$ 的表，省 $\Theta(mn)$ 空间；
* 但总空间仍是 $\Theta(mn)$，因为 $c$ 本身就这么大。

### 8.2 只算长度：滚动数组 / 一维 DP（P25, 30–31）

如果你**只关心 LCS 的长度，不需要具体序列**，还可以进一步省空间：

1. **只保留 c 表的两行**（P25, P30）

   * 计算当前行 $i$ 的时候，只需要上一行 $(i-1)$ 和当前行本身，因此只要 $2\times(n+1)$ 的数组。
2. **甚至只用一行，再加两个临时变量**（P31）

   * 用 $c[0..n]$ 存“上一行或当前行”的值，
   * 再用两个标量 $d_1,d_2$ 临时保存 $c[i-1,j-1]$ 之类的信息。

这种写法的空间复杂度可以降到 $O(\min(m,n))$。

> 但是：一旦你把以前的行覆盖掉，就无法再回溯出完整的 LCS 序列，所以这种优化只适合**只要长度不要解**的场景。

---

## 9. 练习和扩展应用（P26–28）

PPT 最后提到几个练习与扩展：

* 练习 15.4-1：给定两个 0/1 序列，自己算一个 LCS 并编程实现。
* 练习 15.4-2：只用 c 表写 `PRINT-LCS`（P29 给了参考答案）。
* 练习 15.4-4：用滚动数组或一维数组实现只算长度的 LCS（P30–31 给了代码）。
* 练习 15.4-5：用 LCS 思想解决“最长递增子序列”问题。
* 还有一个扩展题 15-8：图像压缩中的 seam carving，也可以建成一个 DP 模型（P26 的论文截图）。

这些都是在告诉你：**LCS 只是 DP 的一个典型模型，很多“相似度”“路径选择”类问题都可以改写成类似的 DP 表格。**

---

## 10. 全章小结（再讲一遍这一章在干什么）

从“算法 + 案例 + 解法”的角度，这章做的事情可以压缩成这几条：

1. **应用场景**：比较两个序列（尤其是 DNA、文本、字符串）“有多相似”，用**最长公共子序列长度**来衡量相似度。
2. **基础概念**：

   * 子序列：从原序列中按顺序抽若干个元素，中间可跳过；
   * 公共子序列：同时是两个序列的子序列；
   * LCS：长度最大的公共子序列（可能有多个）。
3. **暴力想法**：枚举 $X$ 的所有子序列，检查是否也属于 $Y$，复杂度 $2^m$，不可行。
4. **动态规划四步法在 LCS 上的体现：**

   * **Step 1（最优子结构）：** 定理 15.1

     * 若末尾字符相同：LCS 末尾就是它，转成前缀子问题 $(m-1,n-1)$；
     * 若不同：LCS 要么是 $(m-1,n)$ 的 LCS，要么是 $(m,n-1)$ 的 LCS。
   * **Step 2（递归形式）：**
     定义 $c[i,j]=$ 长度为 $i$ 的前缀与长度为 $j$ 的前缀的 LCS 长度，得到递推式
     $$
     c[i,j]=
     \begin{cases}
     0, & i=0 \text{ 或 } j=0
     c[i-1,j-1]+1, & x_i=y_j
     \max(c[i-1,j],c[i,j-1]), & x_i\ne y_j
     \end{cases}
     $$
   * **Step 3（自底向上填表）：**
     算法 `LCS-LENGTH` 用一个 $c$ 表存长度、用 $b$ 表存方向，时间 $O(mn)$，空间 $O(mn)$。
   * **Step 4（恢复一个最优解）：**
     `PRINT-LCS(b,X,m,n)` 从右下角按箭头回溯，输出一个 LCS。
5. **改进**：

   * 可以只用 $c$ 表来回溯 LCS，不用 $b$ 表；
   * 如果只求长度，可以用两行甚至一行滚动数组，把空间从 $O(mn)$ 降到 $O(\min(m,n))$。

> 换句话说：
> **这一章就是用“最长公共子序列”这个例子，把动态规划的完整套路又演示了一遍：
> 找子问题 → 写递推 → 表格 DP → 记录信息恢复最优解 → 再做一些实现层面的优化。**

如果你愿意，我可以：

* 用具体的 $X=\text{ABCBDAB}$, $Y=\text{BDCABA}$，帮你**一步一步手算出整个 $c$ 表和 $b$ 表**，并且画出如何沿着箭头获得 LCS；
* 或者帮你写一个带详细中文注释的伪代码 / C / Python 版本，让你照着实现。



# a07-2

这一章是“**贪心算法（Greedy Algorithms）**”，主要围绕三个经典例子：

1. **活动安排问题（Activity Selection）**
2. **背包问题：0-1 背包 vs 分数背包**
3. **哈夫曼编码（Huffman Codes）用于数据压缩**

并用这些例子说明：贪心算法什么时候能给最优解，它和动态规划有什么区别。

我按 PPT 顺序给你讲。

---

## 一、贪心算法总览（P1–4）

* 很多**最优化问题**可以分成一系列步骤，每一步都有很多选择。

* **动态规划（DP）**：一般是

  > “先把所有子问题的最优解都算出来，再从中选。”

* **贪心算法（GA）**：则是

  > “在当前这一步，选看起来最有利的那个，然后只考虑剩下的子问题。”

贪心优点：

* 思路简单，代码短；
* 对某些问题，复杂度比 DP 小很多。

本章后面还提到：最小生成树、最短路、集合覆盖启发式等很多算法也是贪心思想。

---

## 二、16.1 活动安排（Activity-Selection Problem）

### 2.1 场景 & 问题描述（P5–8）

P5 页用“环球度假区游玩计划”的例子：一整天有很多表演/项目，每个项目都有开始时间和结束时间，而你一次只能参加一个项目，想去的项目当然越多越好。

抽象成算法问题：

* 有一组活动
  $$
  S = {a_1,a_2,\dots,a_n}
  $$
* 活动 $a_i$ 占用公共资源（比如操场、教室、会议室）时间区间为
  $$
  [s_i, f_i)
  $$
  其中 $s_i$ 是开始时间，$f_i$ 是结束时间。
* **两个活动冲突**：时间区间有交叉；
* **相容（compatible）**：区间不重叠。

> 目标：从中选出一个**最多活动数目的相容子集**。

P8 给了一个例子（已按结束时间排序）：

* 最优解可以是 ${a_1,a_3,a_6,a_8}$，
* 也可以是 ${a_2,a_5,a_7,a_9}$，
  说明最优解不唯一，但最大数量都是 4 个活动。

---

### 2.2 用 DP 的思路建子问题（P9–13）

先用“复杂一点”的 DP 看问题结构，然后再简化成贪心。

#### 2.2.1 子问题 $S_{ij}$ 的定义（P9–11）

先加两个“虚构活动”：

* $a_0=[-\infty,0)$
* $a_{n+1}=[\infty, +\infty+1)$

这样整个问题就是 $S_{0,n+1}$。

定义子问题集合：

$$
S_{ij} = {,a_k\in S : f_i \le s_k < f_k \le s_j,}
$$

也就是说：

> 所有在 $a_i$ 结束之后开始、并且在 $a_j$ 开始之前就结束的活动 $a_k$。

有一个假设：**把活动按结束时间非减排序**：

$$
f_0 \le f_1 \le \dots \le f_n < f_{n+1}
$$

在这个排序下，可以证明：

* 如果 $i\ge j$，则 $S_{ij}=\varnothing$（空集），相当于“没有可安排的活动”。

因此，只要考虑 $0\le i<j\le n+1$ 的子问题。

#### 2.2.2 最优子结构：$S_{ij}$ 里的一个活动 $a_k$（P12）

设 $S_{ij}$ 的某个最优解（一个最大数量的活动集合）中包含活动 $a_k$，那么：

* 在 $a_k$ 左边的活动来自子问题 $S_{ik}$；
* 在 $a_k$ 右边的活动来自子问题 $S_{kj}$；

于是有结构关系：

$$
\text{A}*{ij} = \text{A}*{ik} \cup {a_k} \cup \text{A}_{kj}
$$

这里 $\text{A}*{ij}$ 表示 $S*{ij}$ 的某个最优活动集合。

这说明：

> **原问题的最优解可以拆成两个子问题的最优解 + 当前选的活动**。

这是 DP 和贪心都离不开的“最优子结构”性质。

---

### 2.3 DP 递归式（还没贪心前）（P13–14）

定义：

* $c[i,j]$：子问题 $S_{ij}$ 的一个最优解里，活动的最大数量。

因为 $i\ge j$ 时 $S_{ij}$ 空集，所以 $c[i,j]=0$。

如果 $S_{ij}\neq\varnothing$，就要枚举可能作为“中间活动”的 $a_k$：

$$
c[i,j] = \max\limits_{a_k\in S_{ij}} {,c[i,k] + 1 + c[k,j],}
$$

再加上空集情况，就是 PPT 上的式 (16.3)：

$$
c[i,j] =
\begin{cases}
0, & S_{ij} = \varnothing,[4pt]
\max\limits_{i<k<j} {c[i,k] + 1 + c[k,j]}, & S_{ij} \neq \varnothing.
\end{cases}
$$

这个递推式可以写递归或 DP 表，**但是：**

* 每个子问题要试很多 $k$；
* 子问题数量是 $O(n^2)$；

虽然能解，但有点“用力过猛”。于是 PPT 在后面问：“能不能更简单？能不能直接贪心？”

---

### 2.4 关键定理：最早结束的活动一定在某个最优解里（P15–18）

**定理 16.1**（非常重要）：

在 $S_{ij}\neq\varnothing$ 时，令 $a_m$ 是 $S_{ij}$ 中**结束时间最早**的活动，即

$$
f_m = \min{,f_k : a_k\in S_{ij},}
$$

则有两点：

1. $a_m$ 一定包含在 **某个最大相容活动子集** 里（也就是某个最优解里）。

2. $S_{im} = \varnothing$，也就是说：

   > 选了 $a_m$ 以后，左边再也没有可选活动了，**只剩右侧一个子问题 $S_{mj}$**。

第二点很直观：如果还有 $a_\ell\in S_{im}$，它的结束时间会早于 $a_m$，那就和 “$a_m$ 是最早结束的活动” 矛盾。

定理的意义（P18）：

* 以前一个解要拆成两个子问题：$S_{ik}$ 和 $S_{kj}$；
* 有了这个定理，我们知道 **“选最早结束的活动” 是安全的贪心选择**，
  只剩下**一个子问题 $S_{mj}$** 要继续递归。
* 考虑的 $k$ 从“很多个”变成“唯一一个 $m$”。

---

### 2.5 贪心算法：递归版 REC-ACTIVITY-SELECTOR（P19–23）

利用定理 16.1，可以从 DP 换成简单的贪心递归。

前置条件：**所有活动已按结束时间升序排序**：

$$
f_1 \le f_2 \le \dots \le f_n
$$

再加上虚构活动 $a_0$（结束时间 $f_0=0$）。

递归函数 `REC-ACTIVITY-SELECTOR(s, f, i, n)` 的含义：

> 给定当前上一个选的活动是 $a_i$（第 $i$ 个），
> 从 $a_{i+1},\dots,a_n$ 中，按贪心规则选出一个**最大数量的相容活动集合**。

伪代码要点（P22–23）：

```text
REC-ACTIVITY-SELECTOR(s, f, i, n)
1 m ← i+1
2 while m ≤ n and s_m < f_i      // 找到第一个与 a_i 不冲突的活动
3     m ← m+1
4 if m ≤ n
5     return { a_m } ∪ REC-ACTIVITY-SELECTOR(s, f, m, n)
6 else
7     return ∅
```

* `while` 循环从 $a_{i+1}$ 开始往后找，第一个满足 $s_m \ge f_i$ 的活动，就是**当前子问题里结束时间最早且与 $a_i$ 相容的活动** $a_m$；
* 选它，然后递归解决剩下的子问题 $S_{m,n+1}$；
* 如果没找到（$m>n$），说明没有可选活动，返回空集。

初始调用：

```text
REC-ACTIVITY-SELECTOR(s, f, 0, n)
```

（把 $a_0$ 当成前一个活动，$f_0 = 0$。）

这就是**自顶向下的贪心**：每次都选“能选的活动中，结束最早的那个”。

---

### 2.6 迭代版：GREEDY-ACTIVITY-SELECTOR（P27）

由于递归是“尾递归”，可以轻松改成循环版：

```text
GREEDY-ACTIVITY-SELECTOR(s, f, n)
1 A ← {a_1}       // 先选第一个结束的活动
2 i ← 1
3 for m ← 2 to n
4     if s_m ≥ f_i
5         A ← A ∪ {a_m}
6         i ← m
7 return A
```

解释：

* 已按结束时间排序，所以**第一个结束的活动 $a_1$ 肯定可以选**；
* 然后从 $a_2$ 到 $a_n$ 依次看：

  * 如果它和上一个选的活动 $a_i$ 不冲突（$s_m\ge f_i$）就选它，并更新 $i=m$；
* 扫一遍就结束，每个活动只看一次。

> 这就是经典的“**最早结束时间优先**”活动安排贪心算法。

---

## 三、16.2 贪心策略要素 & 背包问题

### 3.1 贪心策略的几个关键点（P28–34）

PPT 总结了贪心算法与 DP 的关系（P28–33）：

* 两者都需要 **最优子结构**：

  > 原问题的最优解能由若干子问题的最优解拼出来。

* **不同点：**

  * DP：

    * **先解子问题**（bottom-up），
    * 用子问题最优解来作出选择。
  * 贪心：

    * **先做选择**（top-down），
    * 只留下一个（或少数）子问题继续解决。

要证明一个贪心算法是正确的，一般要做两件事：

1. **贪心选择性质（greedy-choice property）**：

   * 证明“在当前这一步做的那个局部最优选择”一定出现在某个全局最优解里，是“安全”的。
   * 对活动安排就是：选最早结束的活动 $a_m$ 总是安全的（定理 16.1）。
2. **最优子结构（optimal substructure）**：

   * 选完这一步以后，剩下来的子问题的最优解 + 这一步的贪心选择 = 原问题的最优解。

只有同时有这两个性质，贪心算法才会真正给出最优解。

---

### 3.2 背包问题：贪心 vs DP（P35–42）

PPT 用背包问题来对比：

* 有些问题既有最优子结构又有贪心选择性质（能用贪心）；
* 有些问题只有最优子结构，没有贪心选择性质（要用 DP）。

#### 3.2.1 0-1 背包问题（P35,42,66–67）

**问题模型：**

* 有 $n$ 件物品，第 $i$ 件价值 $v_i$，重量 $w_i$；
* 背包容量为 $W$；
* 每件物品**要么不拿，要么全部拿**（不能拆开）。

> 目标：在总重量 $\le W$ 的前提下，
> 让总价值 $\sum v_i$ 最大。

这个问题有最优子结构，但**没有适用的贪心选择**（比如按单位价值排序贪心是错的，后面有例子）。
因此通常用 **DP** 来解：

定义状态（见 P66–67）：

* $c[i,w]$：只考虑前 $i$ 件物品，在背包容量为 $w$ 时能取得的最大总价值。

转移方程：

$$
c[i,w] =
\begin{cases}
0, & i=0\text{ 或 } w=0;[4pt]
c[i-1,w], & w_i > w;[4pt]
\max{,c[i-1,w],\ v_i + c[i-1, w-w_i],}, & w_i \le w.
\end{cases}
$$

解释：

* 如果第 $i$ 件物品太重（$w_i>w$），根本不能选，只能继承 $c[i-1,w]$；
* 否则，有两种选择：

  * 不选它：价值 $c[i-1,w]$；
  * 选它（价值 $v_i$），剩余容量 $w-w_i$，再加上前 $i-1$ 件物品能装出的最好价值 $c[i-1,w-w_i]$；
* 在两者中取较大值。

DP 伪代码 `DP01-KNAP`（P67）就是双重循环按 $i=1..n$, $w=1..W$ 填表 $c[i,w]$。

如何回溯解：

* 若 $c[i,w] == c[i-1,w]$，说明第 $i$ 件没选；
* 否则第 $i$ 件被选中，然后把容量改为 $w-w_i$，继续看上一行。

---

#### 3.2.2 分数背包问题（Fractional Knapsack，P36,41）

**分数背包问题**：和 0-1 背包几乎一样，唯一不同是：

> 允许拿某个物品的一部分（比如拿半个金条）。

这时有一个非常自然的贪心策略：

* 计算每件物品的**单位价值**：
  $$
  \frac{v_i}{w_i}
  $$
* 按 $\frac{v_i}{w_i}$ **从大到小排序**；
* 从单位价值最高的开始往背包塞：

  * 如果还放得下整件物品，就全放进去；
  * 如果放不下，就把能放的那一部分填满背包，然后结束。

伪代码（P41 的 FRACTIONAL-KNAPSACK）：

```text
FRACTIONAL-KNAPSACK(v, w, W)
1 load ← 0        // 当前已装重量
2 i ← 1           // 物品已按 v_i / w_i 降序排序
3 while load < W and i ≤ n
4     if w_i ≤ W - load
5         // 能整件放入
6         take all of item i
7         load ← load + w_i
8         i ← i + 1
9     else
10        // 只能放一部分
11        take (W - load) of item i
12        break
```

这个问题既有最优子结构，又有贪心选择性质（可以证明“单位价值最高的物品优先是安全的”），所以贪心算法给出的就是最优解。

---

#### 3.2.3 贪心在 0-1 背包中失败的例子（P42）

例子（P42）：

| 物品 $i$    | 1  | 2   | 3   |
| --------- | -- | --- | --- |
| $v_i$     | 60 | 100 | 120 |
| $w_i$     | 10 | 20  | 30  |
| $v_i/w_i$ | 6  | 5   | 4   |

* 背包容量 $W=50$。
* 按单位价值贪心：先拿 1、再拿 2，总价值 $60+100=160$，总重量 30，还剩 20 的空间，但 3 重 30，放不下，结束。
* **最优解**：拿 2 和 3（不拿 1），重量 $20+30=50$，价值 $100+120=220$。

说明：

> 0-1 背包**只有最优子结构，没有贪心选择性质**，
> 所以不能简单靠贪心，必须用 DP 才能保证最优。

---

### 小结：这一节讲了什么？

1. 总结了贪心算法的两大要点：

   * 贪心选择性质；
   * 最优子结构。
2. 用**活动安排**说明如何从 DP 推出贪心算法。
3. 用**0-1 背包 vs 分数背包**说明：

   * 同样有最优子结构，
   * 但只有分数背包有贪心选择性质，0-1 背包要用 DP。

---

## 四、16.3 哈夫曼编码（Huffman Codes）

这一节是贪心算法的“重量级应用”：**数据压缩**。

### 4.1 背景 & 问题（P43–48）

* 文本文件可以看成是一个字符序列，每个字符来自一个有限字符集（字母表） $C$。
* 每个字符 $c\in C$ 在文件中出现了 $f(c)$ 次。
* 我们希望给每个字符分配一个**二进制串**作为编码（codeword），把整篇文本编码成 0/1 串，**总比特数尽量少**。

举例（P46–48）：

* 字母表只有 6 个字符：$a,b,c,d,e,f$；

* 出现次数（单位：千次）：

  | 字符 | a  | b  | c  | d  | e | f |
  | -- | -- | -- | -- | -- | - | - |
  | 频率 | 45 | 13 | 12 | 16 | 9 | 5 |

#### 定长码 vs 变长码

* **定长码**：每个字符用 3 位二进制编码，例如：

  * $a:000,\ b:001,\ c:010,\ d:011,\ e:100,\ f:101$；
  * 一个 10 万字符的文件需要 $3\times 10^5 = 300,000$ 位。

* **变长码**（P46 的示例）：

  * $a:0$
  * $b:101$
  * $c:100$
  * $d:111$
  * $e:1101$
  * $f:1100$

  总比特数为：

  $$
  (45\cdot1 + 13\cdot3 + 12\cdot3 + 16\cdot3 + 9\cdot4 + 5\cdot4)\times1000
  = 224,000
  $$

  比定长码 300,000 节省了 $25.3%$。

这个变长码其实就是后来证明的**最优编码**之一。

---

### 4.2 前缀码 & 编码树（P49–53）

#### 4.2.1 前缀码（prefix code）

一个二进制码是**前缀码**，如果：

> 没有任何一个字符的编码是另一个编码的前缀。

性质：

* 编码简单：把每个字符的 codeword 直接拼起来即可，例如
  “abc” $\to 0\cdot101\cdot100 = 0101100$。
* 解码也简单：从左到右扫描，只要读到某个前缀对应一个字符，就输出这个字符，然后从头重新匹配后面的位。
* 不会产生歧义。

例如 P50：

* 按上面的变长码，“aabe” 对应编码：

  $$
  0\ 0\ 101\ 1101 \Rightarrow 001011101
  $$

  解码时也唯一能还原成 “aabe”，不会被误读成其它分割方式。

#### 4.2.2 二叉编码树表示（P51–53）

前缀码可以用一棵**二叉树**来表示：

* 每个叶子节点对应一个字符；
* 从根到该叶子的“走法”（左边记为 0，右边记为 1）就是该字符的编码。

例如：

* 左左子树的叶子可能是字符 $a$，编码 00；
* 右–左–右子树的叶子是 $f$，编码 101 等。

对一个最优前缀码对应的树 $T$：

* 树是**满二叉树（full binary tree）**：每个内部结点都有两个孩子；
* 若字母表大小为 $|C|$，则树有 $|C|$ 个叶子和 $|C|-1$ 个内部结点。

---

### 4.3 编码树的代价 $B(T)$（P54）

令 $d_T(c)$ 表示字符 $c$ 在树 $T$ 中叶子的深度（根深度为 0），也就是编码长度。

若 $f(c)$ 是文件中字符 $c$ 出现的次数，那么这棵树编码整个文件所需的总比特数就是：

$$
B(T) = \sum_{c\in C} f(c), d_T(c)
$$

这就是树 $T$ 的**代价（cost）**。

> 我们的目标：在所有前缀码的编码树中，找一棵使 $B(T)$ 最小的树。
> 这样的树就是**最优编码树**。

哈夫曼算法要做的，就是构造这样一棵最优树。

---

### 4.4 哈夫曼贪心算法 HUFFMAN(C)（P55–56）

算法思路：

> 每次把**频率最低的两个结点合并**成一棵小树，
> 再把小树当成一个新的结点加入集合，
> 反复进行，直到只剩一棵树。

伪代码（P55）：

```text
HUFFMAN(C)
1 n ← |C|
2 把每个字符 c∈C 看作一棵只含一个叶子的树，放入最小优先队列 Q
3 重复 n-1 次：
4     从 Q 中取出频率最小的结点 x
5     从 Q 中再取出频率次小的结点 y
6     新建内部结点 z，令 left[z]=x, right[z]=y
7     f[z] = f[x] + f[y]     // 新结点的频率为两子结点之和
8     把 z 插回 Q
9 返回 Q 中剩下的那棵树（其根）
```

* 初始时有 $|C|$ 个小树（每个只有一个字符叶子）；
* 每次合并，树的个数减 1；
* 做 $|C|-1$ 次合并之后，就得到一棵大树——**哈夫曼树**；
* 从根到每个叶子的路径（左 0 右 1）就是最后的哈夫曼编码。

用最小堆实现优先队列，算法时间复杂度为 $O(|C|\log|C|)$。

P56 给了 6 个字符的完整合并过程，对应前面那个频率表，最终得到的树正好产生示例中的那组变长码。

---

### 4.5 正确性（只讲结论和直观，不讲细节推导）（P57–63）

PPT 给了两个关键引理来说明哈夫曼算法为什么是最优的：

#### 4.5.1 引理 16.2：贪心选择性质（P57–59）

> 在所有字符中，设 $x,y$ 是**频率最小的两个字符**，
> 那么存在一棵**最优前缀编码树**，使得：
>
> * $x,y$ 在这棵树中是**兄弟叶子**；
> * 它们具有相同的深度，并且只在最后一位编码上不同。

直观理解：

* 频率越小的字符，对总比特数影响越小，放在越深的位置越不会“吃亏”；
* 可以通过交换树中叶子的方式，把 $x,y$ 调到最深处成为兄弟，而不增加代价 $B(T)$；
* 这说明：**每次选择两个频率最小的字符合并，是安全的贪心选择**。

#### 4.5.2 引理 16.3：最优子结构性质（P60–62）

> 把 $x,y$ 合并成一个新的虚拟符号 $z$，其频率 $f[z]=f[x]+f[y]$，
> 得到新的字母表 $C' = C-{x,y}\cup{z}$。
> 若 $C'$ 的某个最优编码树是 $T'$，
> 把 $T'$ 中叶子 $z$ 替换成把 $x,y$ 作为孩子的新内部结点，就得到一棵树 $T$。
> **则 $T$ 是原字母表 $C$ 的最优编码树。**

意思是：

* 先对合并后的小问题 $C'$ 求最优树（这就是子问题的最优解）；
* 再把 $z$ 向下“拆成” $x,y$，不会破坏最优性。

这就说明哈夫曼算法是一个“**贪心选择 + 最优子结构**”的典型：

1. 贪心：每次合并频率最低的两个符号（引理 16.2 保证安全）；
2. 子问题：合并后形成更小的字母表 $C'$，对其递归求最优编码（引理 16.3 保证这样递归是对的）。

最后的定理 16.4（P63）直接得出结论：

> 过程 `HUFFMAN(C)` 产生的确是一个**最优前缀码**。

---

## 五、整章再总结一遍

这一章围绕“**贪心算法**”展开，按 PPT 顺序说就是：

1. **活动安排问题**

   * 模型：每个活动是时间区间 $[s_i,f_i)$，一次只能做一个活动；
   * 目标：选出最多个互不重叠的活动；
   * 先用 DP 定义子问题 $S_{ij}$ 和递推式 $c[i,j]$；
   * 再用定理 16.1 证明：在任何子问题中，**结束时间最早的活动必在某个最优解中**，且选完后只剩一个子问题；
   * 得到贪心算法：按结束时间排序，每次挑下一个**相容且结束最早**的活动；
   * 给出递归版 `REC-ACTIVITY-SELECTOR` 和迭代版 `GREEDY-ACTIVITY-SELECTOR`。
2. **贪心策略要素 & 背包问题**

   * 总结贪心策略与 DP 的关系：

     * 贪心需要“贪心选择性质 + 最优子结构”；
     * DP 只需要最优子结构。
   * **0-1 背包**：

     * 每件物品只能“取或不取”，没有贪心选择性质；
     * 用 DP 状态 $c[i,w]$ 和公式
       $$
       c[i,w] = \max{c[i-1,w],\ v_i + c[i-1,w-w_i]}
       $$
       来求最优，总复杂度 $O(nW)$。
   * **分数背包**：

     * 可以取物品的一部分；
     * 有贪心选择性质：按单位价值 $v_i/w_i$ 降序装入背包；
     * 算法 `FRACTIONAL-KNAPSACK` 即为最优解。
   * 用具体例子说明“按单位价值贪心”对 0-1 背包是错误的。
3. **哈夫曼编码（Huffman codes）**

   * 场景：给定各字符出现频率，要设计前缀码使编码文件所需比特数最少；
   * 用频率表展示定长码 vs 变长码的压缩率；
   * 引入“前缀码”“编码二叉树”“满二叉树”“代价 $B(T)$” 等概念；
   * 哈夫曼算法：用最小堆维护频率，每次取出两个最小频率结点合并，最终得到最优编码树，复杂度 $O(n\log n)$；
   * 用两个引理说明：

     * 最低频的两个字符可以设成兄弟叶子（贪心选择性质）；
     * 合并这两个字符形成子问题，再展开回来，保持最优性（最优子结构）。

整体来说，这章就是：

> 在学过 DP 之后，
> 告诉你什么时候可以“偷懒”：
> **只做局部看起来最好的选择**，
> 仍然能得到全局最优解，
> 并通过“活动安排、分数背包、哈夫曼编码”三个例子
> 把贪心算法的思想、证明套路和典型应用走了一遍。

如果你想，我可以下一步帮你：

* 用具体的活动时间数据，手算一次贪心选活动的过程；
* 或者用具体的频率表，手把手画出哈夫曼树并写出每个字符的编码。




# a08

这章 PPT 讲的是**图算法（第一部分）**，按顺序主要包括：

- 图的两种基本表示方法
- 图搜索：BFS、DFS
- 拓扑排序（基于 DFS）与强连通分量
- 最小生成树（Kruskal、Prim 只作简介）
- 单源最短路径：Bellman–Ford、拓扑排序版最短路（DAG）、Dijkstra

下面我就按 PPT 顺序，用中文把**每个算法是什么、解决什么典型问题、如何解**给你讲一遍。

------

## 22 图算法基础（Elementary Graph Algorithms）

### 22.1 图的表示：邻接表和邻接矩阵

虽然不完全是“算法”，但这是后面所有算法的基础。

- 图记为：$G=(V, E)$，$V$ 是顶点集合，$E$ 是边集合。
- **邻接表（Adjacency List）**
   用一个数组存所有顶点，每个顶点后面挂一个链表，链表里是“和它相邻的点”。
  
  - 空间：$\Theta(|V|+|E|)$
  - 适合**稀疏图**（边比较少），遍历邻接点效率高。
- **邻接矩阵（Adjacency Matrix）**
   用一个 $|V|\times|V|$ 的矩阵 $A=(a_{ij})$：
   $$
   a_{ij} = \begin{cases}
   1 & (i,j)\in E\\
   0 & \text{otherwise}
   \end{cases}
   $$
  - 空间：$\Theta(|V|^2)$
  - 操作简单（查“是否有边”是 $O(1)$），适合**稠密图**。

后面 BFS/DFS/最短路径都默认用**邻接表**来分析。

------

### 22.2 广度优先搜索 BFS（Breadth-First Search）

#### 1. 算法要解决的问题

- 从一个起点 $s$ 出发，把从 $s$ 能到达的所有点**按距离一圈一圈地找出来**。
- 对**无权图**（或者所有边权都一样，比如都看作长度 1），BFS 找到的就是从 $s$ 到其他点的**最短边数路径**。

典型应用（PPT 地铁图例子）：

> 给定地铁线路图（每一站看作一个顶点，每条线段一条无权边），求“从站 A 到站 B 经过**最少站数**”或者“最少换乘”的路径。
>  —— 解法：把地铁当作无权图，用 BFS，从起点站开始搜索，BFS 的层数就是经过的站数，回溯前驱节点即可得到最短路径。

#### 2. 核心思想

- 使用一个**队列** $Q$，“先进先出”。
- 搜索顺序：
  - 先访问所有距离 $s$ 为 1 的点；
  - 再访问所有距离为 2 的点；
  - ……

对每个顶点维护：

- 颜色：WHITE（未发现）、GRAY（已发现但还在队列里）、BLACK（已经出队、处理完邻接点）。
- $d[v]$：从 $s$ 到 $v$ 的当前最短边数估计（最后就是最短距离）。
- $\pi[v]$：搜索树中的**前驱**，用于还原路径。

#### 3. 算法步骤（邻接表版本）

输入：图 $G=(V,E)$、起点 $s$
 输出：每个点 $v$ 的最短距离 $d[v]$ 和前驱 $\pi[v]$

1. 初始化
   - 对所有 $v\in V$：
     - 设 $v.\text{color}=\text{WHITE}$，$d[v]=\infty$，$\pi[v]=\text{NIL}$
   - 对起点 $s$：$s.\text{color}=\text{GRAY},\ d[s]=0$
   - 队列 $Q\gets{s}$
2. 循环，直到队列空：
   - 从队列头**出队**一个顶点 $u$
   - 对 $u$ 的每个邻接点 $v$：
     - 如果 $v$ 还没有被发现（WHITE），则
        $$
        v.\text{color}=\text{GRAY},\ d[v]=d[u]+1,\ \pi[v]=u
        $$
        并把 $v$ 入队
   - $u.\text{color}=\text{BLACK}$

结论（PPT 上也直接给出）：

- BFS 的时间复杂度（邻接表）是
   $$
   O(|V|+|E|)
   $$
- 对于从 $s$ 可达的任意顶点 $v$，最终的 $d[v]$ 就是无权图最短路长度 $\delta(s,v)$。

------

### 22.3 深度优先搜索 DFS（Depth-First Search）

#### 1. 算法要解决的问题

DFS 是另一种遍历图的方式：

- 从一个顶点出发，**一路沿着一条路径走到底**，走不动再回溯到分叉点，换一条路继续。
- 广泛用于：
  - 判断图是否连通
  - 找所有连通分量
  - 拓扑排序
  - 强连通分量（SCC）
  - 检测有无环、分类边等

PPT 也提到：DFS 在 AI 程序、搜索问题中用得早、用得多。

#### 2. 核心思想：递归或栈

对每个顶点维护：

- 颜色：WHITE / GRAY / BLACK
- 发现时间 $d[v]$：第一次递归到这个点的时间戳
- 完成时间 $f[v]$：从这个点出发的所有邻接边都处理完的时间戳
- 前驱 $\pi[v]$

时间戳从 1 开始，每次访问/结束都自增。

#### 3. 算法步骤（邻接表版本）

输入：图 $G=(V,E)$
 输出：搜索森林（若图不连通会得到多棵 DFS 树）、每个点的 $d[v],f[v]$

1. 初始化：对所有 $u\in V$
   - $u.\text{color}=\text{WHITE}$
   - $u.\pi=\text{NIL}$
2. $time\gets 0$
3. 对所有顶点 $u\in V$：
   - 若 $u.\text{color}=\text{WHITE}$，调用 $\text{DFS-Visit}(u)$

递归过程 $\text{DFS-Visit}(u)$：

1. $time\gets time+1$, $u.d\gets time$
2. $u.\text{color}=\text{GRAY}$
3. 对每个邻接点 $v\in Adj[u]$：
   - 如果 $v.\text{color}=\text{WHITE}$，设 $v.\pi=u$，然后递归 $\text{DFS-Visit}(v)$
4. 所有邻接点处理完：
   - $u.\text{color}=\text{BLACK}$
   - $time\gets time+1$, $u.f\gets time$

复杂度结论：在邻接表下，DFS 也是
$$
 O(|V|+|E|)
$$

#### 4. DFS 的时间戳性质与边分类

**括号结构（Parenthesis Theorem）**：

- 每个顶点对应区间 $[d[u], f[u]]$。
- 若 $v$ 是 $u$ 的后代，则有**嵌套关系**：
   $$
   d[u] < d[v] < f[v] < f[u]
   $$
- 如果 $[d[u],f[u]]$ 和 $[d[v],f[v]]$ 不相交，则 $u,v$ 在 DFS 森林中互不为祖先/后代。

**边的分类（只在 DFS 上定义）**：

1. **树边（Tree edges）**：递归过程中第一次从 $u$ 访问到 $v$ 的边 $(u,v)$。
2. **后向边（Back edges）**：从一个结点指向其祖先的边。
3. **前向边（Forward edges）**：从结点指向其**严格后代**但不是树边的边。
4. **交叉边（Cross edges）**：既不是祖先-后代关系，又不在同一条 DFS 树路径上的边。

典型应用：

- 用 DFS 看一个有向图是否是 DAG：**存在后向边就有环**。
- 后面拓扑排序、强连通分量都基于 DFS 的这些性质。

------

### 22.4 拓扑排序（Topological Sort）

#### 1. 问题

- 

这代表“先做谁，再做谁”的依赖关系。

典型案例（PPT 衣服示例）：

- 结点：穿裤子、穿衬衫、系腰带、打领带、穿外套……

- 边：例如 “穿衬衫 $\to$ 打领带”，“穿裤子 $\to$ 系腰带”。

- 拓扑序就是一条合法的穿衣顺序，比如：

  > 衬衫 $\to$ 打领带 $\to$ 穿裤子 $\to$ 系腰带 $\to$ 穿外套 …

以及任务调度、课程先修关系、并行计算中的任务依赖等。

#### 2. 基于 DFS 的拓扑排序算法

思路（跟 PPT 一致）：

- 先对图做一次 DFS，记录每个顶点的**完成时间** $f[v]$。
- 将顶点按 $f[v]$ **从大到小排序**，这个顺序就是拓扑序。

原因（直观理解）：

- 对于边 $(u,v)$，DFS 不会在 $v$ 完成之后才去探索 $u$ 的未探索子树，所以必有 $f[u] > f[v]$，因此排序后 $u$ 在 $v$ 前。

伪代码简化版：

1. 对图做 DFS，记录所有 $f[v]$。
2. 把所有顶点按 $f[v]$ 从大到小排序，得到序列 $L$。
3. 输出 $L$ 即拓扑序。

复杂度（邻接表）：
$$
O(|V|+|E|)
$$
 （DFS + 排序，可以用“头插链表”的方式边 DFS 边插入，等价于按完成时间倒序）

------

### 22.5 强连通分量（Strongly Connected Components）

PPT 只简单画了图：用两次 DFS 把一个复杂有向图压缩成 DAG。

**问题**：在有向图中，找出所有的**强连通分量**（SCC）——每个分量里任意两点互相可达。

经典算法（Kosaraju）简述一下（PPT 也隐含是这个）：

1. 对原图 $G$ 做一次 DFS，记录所有顶点的完成时间 $f[v]$。
2. 构造**转置图** $G^T$（把每条有向边反向）。
3. 按照第一步得到的完成时间从大到小依次在 $G^T$ 上做 DFS。
   - 每次 DFS 遍历到的一整棵树就是一个 SCC。

这样可以把原图“降维”，每个 SCC 收缩成一个点，得到一个 DAG，方便后续分析。

------

## 23 最小生成树（Minimum Spanning Trees）

这一部分 PPT 只简单提到概念和应用，以及两种经典算法名字。

### 1. 问题定义

在一个**连通无向加权图**中，找到一棵包含所有顶点、且**无环**的子图（树），使得**边权之和最小**，这棵树叫**最小生成树（MST）**。

### 2. 典型案例

PPT 的例子：**电子电路设计** / **电缆布线**：

- 顶点：电路节点/城市/机房
- 边权：连线成本、长度
- 目标：用最少总成本把所有点连起来，但不能有回路（否则浪费线）。

### 3. 两个经典算法（都用贪心策略）

#### (1) Kruskal 算法（适合稀疏图）

步骤：

1. 把所有边按权重从小到大排序。
2. 初始时，每个顶点自己是一个连通分量。
3. 从小到大扫描边 $(u,v)$：
   - 如果 $u$ 和 $v$ 目前不在同一分量（加上这条边不会成环），就选这条边加入 MST，并把两个分量合并；
   - 否则跳过这条边。
4. 选到 $|V|-1$ 条边停止。

常用数据结构：**并查集（Disjoint Set / Union-Find）**。

#### (2) Prim 算法（适合稠密图）

步骤：

1. 随便选一个起点 $s$，把它加入生成树集合 $S$。
2. 每一步：
   - 从所有“连接 $S$ 与 $V\setminus S$ 的边”中，找一条权值最小的 $(u,v)$，其中 $u\in S$，$v\notin S$；
   - 把 $v$ 和边 $(u,v)$ 加入生成树集合。
3. 重复直到所有顶点都在 $S$ 中。

可以看作是对“连通边界”不断挑最便宜的边，逐渐“长”出一棵 MST。

------

## 24 单源最短路径（Single-Source Shortest Paths）

### 24.0 基本概念与问题分类

考虑一个**加权有向图** $G=(V,E)$，每条边有权值函数 $w:E\to\mathbb{R}$。

#### 路径权重

一条路径
$$
 p = \langle v_0,v_1,\dots,v_k\rangle
$$
 的权重定义为：
$$
w(p)=\sum_{i=1}^{k} w(v_{i-1},v_i)
$$

从顶点 $u$ 到 $v$ 的**最短路径权重**定义为：
$$
\delta(u,v)=\begin{cases}
 \min{w(p): p\text{ 是从 }u\text{ 到 }v\text{ 的路径}}, & \text{若存在路径}\\
 \infty, & \text{否则}
 \end{cases}
$$

#### 几类问题

- **单源最短路径**：给定源点 $s$，求 $\delta(s,v)$ 对所有 $v$。
- 单终点最短路径：可以在反向图里做单源。
- 单对最短路径：只关心某一对 $(s,t)$。
- 任意两点最短路径（All-pairs）：可以跑 $n$ 次单源，或者用 Floyd–Warshall 等（不在本 PPT）。

本章重点：**单源，边权可以为负或非负，图可有环**，选不同算法。

#### 性质：最优子结构

- 最短路径有**最优子结构**：一条最短路径中的任意一段子路径，本身也是对应端点间的最短路径。

------

### 24.1 Bellman–Ford 算法（BF）

#### 1. 适用范围

- **允许出现负权边**。
- 图中可以有环，但**不能有从源点可达的负权环**，否则“最短路径”没有意义（可以绕环无限减小）。

BF 的特点：

- 是**通用算法**：只要没有可达负环就能给出正确结果；
- 若存在可达负环，还能检测出来。

#### 2. 核心操作：松弛（Relaxation）

对边 $(u,v)$ 的松弛形式：

若
$$
v.d > u.d + w(u,v)
$$
 则更新：
$$
v.d \gets u.d + w(u,v),\quad v.\pi \gets u
$$

直观理解：发现一条更短的从 $s$ 到 $v$ 的路径，就“收紧” $v.d$。

#### 3. 算法思想

- 最长简单路径最多包含 $|V|-1$ 条边（再多就必有环）。
- 所以把**所有边**做松弛操作，如果重复 $|V|-1$ 轮，理论上所有最短路径都会“传递”到位。

步骤（简化版）：

1. **初始化单源**：
   - 对所有 $v$，$v.d=\infty,\ v.\pi=\text{NIL}$
   - $s.d=0$
2. 重复 $|V|-1$ 轮：
   - 对图中每条边 $(u,v)\in E$：做一次 `relax(u,v)`
3. 再扫描一遍所有边 $(u,v)$：
   - 如果还能满足 $v.d > u.d + w(u,v)$，说明存在可达负环（否则意味着还能变短）。
   - 此时“最短路径”不存在，可以直接报错或返回 `false`。

输出：每个 $v.d$ 和 $v.\pi$。

#### 4. 典型应用

- 存在**负权边**的情况：例如边权代表“收益”（负值代表优惠），或者某种代价差。
- 网络路由协议（如 RIP）类算法，也类似 BF 思想。

#### 5. 复杂度结论

- 每一轮遍历所有边一次，一共 $|V|-1$ 轮：
   $$
   O(|V|\cdot|E|)
   $$

------

### 24.2 基于拓扑排序的最短路（DAG Shortest Paths，TS 算法）

这是对**有向无环图（DAG）**的特殊情况，比 BF 快得多，也能处理负权边。

#### 1. 适用范围

- 图必须是 **DAG**（有向无环）。
- 边权可以是**负数**。

典型场景：任务依赖图、流水线、课程先修体系，边的权值是执行时间、延迟等。

#### 2. 算法思想

- 因为没有环，每条从源点出发的路径长度有限且不会“绕圈”。
- 只要按照拓扑序，一次性把所有边松弛一遍，就够了。

步骤：

1. 对图做拓扑排序，得到序列 $L$。
2. 初始化单源：$s.d=0$，其他 $v.d=\infty$，$\pi=\text{NIL}$。
3. 按拓扑序列从前到后扫描每个顶点 $u$：
   - 对每条出边 $(u,v)$ 执行一次 `relax(u,v)`。

因为拓扑序保证：到处理 $u$ 时，所有可能到达 $u$ 的路径都已经处理完了，$u.d$ 已经是最短的，所以每条边只需要松弛一次。

#### 3. 典型案例（PPT 提供的图）

PPT 中给出一个从顶点 $r$、$s$、$t$、$x$ 等组成的 DAG，按拓扑序依次对边松弛。你可以把它看成“多阶段任务”：

- 顶点：任务
- 边：任务之间的依赖关系，权值：执行时间
- 目标：某个源任务 $s$ 到其他任务的最早完成时间。

#### 4. 复杂度结论

- 拓扑排序：$O(|V|+|E|)$
- 遍历一次所有边松弛：$O(|V|+|E|)$
- 总复杂度：
   $$
   O(|V|+|E|)
   $$

------

### 24.3 Dijkstra 算法（贪心策略）

#### 1. 适用范围

- 图可以有环。
- **所有边权必须是非负的**：$w(u,v)\ge 0$。

典型案例（PPT 的百度地图例子）：

> 城市道路网，每条路有正长度或正耗时。求从北京到西安的最短路径。
>  —— 所有边权为距离/时间，非负，适合用 Dijkstra。

#### 2. 算法思想

Dijkstra 维持两个东西：

- 集合 $S$：已经确定**最短路径长度**的顶点集合。对每个 $u\in S$，$d[u]=\delta(s,u)$。
- 一个**最小优先队列** $Q$（通常用最小堆实现）：包含 $V\setminus S$ 中的顶点，按当前 $d[v]$ 排序。

大致过程：

1. 初始化：
   - 所有顶点 $v$：$d[v]=\infty,\ \pi[v]=\text{NIL}$
   - $d[s]=0$
   - $S=\varnothing$
   - $Q$ 中包含所有顶点（初始键值就是 $d[v]$）
2. 循环直到 $Q$ 为空：
   1. 从 $Q$ 里取出 $d[u]$ 最小的顶点 $u$（Extract-Min），加入 $S$。
   2. 对 $u$ 的每条出边 $(u,v)$：
      - 如果 $v\notin S$ 且
         $$
         d[v] > d[u] + w(u,v)
         $$
         就更新：
         $$
         d[v]\gets d[u] + w(u,v), \quad v.\pi\gets u
         $$
         并在优先队列中降低 $v$ 的键值（Decrease-Key）。

因为边权非负，一旦 $u$ 被从队列中“弹出”，就可以确定 $d[u]$ 已经是全局最短，不会以后再变小（这是 Dijkstra 的关键贪心正确性依据）。

#### 3. 典型案例

- 地图导航（道路长度/时间非负）。
- 网络路由（链路代价非负）。
- 任意需要求“正权图”单源最短路径的应用。

例如：从某个服务器到所有其它服务器的最小延迟路径；从起点城市到全国所有城市的最短距离。

#### 4. 为何不能有负边？

如果存在负边，就可能出现这样的情况：

- 在某步中，某个顶点 $u$ 被当成“当前最小”弹出，加入 $S$；
- 但之后通过某条包含负权边的路径再到 $u$，总权重变得比当前 $d[u]$ 更小。

这会违反“加入 $S$ 后 $d[u]$ 不再变”的贪心假设，导致算法错误。

#### 5. 复杂度结论（取决于优先队列实现）

- 若用简单数组实现优先队列：
  - Extract-Min 每次 $O(|V|)$，共 $|V|$ 次
  - 总复杂度 $O(|V|^2)$
- 若用二叉堆（最常见）：
  - Extract-Min：$O(\log |V|)$，$|V|$ 次
  - Decrease-Key：$O(\log |V|)$，最多 $|E|$ 次
  - 总复杂度：
     $$
     O\big((|V|+|E|)\log |V|\big)
     $$

------

### 24 小结（与 PPT 对应）

单源最短路三大算法的对比可以用表格记忆：

| 算法            | 允许负权边    | 允许环   | 时间复杂度（邻接表） | 适用图 |
| --------------- | ------------- | -------- | -------------------- | ------ |
| Bellman–Ford    | ✔             | ✔        | $O(                  | V      |
| DAG + Topo Sort | ✔             | ✖（DAG） | $O(                  | V      |
| Dijkstra        | ✖（必须非负） | ✔        | $O(                  | V      |

再加上：

- BFS：**无权图**最短路，$O(|V|+|E|)$。
- 图的表示：邻接表 / 邻接矩阵。
- 搜索：DFS、拓扑排序、强连通分量。
- 最小生成树：Kruskal、Prim（都用贪心）。

这些就是这章 PPT 中所有算法以及对应的典型问题和解法的主线内容。





# a09-1

这一章讲的是**任意两点最短路径（All-Pairs Shortest Paths）**，主要有两条线：

1. 用“矩阵乘法风格”的动态规划来求所有点对间的最短路；
2. 著名的 **Floyd–Warshall 算法**，同样是动态规划，但更简洁、效率也更高。

我按 PPT 的顺序，一段一段给你讲算法、典型场景和解法思路。

------

## 一、问题背景：任意两点最短路径

### 1.1 要解决什么问题？（P3）

我们以前学的都是**单源最短路**：给一个起点 $s$，求 $s$ 到所有点的最短路径。

这章要解决的是：

> 在一个图里，**每一对顶点 $(i,j)$** 之间的最短路径距离都要算出来。

典型场景（P3 公路地图例子）：

- 给一个全国公路网：
  - 顶点：城市
  - 边权：城市间的公路距离
- 需要做一本“公路里程表”，里面有一个 $n\times n$ 的表，每个单元格是“城市 $i$ 到城市 $j$ 的最短路程”。
- 这就是**任意两点最短路问题**（All-Pairs Shortest Paths）。

一种直观但笨的方法：

- 对每个城市 $i$ 跑一次单源最短路（比如 Bellman-Ford、Dijkstra），一共 $|V|$ 次；
- 把结果拼在一起，就是所有点对的最短路。

但本章更关心的是：**能不能一次性、统一地算出整个 $n\times n$ 的最短路矩阵？**

------

### 1.2 输入输出是怎样表示的？（P4）

本章大部分算法都用**邻接矩阵**表示图。

- 图有 $n$ 个顶点（编号 $1..n$），用一个 $n\times n$ 的矩阵 $W=(w_{ij})$ 表示边权：

$$
 w_{ij} =
 \begin{cases}
 0, & i=j\[4pt]
 \text{边 }(i,j)\text{ 的权重}, & i\ne j,\ (i,j)\in E\[4pt]
 \infty, & i\ne j,\ (i,j)\notin E
 \end{cases}
$$

- 输出是一个同样大小的矩阵 $D=(d_{ij})$：

$$
 d_{ij} = \delta(i,j)
$$

这里 $\delta(i,j)$ 表示从 $i$ 到 $j$ 的最短路径长度（如果不可达，就记为 $\infty$）。

P4 的图示：左边是输入的权重矩阵 $W$，右边是输出的最短路径矩阵 $D$，直观告诉你：算法就是把“直接边的距离矩阵”变成“最短路距离矩阵”。

------

### 1.3 除了距离，还要前驱矩阵 $\Pi$（P5）

只知道最短距离还不够，实际应用里我们通常还想要**具体路径**。

为此，P5 引入了**前驱矩阵**：

- 定义一个矩阵 $\Pi=(\pi_{ij})$：
  - 如果 $i=j$ 或 $i$ 到 $j$ 没有路径，$\pi_{ij}=\text{NIL}$；
  - 否则，$\pi_{ij}$ 是从 $i$ 到 $j$ 的某条最短路径上，顶点 $j$ 的前驱结点。

也就是说，**第 $i$ 行**记录的是“以 $i$ 为起点”的所有最短路径的前驱信息。

PPT 上画了一个例子（P5）：

- 上面是最短路径矩阵 $D$，下面是前驱矩阵 $\Pi$；
- 取第 2 行来看，相当于“以 2 为根”的一棵最短路树：
  - $2\to3$ 的最短路上，$3$ 的前驱是 $4$，所以 $\pi_{23}=4$；
  - $2\to4$ 的最短路上，$4$ 的前驱是 $2$，所以 $\pi_{24}=2$；
  - ……

通过这一行的 $\pi_{2j}$，就能画出一棵“从 2 出发的最短路树”。

后面 Floyd 算法也会给出这样的 $\Pi$ 矩阵，用来**还原路径**。

------

## 二、25.1 基于“矩阵乘法”的动态规划算法

这一节讲的是一类“从路径边数入手”的 DP 算法，看起来很像矩阵乘法，只不过普通矩阵乘法是“加法+乘法”，这里是“加法+取最小值”。

### 2.1 最短路径的“最优子结构”（P6）

先看一条从 $i$ 到 $j$ 的最短路径 $p$（假设 $i\ne j$）：

- 在 $p$ 上，设 $j$ 的前驱是 $k$，那么路径可以拆成：

$$
p = p' + (k,j)
$$

其中：

- $p'$ 是从 $i$ 到 $k$ 的那一段子路径；
- 整条路径的长度：

$$
 \delta(i,j) = \delta(i,k) + w_{kj}
$$

**关键结论：**

> 如果 $p$ 是 $i\to j$ 的最短路径，那么它的任意一段子路径（例如 $i\to k$ 的那段 $p'$）也是 $i\to k$ 的最短路径。

这叫做**最优子结构**，是很多 DP 算法的基础。

------

### 2.2 用“最多 $m$ 条边”的思路来写递推（P7–P9）

定义：

$$
l_{ij}^{(m)} = \text{从 } i \text{ 到 } j \text{，且边数最多为 $m$ 的所有路径中，最短路径的长度}
$$

如果图中没有**负权环**，那么任何最短路径的边数都不会超过 $n-1$。

于是有：

$$
 \delta(i,j) = l_{ij}^{(n-1)}
$$

接下来要找 $l_{ij}^{(m)}$ 的递推关系。

------

**递推式：**

考虑从 $i$ 到 $j$，边数最多为 $m$ 的最短路径。最后一条边一定是某个 $(k,j)$，于是：

- 前面一段 $i\to k$ 最多用 $m-1$ 条边；
- 那段的最短长度是 $l_{ik}^{(m-1)}$；
- 再加上最后一条边的长度 $w_{kj}$。

在所有可能的 $k$ 中取最小，就得到：

$$
 l_{ij}^{(m)} = \min_{1 \le k \le n} \big( l_{ik}^{(m-1)} + w_{kj} \big)
$$

**初始条件：**

- $m=0$ 时，没有中间边：
  - $l_{ii}^{(0)} = 0$
  - $l_{ij}^{(0)} = \infty,\ i\ne j$

这就是一个标准的 DP。

------

### 2.3 “慢速版”矩阵 DP 算法（P8–P9）

设 $L^{(m)}=(l_{ij}^{(m)})$，PPT 中给了伪代码 **EXTEND-SHORTEST-PATHS(L, W)**：

- 输入：上一轮的矩阵 $L^{(m-1)}$ 和权重矩阵 $W$；
- 输出：新的矩阵 $L^{(m)}$；
- 内部就是三重循环，按刚刚的递推式求每个 $l_{ij}^{(m)}$。

然后“**慢速版任意两点最短路算法**”是：

1. $L^{(1)} = W$
2. 对 $m=2,3,\dots,n-1$：
   - $L^{(m)} = \text{EXTEND-SHORTEST-PATHS}(L^{(m-1)}, W)$
3. 返回 $L^{(n-1)}$ 作为最终的最短路矩阵。

直观理解：

- 第一次只允许最多 1 条边（就是直接边）；
- 第二次允许最多 2 条边；
- …
- 最后允许最多 $n-1$ 条边，就囊括了所有可能长度的简单路径。

PPT 上提了运行时间是 $\Theta(n^4)$，你只需要记住这个结论即可。

------

### 2.4 用“重复平方”加速（P10–P12）

为了加速，PPT 讲了一个**改进版**：利用“重复平方”的思想。

观察：

- 一条“最多 $2m$ 条边”的最短路径，可以拆成两段，每段最多 $m$ 条边；
- 所以可以把 $L^{(m)}$ 和 $W$ 的乘法，换成 $L^{(m)}$ 和 $L^{(m)}$ 的“乘法”（还是用 $\min$ 和 $+$ 的规则）。

于是就有了“快速版”：

1. $L^{(1)} = W$，令 $m=1$；
2. 当 $m < n-1$ 时：
   - 计算 $L^{(2m)} = \text{EXTEND-SHORTEST-PATHS}(L^{(m)}, L^{(m)})$；
   - 把 $m$ 变成 $2m$；
3. 最后得到的 $L^{(m)}$，当 $m \ge n-1$ 时，就包含了所有最短路径长度。

相当于：

- 从 $1$ 条边跳到 $2$、$4$、$8$、……，只做 $O(\log n)$ 轮扩展；
- 每轮扩展是一次“矩阵乘法式”的运算，代价 $\Theta(n^3)$；
- 所以总时间是 $\Theta(n^3 \log n)$。

PPT 在 P11–P12 上给了这个对比：

- 慢速版：$\Theta(n^4)$
- 快速版：$\Theta(n^3\log n)$

这一节总结：**用“路径长度不超过 $m$”做状态的 DP，可以用矩阵形式写出，进而和普通矩阵乘法对比、优化。**

------

## 三、25.2 Floyd–Warshall 算法（重点）

这一节是本章真正的主角。它同样是 DP，但换了一个状态定义，使得算法变得**非常简单好写**，而且只需要 $\Theta(n^3)$ 的时间。

### 3.1 “之间顶点”的概念（P13–P15）

Floyd–Warshall 把注意力放在“**路径中允许出现哪些中间点**”上。

先定义：

- 对一条简单路径 $p = \langle v_1, v_2,\dots,v_\ell \rangle$：
  - 端点是 $v_1$ 和 $v_\ell$；
  - 其他顶点 ${v_2,\dots,v_{\ell-1}}$ 叫做这条路的**中间顶点**（intermediate vertices）。

然后，对于任意一对顶点 $(i,j)$ 和一个整数 $k$，考虑这样的路径集合：

> 从 $i$ 到 $j$ 的所有路径中，**所有中间顶点都在集合 ${1,2,\dots,k}$ 里**。

在这些路径中，取一条最短的，记它的长度为 $d_{ij}^{(k)}$。

特别地：

- 当 $k=0$ 时，不允许任何中间点，所以路径最多只有一条边（直接从 $i$ 到 $j$）。

------

**关键的结构性质（P14–P16）：**

对 $d_{ij}^{(k)}$ 对应的最短路径 $p$，有两种可能：

1. **路径 $p$ 不经过顶点 $k$**：

   - 那么 $p$ 的所有中间点都在 ${1,\dots,k-1}$ 里；

   - 所以 $p$ 已经是“只允许中间点在 ${1,\dots,k-1}$ 的最短路”，也就是：

     $$
      d_{ij}^{(k)} = d_{ij}^{(k-1)}
     $$

2. **路径 $p$ 经过顶点 $k$**：

   - 那么 $p$ 可以拆成两段：
      $$
      p = p_1 + p_2
      $$
      其中
      $p_1$ 是 $i\to k$ 的最短路，中间点在 ${1,\dots,k-1}$；
      $p_2$ 是 $k\to j$ 的最短路，中间点也在 ${1,\dots,k-1}$。

   - 所以：

     $$
      d_{ij}^{(k)} = d_{ik}^{(k-1)} + d_{kj}^{(k-1)}
     $$

综上，两种情况取更小的一个：

$$
 d_{ij}^{(k)} = \min \big( d_{ij}^{(k-1)},\ d_{ik}^{(k-1)} + d_{kj}^{(k-1)} \big)
$$

**初始条件：**

- $k=0$ 时，只允许路径长度最多一条边，所以：

$$
 d_{ij}^{(0)} = w_{ij}
$$

------

由于对任意路径来说，它的中间顶点肯定属于 ${1,\dots,n}$，所以最终的答案是：

$$
 d_{ij} = d_{ij}^{(n)},\quad \forall i,j
$$

这就是 Floyd–Warshall 的 DP 公式。

------

### 3.2 Floyd 算法的具体流程（P17）

P17 的伪代码 **FLOYD-WARSHALL(W)**，可以用自然语言描述如下：

1. 初始化：$D^{(0)} = W$；

2. 对 $k$ 从 $1$ 到 $n$：

   - 构造一个新矩阵 $D^{(k)}$；

   - 对所有 $i,j$：按公式

     $$
      d_{ij}^{(k)}
      = \min \big( d_{ij}^{(k-1)},\ d_{ik}^{(k-1)} + d_{kj}^{(k-1)} \big)
     $$

   - 也可以原地更新一个数组 $D$，不断覆盖。

3. 返回 $D^{(n)}$ 作为最终的最短路矩阵。

直观上：

- 外层循环 $k$：逐渐允许路径中使用更多编号的中间点；
- 内层双循环 $i,j$：用“是否经过 $k$” 来更新 $i \to j$ 的最短路长度。

结论（P21 给出的）：Floyd–Warshall 算法的时间复杂度是 $\Theta(n^3)$。

------

### 3.3 前驱矩阵 $\Pi^{(k)}$：构造具体最短路径（P18）

为了能输出**具体路径**，P18 又对 Floyd 定义了一组前驱矩阵：

- $\Pi^{(k)}=(\pi_{ij}^{(k)})$
- $\pi_{ij}^{(k)}$ 表示：在所有中间顶点属于 ${1,\dots,k}$ 的 $i\to j$ 最短路径中，顶点 $j$ 的前驱是谁。

初值（$k=0$）：

$$
\pi_{ij}^{(0)} =
 \begin{cases}
 \text{NIL}, & i=j \text{ 或 } w_{ij} = \infty\\
 i, & i\ne j,\ w_{ij} < \infty
 \end{cases}
$$

递推（$k\ge1$）：

- 若最短路径没有因为允许使用 $k$ 而变短，即

  $$
   d_{ij}^{(k)} = d_{ij}^{(k-1)}
  $$

  那么前驱不变：

  $$
   \pi_{ij}^{(k)} = \pi_{ij}^{(k-1)}
  $$

- 若使用 $k$ 可以得到更短的路径：

  $$
   d_{ij}^{(k)} = d_{ik}^{(k-1)} + d_{kj}^{(k-1)}
  $$

  那么从 $i$ 到 $j$ 的最短路是

  $i \to k \to j$ 的组合，其中最后一段是“从 $k$ 到 $j$ 的最短路”，

  所以 $j$ 的前驱就是那条 $k\to j$ 最短路中的前驱：

  $$
   \pi_{ij}^{(k)} = \pi_{kj}^{(k-1)}
  $$

P18 把这个逻辑画在黄色框里，写得非常清楚。

最终的前驱矩阵：

$$
 \Pi = \Pi^{(n)}
$$

配合 $D=D^{(n)}$，就可以像前面那章一样恢复出所有最短路径。

------

### 3.4 示例：如何用 Floyd + 前驱矩阵求具体路径？（P19–P20）

PPT 在 P19–P20 给了一个**带 5 个顶点的小有向图**（带负权但无负环），并详细展示了各轮 $D^{(k)}$ 和 $\Pi^{(k)}$ 的变化。

#### 例1：一个更新的计算过程（P19）

例如计算 $d_{42}^{(1)}$ 时：

$$
 d_{42}^{(1)} = \min \big( d_{42}^{(0)},\ d_{41}^{(0)} + d_{12}^{(0)} \big)
 = \min(\infty, 2+3) = 5
$$

含义：

- 初始时，从 $4$ 直接到 $2$ 没有边，所以 $d_{42}^{(0)} = \infty$；
- 允许中间顶点 ${1}$ 之后，可以走 $4\to1\to2$；
- $d_{41}^{(0)} = 2$，$d_{12}^{(0)} = 3$，加起来是 5；
- 因为 $5 < \infty$，于是更新为 5。

P19 中还展示了递推更新若干元素的具体数值。

#### 例2：如何恢复一条最短路径（P20）

在最终的 $D$ 矩阵中，有：

$$
 \delta(3,5) = 3
$$

P20 问：**“最短路径为 3，是哪条路？”** 然后在图上画出了这条路径：

$$
 3 \to 2 \to 4 \to 1 \to 5
$$

恢复方法就是利用最终的前驱矩阵 $\Pi$：

1. 从终点 5 往前查：$\pi_{3,5} = 1$，说明最短路上 5 的前驱是 1；
2. 再查：$\pi_{3,1} = 4$，说明 1 的前驱是 4；
3. 再查：$\pi_{3,4} = 2$；
4. 再查：$\pi_{3,2} = 3$；
5. 到起点 3 为止。

倒着读得到：$5 \leftarrow 1 \leftarrow 4 \leftarrow 2 \leftarrow 3$，顺过来就是

$$
 3 \to 2 \to 4 \to 1 \to 5
$$

这条路径的边权和就是 $3$（在 PPT 的矩阵里可见）。

------

### 3.5 为什么 Floyd 比“矩阵乘法 DP”快？（P21）

P21 做了一个对比：

- 基于“最多 $m$ 条边”的 DP，用的是下标 $m$ 和中间顶点 $k$ 两个层次，导致算法要做 $O(n)$ 轮矩阵“乘法”，总复杂度 $\Theta(n^4)$；

- Floyd 直接用一个维度 $k$ 来控制“允许的中间顶点集合”，递推式：

  $$
  d_{ij}^{(k)} = \min \big( d_{ij}^{(k-1)},\ d_{ik}^{(k-1)} + d_{kj}^{(k-1)} \big)
  $$

  只需要三层循环（$k,i,j$），自然就是 $\Theta(n^3)$。

简单说：

> Floyd 把状态设计得更巧，只用“允许中间点的集合”这个维度，把原来“边数”和“中间点集合”两个维度压缩成一个，所以既简单又快。

------

## 四、补充：传递闭包 & Johnson 算法（P22）

P22 只是简单提了一嘴两个扩展应用：

1. **有向图的传递闭包（Transitive closure）**
   - 问题：只关心“是否可达”，不关心具体距离；
   - Floyd 可以在**布尔矩阵**上改写：
     - 把 $+\ /\ \min$ 换成 $\land\ /\ \lor$，就能得到所有点对间的可达性；
   - 这就是 Warshall 算法的思想来源。
2. **Johnson 算法（适合稀疏图的任意两点最短路）**
   - 场景：图非常大但边比较“稀疏”，同时存在负权边但没有负环；
   - 思路（PPT 没展开）：
     - 先用 Bellman–Ford 做一次重新“加权”，把所有边权变成非负；
     - 然后对每个顶点作为源点跑一次 Dijkstra；
   - 这样在稀疏图上会比 Floyd 更快。

因为 PPT 上这两个部分标了星号，只是点名，不需要你现在掌握细节。

------

## 五、最后一页小结（P23）

最后一页是整个图算法部分的**方法论总结**，顺带也帮你回顾前面几章：

- BFS / DFS：属于枚举 + 队列/递归思想；
- 单源最短路（Dijkstra、Bellman–Ford）：用的是**贪心 + 松弛**；
- 本章的任意两点最短路（矩阵 DP、Floyd）：用的是**动态规划**；
- 还有最大流算法等等。

对你来说，这一章的核心就是：

1. 理解**任意两点最短路问题**的输入输出（矩阵 $W$、$D$、$\Pi$）；
2. 知道有两种主要的 DP 思路：
   - 基于“最多 $m$ 条边”的**矩阵乘法式 DP**（慢速版、快速版）；
   - 基于“中间顶点集合 ${1,\dots,k}$” 的 **Floyd–Warshall 算法**；
3. 会读懂 Floyd 的递推公式和伪代码，知道它的时间复杂度是 $\Theta(n^3)$；
4. 理解如何利用**前驱矩阵**恢复具体最短路径。

这样你再看 PPT 上的公式和示例，就能对号入座了。



# a09-2

这一章讲的是**最大流（Maximum Flow）和它的一些典型应用**，比如最大二分图匹配等。下面我严格按 PPT 的顺序，用中文把每个概念、算法和典型案例讲一遍，只讲“算法和怎么用”，不讲证明细节。

------

## 1. 最大流问题直观理解（P3–P4）

PPT 一开始用很多现实例子说明“流网络”：

- 石油 / 水管道网络
- 装配流水线上的产品流
- 电路中的电流
- 高速路上的车流
- 计算机网络中的数据流

**统一抽象：**

- 有一个**源点** $s$，不停地产生“流量”（水、油、车、数据……）。
- 有一个**汇点** $t$，不停地消耗这些流量。
- 各条边是“管道”，每条边有一个**容量** $c(u,v)$，表示单位时间内最多能通过多少流量。
- 我们要问：**从 $s$ 到 $t$，在不超过各边容量的前提下，最多能送多少流？** 这就是**最大流问题**。

P3 的图里有两个图：左边只有容量、流全为 0，叫 **capacity network（容量网络）**；右边在这些边上已经安排了一些流，就是一个 **flow network（流网络）**，图例中给了一个总流量是 19 的例子，问题是：**19 是不是最大？**

------

## 2. 26.1 流网络和流的基本定义（P7–P10）

### 2.1 流网络 $G=(V,E)$

- 这是一个**有向图**。
- 每条边 $(u,v)\in E$ 有一个**容量** $c(u,v)\ge 0$。
- 若 $(u,v)\notin E$，约定 $c(u,v)=0$。
- 每个点都在从 $s$ 到 $t$ 的某条路径上（否则可以删掉）。

**源点**：$s$
 **汇点**：$t$

------

### 2.2 流 $f$ 的定义

一个流是一个函数：

$$
 f:V\times V\to\mathbb{R}
$$

满足两条性质（这才配叫“流”）：

1. **容量约束（Capacity constraint）**

   对所有 $u,v\in V$：

   $$
   0 \le f(u,v) \le c(u,v)
   $$

   —— 流不能为负，也不能超过边的容量。

2. **流守恒（Flow conservation）**

   对所有**既不是源点也不是汇点**的顶点 $u\in V\setminus{s,t}$：

   $$
    \sum_{v\in V} f(v,u)
    = \sum_{v\in V} f(u,v)
   $$

   —— 流入 $u$ 的总流量 = 流出 $u$ 的总流量（中间节点不“产流”也不“吃流”）。

当 $(u,v)\notin E$ 时，我们强制规定 $f(u,v)=0$。

------

### 2.3 流的值 $|f|$ 与最大流问题（P9）

**流的值**是“从源点真正流出去多少”，公式是：
$$
|f| = \sum_{v\in V} f(s,v) - \sum_{v\in V} f(v,s)
$$

直观理解：**从 $s$ 流出去的总量减去流回 $s$ 的量**。
 由于中间点流守恒，也可以证明：对任意一个“从 $s$ 到 $t$ 的割（后面讲）”，净流量都等于 $|f|$。

> **最大流问题**：
>  给定流网络 $G$、源点 $s$ 和汇点 $t$，在满足容量约束和流守恒的前提下，找到一个流 $f$，使得 $|f|$ 最大。

P9 图上展示了一个网络，当前有一个流，$|f|=19$，并通过“增加 $s\to v_2$ 的流，再连锁影响到 $v_3$ 和 $t$”说明：只要还能想办法多送一点到 $t$，$|f|$ 就能增大。

------

### 2.4 多源多汇如何处理？（P10）

如果网络有多个源点、多个汇点，可以用标准技巧化成单源单汇：

- 新建一个超级源点 $s$，连向所有“原来的源点”，边容量足够大（比如各自可生产能力）。
- 新建一个超级汇点 $t$，所有“原来的汇点”连向 $t$，边容量为各自最大消耗能力。

这样就又回到“单源单汇”的最大流问题了。

------

## 3. 26.2 Ford–Fulkerson 方法：最大流的核心思路（P11–P26）

PPT 强调这是一个“方法”而不是具体“算法”，因为它只给了大的框架，里面“怎么找增广路径”有多种选择。

Ford–Fulkerson 方法依赖三个关键概念：

1. **残留网络（Residual network）**
2. **增广路径（Augmenting path）**
3. **割（Cut）和最大流–最小割定理**

下面按 PPT 顺序讲。

------

### 3.1 残留容量与残留网络（P12–P15）

先看“残留容量”：

给定原图中的一条边 $(u,v)$，容量为 $c(u,v)$，当前流为 $f(u,v)$。
 我们还可以在正向上再多送的流量就是：

$$
 c_f(u,v) = c(u,v) - f(u,v)
$$

此外，Ford–Fulkerson 还允许“**在反方向上回退流**”。
 如果原图有边 $(u,v)$，当前流 $f(u,v)$，那么我们也可以在残留网络中放一条反向边 $(v,u)$，其残留容量为：

$$
c_f(v,u) = f(u,v)
$$

综合起来，残留容量的定义（P12）可以写成：

$$
c_f(u,v) =
 \begin{cases}
 c(u,v) - f(u,v), & (u,v)\in E \\
 f(v,u), & (v,u)\in E \\
 0, & \text{otherwise}
 \end{cases}
$$

**直观**：

- 正向边：看还能再多送多少流。
- 反向边：看最多能“退回”多少以前送过的流。

有了 $c_f$，我们就可以定义**残留网络**：

$$
 G_f = (V, E_f)
$$

其中

$$
E_f = \{ (u,v)\in V\times V : c_f(u,v) > 0 \}
$$

也就是说，残留网络和原图顶点相同，但是只保留“还有富余容量”的方向边，并用 $c_f(u,v)$ 作为它们的容量。

P13–P14 的图对比了：

- (a) 原来的流网络
- (b) 根据当前流得到的残留网络（边变多，因为多了反向边）
- 再沿着某条增广路径增加流后，得到新的流网络，再构造新的残留网络……

**特别注意：**
 如果一开始流全是 0，那么
$$
 c_f(u,v) = c(u,v)
$$

此时残留网络就是原来的**容量网络本身**（P14）。

------

### 3.2 如何在残留网络里“再加一份流”？（P15–P18）

给定原图中一个流 $f$，以及在**残留网络** $G_f$ 中的一个新流 $f'$，
 我们可以把它们合成一个新流：
$$
(f \uparrow f')(u,v) = f(u,v) + f'(u,v)
$$

P15 的 Lemma 26.1 说明：

> $(f \uparrow f')$ 仍然是原图 $G$ 上的一个合法流，而且
>  $$
>  |f \uparrow f'| = |f| + |f'|
>  $$

也就是说，只要能在残留网络上再找一份流 $f'$，就能把原流 $f$ 的值提升 $|f'|$。

那么问题变成：**如何在 $G_f$ 中找到这样的 $f'$？**

------

### 3.3 增广路径与路径残留容量（P16–P18）

**增广路径（Augmenting path）**：

- 在残留网络 $G_f$ 中，从源点 $s$ 到汇点 $t$ 的一条**简单路径** $p$。

**路径的残留容量**定义为（P17）：
$$
c_f(p) = \min\{ c_f(u,v) : (u,v)\ \text{在路径 } p\ \text{上}\}
$$

也就是路径上所有边的残留容量的最小值——瓶颈边。

P17 的 Lemma 26.2：

> 对于残留网络中的一条增广路径 $p$，定义一个函数 $f_p$：
>  $$
>  f_p(u,v) =
>  \begin{cases}
>  c_f(p), & (u,v)\ \text{在 }p\ \text{上}\\
>  0, & \text{否则}
>  \end{cases}
>  $$
>  则 $f_p$ 是 $G_f$ 上的一个流，并且
>  $$
>  |f_p| = c_f(p) > 0
>  $$

进一步，P18 的推论（Corollary 26.3）：

> 把 $f_p$ 和原流 $f$ 合并：
>  $$
>  f' = f \uparrow f_p
>  $$
>  得到的新流 $f'$ 的值为：
>  $$
>  |f'| = |f| + |f_p| = |f| + c_f(p) > |f|
>  $$

**直观算法含义：**

- 在残留网络里找一条 $s\to t$ 的路径 $p$；
- 算出它的瓶颈容量 $c_f(p)$；
- 在原流 $f$ 上沿这条路径“压入” $c_f(p)$ 的流：
  - 正向边就增加流
  - 反向边就减少流（等价于撤销部分原来的流）
- 这样总流量就提高了 $c_f(p)$。

------

### 3.4 割（Cut）、净流和容量（P19–P21）

**割（Cut） $(S,T)$**：

- 把顶点集合 $V$ 划分为两个互不相交的集合 $S$ 和 $T=V\setminus S$，
- 要求源点 $s\in S$，汇点 $t\in T$。

**一个割的净流 $f(S,T)$** 定义为：
$$
f(S,T) =
 \sum_{u\in S}\sum_{v\in T} f(u,v) 
-
 \sum_{u\in S}\sum_{v\in T} f(v,u)
$$

就是从 $S$ 流到 $T$ 的流量减去从 $T$ 回到 $S$ 的流量。

**一个割的容量 $c(S,T)$** 定义为（注意只看正向容量）：
$$
 c(S,T) = \sum_{u\in S}\sum_{v\in T} c(u,v)
$$

P19 的图中给出一个割 $(S,T)$，计算出 $f(S,T)=19$，$c(S,T)=26$。

P20 的 Lemma 26.4：

> 对任意一个流 $f$ 和任意一个割 $(S,T)$，都有
>  $$
>  f(S,T) = |f|
>  $$
>  —— 也就是说，**任何割上的净流都等于流的总值**。

于是 P21 的推论（Corollary 26.5）：

> 任意流 $f$ 的值都不会超过任意割的容量：
>  $$
>  |f| \le c(S,T)
>  $$

因为沿着割 $(S,T)$ 的实际净流 $f(S,T)$ 一定不能超过这条割的总容量 $c(S,T)$。

------

### 3.5 最大流–最小割定理（P22–P23）

**定理 26.6（Max-flow min-cut theorem）**：

对一个流网络中的流 $f$，以下三条是等价的：

1. $f$ 是一个**最大流**；
2. 残留网络 $G_f$ 中**不存在增广路径**；
3. 存在某个割 $(S,T)$，使得
    $$
    |f| = c(S,T)
    $$

这就是著名的“**最大流 = 最小割**”定理：

- 最大可能的流量，等于最小容量割的容量。
- 当我们找到了一个流，使得残留网络再也找不到增广路径时，这个流就已经是最大流，而且与某个最小割容量相等。

P23 也顺便总结：求最大流有两种思路

1. 网络很小时：直接穷举所有割 $(S,T)$，找出容量最小的割，容量就是最大流值。
2. 网络较大时：用 Ford–Fulkerson 思想
   - 构造残留网络 $G_f$
   - 找增广路径 $p$
   - 计算 $c_f(p)$ 并沿路径增加流
   - 直到没有增广路径为止。

------

### 3.6 Ford–Fulkerson 算法伪代码与示例（P24–P25）

**基本 Ford–Fulkerson 算法：**

1. 初始所有边的流 $f(u,v)=0$。
2. 构造残留网络 $G_f$（一开始就是容量网络本身）。
3. 只要在 $G_f$ 中还能找到一条从 $s$ 到 $t$ 的路径 $p$：
   1. 计算路径的残留容量：
       $$
       c_f(p) = \min \{ c_f(u,v) : (u,v)\in p \}
       $$
   2. 对路径上的每条边做：
      - 如果是原图中的**正向边** $(u,v)\in E$：
         $$
         f(u,v) \gets f(u,v) + c_f(p)
         $$
      - 如果是残留网络中的**反向边** $(u,v)\notin E$，即原图有 $(v,u)$：
         $$
         f(v,u) \gets f(v,u) - c_f(p)
         $$
4. 当再也找不到增广路径时，当前的 $f$ 就是最大流。

P25 用了 6 张小图演示同一个网络上“不断选择增广路径、更新流和残留网络”的过程：

- (a) 初始流为 0，找到第一条增广路径，残留容量为 4，压入 4；
- 得到新流网络，再构造残留网络，继续找增广路径……
- 直到某一步残留网络里再也找不到从 $s$ 到 $t$ 的路径，说明最大流已经找到。

------

### 3.7 Ford–Fulkerson 的效率与缺点（P26–P29）

P26 给出一个结论：当边容量都是**整数**，且最大流值 $f^*$ 比较小时，
 基本 F–F 算法的时间复杂度（**结论即可**）：

$$
 O(|E|\cdot f^*)
$$

大致原因是：
 每次增广最少会让总流量增加 1，一共只需增广 $f^*$ 次；
 每次找增广路径 + 更新流的开销大致是 $O(|E|)$。

但如果 $f^*$ 很大呢？P27–P29 给了一个反例：
 通过不太聪明的“选增广路径方式”，即使最大流只有 10，每次只增广 1 单位，
 也要增广很多次，效率会很差。

**结论：**

- Ford–Fulkerson 方法本身只是一个框架，选择“哪条增广路径”很关键。
- 为了解决效率问题，需要一个更具体、更有保证的策略——这就引出 Edmonds–Karp 算法。

------

## 4. Edmonds–Karp 算法（P30–P32）

### 4.1 核心 idea（P30）

Edmonds–Karp 算法就是：

> 在 Ford–Fulkerson 框架中，**固定规定**：每次在残留网络 $G_f$ 中，用 **BFS（广度优先搜索）找一条最短的增广路径**（按边数算）。

也就是说，“增广路径 $p$”总是“从 $s$ 到 $t$ 的最少边数路径”。

伪代码和基本 F–F 一样，只是在第 3 行强调：用 BFS 找路径。
 算法结论上的时间复杂度为：
$$
 O(|V|\cdot |E|^2)
$$

你只需要记住这个结论，不必推导。

### 4.2 为什么这样做会更稳？（直观理解）

- 每次选择“最短增广路径”，可以证明：
  - 每条边成为“瓶颈边（增广路径上最小残留容量的边）”的次数是 $O(|V|)$ 次；
  - 总共最多出现 $O(|V|\cdot|E|)$ 次“关键边”；
- 每次增广（找到路径 + 更新流）开销 $O(|E|)$；
- 所以总时间为 $O(|V|\cdot|E|^2)$。

P31 有一个小图，示意 BFS 选择最短路径，残留网络逐渐变化。

### 4.3 实现要点（P32、P35）

实现 Edmonds–Karp 时的流程可以概括为：

1. 初始化流 $f(u,v)=0$。
2. 反复执行：
   1. 在当前残留网络 $G_f$ 上，用 BFS 找一条从 $s$ 到 $t$ 的最短路径 $p$；
   2. 若找不到，结束；
   3. 否则计算路径瓶颈 $c_f(p)$；
   4. 沿路径更新原图的流（正向加、反向减）。
3. 返回 $f$ 和 $|f|$。

P35 的练习题就是：让你用 Edmonds–Karp 算法，画出一个具体网络的增广过程——
 一轮轮给出流网络和残留网络的变化，直到得到最大流。

------

## 5. 最大二分图匹配（Maximum Bipartite Matching，P33）

这一节讲的是一个**非常经典的应用**：利用最大流来解“最大二分匹配”。

### 5.1 问题和典型例子

二分图：有两类顶点 $L$ 和 $R$，边只在 $L$ 与 $R$ 之间。

典型场景（P33 上列举了一堆）：

- $L$ : 机器； $R$ : 任务 —— 给机器分配任务；
- $L$ : 学生； $R$ : 奖学金 —— 每个学生能和哪些奖学金匹配；
- $L$ : 学生； $R$ : 导师；
- $L$ : 学生； $R$ : 公司；
- $L$ : 男士； $R$ : 女士；
- ……

**最大二分匹配问题**：
 在二分图中选出尽可能多的边，使得每个顶点**最多只被一条选中的边连接**（不能一个学生拿两个奖学金那种）。

### 5.2 转化为最大流的“构图方法”

对一个二分图 $G=(L\cup R, E)$，构造如下的流网络：

1. 新增一个源点 $s$，连向所有左侧顶点 $u\in L$，边 $(s,u)$ 的容量设置为 1。
2. 对原二分图中每条边 $(u,v)$，$u\in L, v\in R$，在流网络中保留这条边，容量为 1。
3. 新增一个汇点 $t$，让所有右侧顶点 $v\in R$ 通过边 $(v,t)$ 连接到 $t$，容量为 1。

然后在这个流网络上跑 Edmonds–Karp（或者其它正确的最大流算法），得到最大流 $f$。

- 对于每条 $u\in L, v\in R$ 间的边，如果 $f(u,v)=1$，就表示我们在匹配中选择了这条边；
- 所有这样的边组成的集合就是**一个最大匹配**；
- 最大流的值 $|f|$ 就是最大匹配的大小（匹配边条数）。

**直观理由**：

- 容量都是 1，保证每个点最多“通过 1 单位流”，也就是最多参与一条匹配边；
- 尽量让从 $s$ 到 $t$ 的流量变大，就相当于让参与匹配的边数尽量增多。

------

## 6. 提到但不展开的高级算法（P34）

P34 只是点名了一些更高级的内容：

- 26.4 **Push-relabel（压入–重标记）算法**
- 26.5 **Relabel-to-front（重标记–前移）算法**
- 第 29 章：**用线性规划来表示和求解最大流问题**

这些都是最大流的更高效或更一般化的求解方法，但 PPT 上用星号标注，属于拓展内容，本章不要求掌握细节。

------

## 7. 小结：这一章你需要记住什么？

1. **流网络与流的定义**
   
   - 边容量 $c(u,v)$；
   - 流 $f(u,v)$ 满足：
      $$
      0\le f(u,v)\le c(u,v),\quad \\
      \sum_{v} f(v,u) = \sum_{v} f(u,v)\quad (u\ne s,t)
      $$
   - 流的值：
      $$
      |f| = \sum_v f(s,v) - \sum_v f(v,s)
      $$
2. **残留容量与残留网络**
    $$
    c_f(u,v) =
    \begin{cases}
    c(u,v) - f(u,v), & (u,v)\in E\\
    f(v,u), & (v,u)\in E\\
    0, & \text{otherwise}
    \end{cases}
    $$
    在残留网络 $G_f$ 中寻找增广路径。
3. **增广路径与路径残留容量**
   - 增广路径：$G_f$ 中从 $s$ 到 $t$ 的简单路径 $p$；
   - 残留容量：
      $$
      c_f(p) = \min\{c_f(u,v):(u,v)\in p\}
      $$
   - 沿 $p$ 压入 $c_f(p)$ 的流可增加总流量。
4. **割与最大流–最小割定理**
   - 割 $(S,T)$ 的容量：
      $$
      c(S,T) = \sum_{u\in S,v\in T} c(u,v)
      $$
   - 对任意流 $f$，任意割 $(S,T)$：
      $$
      |f| = f(S,T) \le c(S,T)
      $$
   - 最大流–最小割定理：
      最大流值 = 某个最小割的容量；
      残留网络中无增广路径 $\Leftrightarrow$ 已经是最大流。
5. **Ford–Fulkerson 方法与 Edmonds–Karp 算法**
   - Ford–Fulkerson：反复在残留网络中找增广路径、压入瓶颈流，直到没有增广路径。
   - Edmonds–Karp：在 F–F 中规定用 BFS 找**最短增广路径**，时间复杂度为
      $$
      O(|V|\cdot|E|^2)
      $$
6. **最大二分图匹配的流模型**
   - 二分图 $L,R$；
   - 构造 $s\to L$ 和 $R\to t$ 的容量为 1 的边，中间 $L\to R$ 边容量为 1；
   - 跑最大流，流值 = 最大匹配大小，流量为 1 的 $L\to R$ 边就是匹配边。

掌握这些内容，再回去对照 PPT 的英文，你就可以只把英文当符号图看了，逻辑和算法思路都能用中文在脑子里复现。



# a10

这章是**计算几何入门**，主要围绕「二维平面上的点和线段」讲了几类典型算法：

- 利用**叉积**解决：
  - 向量顺/逆时针判断
  - 三点是左转还是右转
  - 两线段是否相交
- 用**扫描线（sweep line）**快速判断：一堆线段中是否存在相交对
- 用**栈 / gift wrapping**求点集的**凸包**：
  - Graham 扫描算法
  - Jarvis 包裹算法
- 用**分治法**求点集的**最近点对**

我按 PPT 的顺序，一块一块讲：都只讲“算法+典型场景+怎么干”，复杂度只给结论，不展开证明。

------

## 33.1 线段性质：叉积 + 三个基本问题

### 1. 凸组合和线段

给两点
$$
 p_1=(x_1,y_1),\quad p_2=(x_2,y_2)
$$

它们的**凸组合**是

$$
 p_3 = \alpha p_1 + (1-\alpha)p_2,\quad 0\le\alpha\le1
$$

也就是
$$
 x_3=\alpha x_1+(1-\alpha)x_2,\quad
 y_3=\alpha y_1+(1-\alpha)y_2
$$

几何意义：$p_3$ 就是**线段 $p_1p_2$ 上的某一点**（含端点）。所有凸组合构成的集合就是线段 $p_1p_2$。

------

### 2. 三个基础问题（Q1–Q3）

给 4～5 个点，PPT 提出 3 个常见问题：

1. **Q1：方向比较**
    两个共享端点的有向线段 $\overrightarrow{p_0p_1}$ 和 $\overrightarrow{p_0p_2}$，
    问：$\overrightarrow{p_0p_1}$ 相对 $\overrightarrow{p_0p_2}$ 是**顺时针**还是逆时针？
2. **Q2：左转/右转**
    走线段 $p_0p_1$ 再走 $p_1p_2$，在 $p_1$ 处是**左转**还是右转？
    等价于问：$\angle p_0p_1p_2$ 是向左还是向右弯。
3. **Q3：两线段是否相交**
    线段 $p_1p_2$ 与 $p_3p_4$ 是否有公共点（含端点重合、部分重合）？

PPT 强调：我们希望这些问题都能 **$O(1)$ 时间解决**，而且只用加减乘和比较，**不用除法和三角函数**，为了数值稳定。

核心工具就是——**叉积（cross product）**。

------

### 3. 叉积的定义与几何意义

把点当作向量，比如
$$
 p_1=(x_1,y_1),\quad p_2=(x_2,y_2)
$$

定义二维向量的叉积为矩阵行列式：

$$
p_1\times p_2=
 \begin{vmatrix}
 x_1 & y_1\\
 x_2 & y_2
 \end{vmatrix}
 = x_1y_2 - x_2y_1
$$

几何意义（PPT 图 33.1(a)）：

- $|p_1\times p_2|$ 等于由 $0,p_1,p_2,p_1+p_2$ 这四个点构成的**平行四边形的有符号面积**；
- 号的正负给出**方向信息**：
  - 若 $p_1\times p_2>0$，则相当于从 $p_2$ 旋转到 $p_1$ 是**顺时针**；
  - 若 $p_1\times p_2<0$，则从 $p_2$ 到 $p_1$ 是**逆时针**；
  - 若 $p_1\times p_2=0$，说明两向量**共线**。

图 33.1(b)(c) 在 $p$ 周围画了两块阴影区：浅色表示所有“顺时针”的向量，深色表示“逆时针”的向量。

------

### 4. 算法 1：判断一个向量相对另一个是顺时针还是逆时针（Q1）

**问题：**

> 已知两条有向线段 $\overrightarrow{p_0p_1}$ 和 $\overrightarrow{p_0p_2}$，问 $\overrightarrow{p_0p_1}$ 是否在 $\overrightarrow{p_0p_2}$ 的顺时针方向？

**做法：**

把 $p_0$ 平移到原点，相当于看向量：

$$
 \vec a = p_1-p_0,\quad \vec b = p_2-p_0
$$

计算叉积：

$$
 \vec a\times\vec b
 = (p_1-p_0)\times(p_2-p_0)
 = (x_1-x_0)(y_2-y_0)-(x_2-x_0)(y_1-y_0)
$$

- 若 $\vec a\times\vec b>0$：$\overrightarrow{p_0p_1}$ 在 $\overrightarrow{p_0p_2}$ 的**顺时针**一侧；
- 若 $<0$：在逆时针一侧；
- 若 $=0$：三点共线。

这样只用加减乘和比较，没有除法。

------

### 5. 算法 2：判断三点是左转还是右转（Q2）

**问题：**

> 已知三点 $p_0,p_1,p_2$，走 $p_0\to p_1\to p_2$，在 $p_1$ 处是左转、右转还是直走？

PPT 的技巧：还是用叉积，只是换个写法。

考虑向量：

- $\overrightarrow{p_0p_1}$
- $\overrightarrow{p_0p_2}$

计算

$$
 (p_2-p_0)\times(p_1-p_0)
$$

- 若结果 **$<0$**，说明 $\overrightarrow{p_0p_1}$ 相对于 $\overrightarrow{p_0p_2}$ 是**逆时针**，也就是在 $p_1$ 处**左转**；
- 若 **$>0$**，则在 $p_1$ 处右转；
- 若 **$=0$**，三点共线，既不左转也不右转。

图 33.2 就画了“左转”和“右转”的两种情况。

------

### 6. 算法 3：判断两线段是否相交（Q3）

**问题：**

> 给线段 $p_1p_2$ 和 $p_3p_4$，判断它们是否相交（包括端点重合、一个端点落在另一段上等所有情况）。

PPT 先讲概念：

- 线段 $p_1p_2$ **横跨（straddle）**一条直线：
   就是 $p_1$ 和 $p_2$ 在这条直线的两侧（叉积一正一负）；端点在直线上是边界情况。

**结论：两线段相交当且仅当：**

1. 每一条线段都**横跨**对方所在的直线；
2. **或者**，其中一个线段的端点落在另一个线段上（共线特例）。

为此 PPT 给了完整伪代码 `SEGMENTS-INTERSECT`：

- 辅助函数

  1. `DIRECTION(p_i,p_j,p_k)`：计算叉积
      $$
      (p_k-p_i)\times(p_j-p_i)
      $$
      给出三点相对方向（>0、<0、=0）。

  2. `ON-SEGMENT(p_i,p_j,p_k)`：已知 $p_k$ 与线段 $p_ip_j$ 共线，
      检查 $x_k$ 和 $y_k$ 是否都在 $[x_i,x_j]$、$[y_i,y_j]$ 的区间内，
      也就是看 $p_k$ 是否在线段 $p_ip_j$ 内部/端点：

     $$
      \min(x_i,x_j)\le x_k\le\max(x_i,x_j),\
      \min(y_i,y_j)\le y_k\le\max(y_i,y_j)
     $$

- 主过程 `SEGMENTS-INTERSECT(p1,p2,p3,p4)`：

  1. 计算
      $$
      d_1=DIRECTION(p_3,p_4,p_1),\
      d_2=DIRECTION(p_3,p_4,p_2)
      $$
      $$
      d_3=DIRECTION(p_1,p_2,p_3),\
      d_4=DIRECTION(p_1,p_2,p_4)
      $$
  2. 若 $d_1$ 与 $d_2$ 异号 **且** $d_3$ 与 $d_4$ 异号，则两段互相横跨，返回 `TRUE`。
  3. 否则检查四个“端点在线段上”的共线特例：
     - 若 $d_1=0$ 且 `ON-SEGMENT(p3,p4,p1)` 为真，也返回 `TRUE`；
     - 类似对 $p_2,p_3,p_4$ 分别检查。
  4. 否则返回 `FALSE`。

典型应用：

- 图形学里判断边界是否相交；
- 地理信息中多条道路是否交叉；
- 后面的扫描线算法、凸包算法都要频繁判断线段交和左/右转。

------

## 33.2 利用扫描线：判断是否存在相交线段对

**问题：**

> 输入 $n$ 条线段，只问一句：**有没有任何一对线段相交？**
>  不需要找出所有交点，也不关心交点坐标。

如果暴力两两判断，要做 $\binom{n}{2}$ 次，相当于 $O(n^2)$ 次 `SEGMENTS-INTERSECT`，线段很多时会很慢。

PPT 使用经典的**扫描线（sweeping）**技巧，把时间降到 $O(n\log n)$。

------

### 1. 扫描线思想（图 33.4 附近）

- 在平面上竖一条从左到右移动的**垂直直线**（扫描线）；
- 扫描线从最左侧开始，慢慢向右扫过所有线段的端点；
- 在某个位置，只关心**与扫描线相交的那些线段**，把它们按“与扫描线交点的 $y$ 坐标”从下到上排序，放入一个平衡树 $T$ 里。

**关键事实：**

若有哪两条线段相交，那么当扫描线走到靠近交点的位置时，这两条线段在 $T$ 中必定成为**相邻**的两条（上下紧挨着）。
 所以我们只需要关注每次插入/删除时，某条线段在 $T$ 中的**前驱和后继**是否与它相交，而不用所有成对检查。

------

### 2. 算法 ANY-SEGMENTS-INTERSECT 的步骤

伪代码结构（P25）大致是：

1. 建一个空的有序集合 $T$，用于维护“当前穿过扫描线的线段，按 $y$ 顺序”。
2. 把所有线段的端点按 $x$ 坐标从小到大排序（左端点在前，同 $x$ 时先处理左端后处理右端；再用 $y$ 打破平局）。
3. 依次扫描每个端点 $p$：
   - 如果 $p$ 是某条线段 $s$ 的**左端点**：
     1. 把 $s$ 插入 $T$ 中合适位置（用二分+叉积判断在谁的上面/下面）；
     2. 找到 $s$ 在 $T$ 中的上邻居 `ABOVE(T,s)` 和下邻居 `BELOW(T,s)`，
        - 若上邻居与 $s$ 相交，或下邻居与 $s$ 相交，则返回 `TRUE`。
   - 如果 $p$ 是某条线段 $s$ 的**右端点**：
     1. 在 $T$ 中找出 $s$ 的上邻居 a、下邻居 b；
     2. 删除 $s$；
     3. 如果 a 和 b 都存在，检查它们是否相交；若是，返回 `TRUE`。
4. 如果扫描完所有端点都没发现相交，返回 `FALSE`。

这里 `ABOVE/BELOW`、`INSERT`、`DELETE` 用的是红黑树一类的平衡树，能在 $O(\log n)$ 时间维护好顺序关系（P25 左下角特别标注“Chap13 红黑树”）。

**复杂度结论：**

- 排序端点：$O(n\log n)$；
- 每个端点最多做一次插入或删除，每次 $O(\log n)$，加上常数次相交检测；
- 总体：
   $$
   T(n)=O(n\log n)
   $$

典型应用：

- 快速判断一个复杂的折线网络是否内部自交；
- CAD、道路网络、VLSI 设计中检测“连线之间是否相撞”。

------

## 33.3 求点集的凸包（Convex Hull）

### 1. 凸包定义与直观理解

给点集 $Q={p_0,p_1,\dots,p_{n-1}}$，它的凸包 $\text{CH}(Q)$ 定义为：

> 面积最小的一个**凸多边形 $P$**，使得 $Q$ 中每个点要么在 $P$ 的边界上，要么在 $P$ 内部。

可以想象：
 把每个点看成木板上的钉子，外面套一根橡皮筋，松手后橡皮筋绷紧时形成的那条曲线，就是凸包的边界（图 33.6）。

**典型应用：**

- 求平面上**最远点对**：PPT 练习 33.3-3 让你证明，“最远的那两个点一定在凸包顶点里”。
- 碰撞检测、图形裁剪、模式识别等。

PPT 重点讲两种经典算法：

- **Graham’s scan**：$O(n\log n)$；
- **Jarvis’s march**（gift wrapping）：$O(nh)$，$h$ 是凸包顶点个数。

------

### 2. Graham 扫描算法：用栈 + 左转判断

**核心想法：**

1. 先按“极角”把点排一圈；
2. 然后模拟用橡皮筋绕一圈的过程：
    用一个栈 $S$ 存“当前可能是凸包顶点的点”，
    每加入一个新点，就看最后两条边在中间点处是否左转：
   - 左转：说明折线仍然向外鼓，新点进栈；
   - 直走或右转：说明栈顶那个点在凸包内部，弹出它，继续检查。

**步骤：**

设输入点集为 $Q$：

1. 找到 $Q$ 中 **$y$ 坐标最小**的点 $p_0$，若有并列，取最左的。
2. 把其余点按围绕 $p_0$ 的**极角从小到大**排序，得到序列
    $$
    \langle p_1,p_2,\dots,p_m\rangle
    $$
    若多个点极角相同，只保留距离 $p_0$ 最远的那个。
    比较极角也可用叉积实现（P37 页解释了“用叉积比较两点极角大小”）。
3. 初始化栈：
    $$
    S=\text{[ }p_0,p_1,p_2\text{ ]}
    $$
4. 对 $i=3$ 到 $m$：
   - 设
     - $p_{\text{top}}=\text{TOP}(S)$
     - $p_{\text{next}}=\text{NEXT-TO-TOP}(S)$
     - 当前新点为 $p_i$
   - 当三点 $(p_{\text{next}},p_{\text{top}},p_i)$ 构成的折线在 $p_{\text{top}}$ 处**不是左转**（即直线或右转）时：
     - `POP(S)`，把 $p_{\text{top}}$ 从栈中弹出（说明它不在凸包上）。
   - 当循环结束（折线在栈顶处变成左转）时，把 $p_i$ push 进栈：`PUSH(p_i,S)`。
5. 最后栈 $S$ 中从底到顶依次就是凸包顶点，按逆时针顺序。

“是不是左转”的判断，还是用前面说的叉积公式：

$$
 (p_i - p_{\text{next}}) \times (p_{\text{top}}-p_{\text{next}})
$$

- $<0$：左转；
- $\ge0$：非左转（直线或右转）。

PPT 上的连续图 (g)(h)(i) 展示了：加入每个新点时如何不断弹栈再压栈，最后形成外轮廓。

**复杂度结论：**

- 选 $p_0$：$O(n)$；
- 按极角排序：$O(n\log n)$；
- 主循环中，每个点最多压栈一次、弹栈一次，所以总共 $O(n)$。
- 综合：
   $$
   T(n)=O(n\log n)
   $$

------

### 3. Jarvis 包裹算法（gift wrapping）

**直观故事：**

- 想象你拿一张硬纸，把它一边贴在最左下角的点 $p_0$，另一边拉到右方；
- 然后慢慢往上旋转，直到纸的边第一次碰到另一点，那点一定是凸包顶点 $p_1$；
- 接着把纸一边固定在 $p_1$，继续旋转找 $p_2$，……
- 这样一圈“包裹”下来，回到 $p_0$ 时就得到整个凸包。

**算法步骤：**

1. 找到一个显然在凸包上的点，比如 **最左（或最下）**的点 $p_0$，作为起点。
2. 把 $p_0$ 作为当前点 $p$，然后重复：
   - 在所有其它点中，找一个点 $q$，使得对任意其他点 $r$，
      向量 $\overrightarrow{pq}$ 相比 $\overrightarrow{pr}$ **最“逆时针”**（或者等价地，与当前基准方向极角最小）。
      对于当前候选 $q$ 和另一个点 $r$，比较：
      $$
      (r-p)\times(q-p)
      $$
     - 若 $>0$，则 $r$ 更逆时针，把 $q$ 换成 $r$；
     - 否则保持 $q$。
   - 当扫描完所有点后，$q$ 就是下一个凸包顶点。
   - 把 $q$ 输出到凸包序列中，并令 $p\leftarrow q$ 继续。
3. 直到新的 $q$ 又回到 $p_0$，停止。

也就是说，每确定一个凸包顶点，就需要对全部 $n$ 个点扫描一遍。
 若凸包有 $h$ 个顶点，总工作量是 $O(nh)$。

**复杂度结论：**

$$
T(n,h)=O(nh)
$$

- 当凸包点数 $h$ 很少（例如所有点都集中在内部），Jarvis 算法非常快；
- 当 $h$ 接近 $n$（比如点都在圆上），就变成 $O(n^2)$，反而比 Graham 慢。

------

## 33.4 最近点对问题（Closest Pair of Points，标星节）

这一节打了星号，属于扩展内容，大意如下：

### 1. 距离度量

在平面上，两点 $p_1=(x_1,y_1)$、$p_2=(x_2,y_2)$ 的 Minkowski 距离：

$$
L_m(p_1,p_2) = \big(|x_1-x_2|^m + |y_1-y_2|^m\big)^{1/m}
$$
常用特殊情况：

- $L_1$：曼哈顿距离
$$
   |x_1-x_2|+|y_1-y_2|
$$
- $L_2$：欧几里得距离（普通直线距离）
- $L_\infty$：$\max(|x_1-x_2|,|y_1-y_2|)$，视觉距离/棋王距离

通常最近点对问题指 **欧几里得距离**。

------

### 2. 暴力解法（Brute Force）

**问题：**

> 给 $n$ 个点，找出距离最近的一对。

最直接的方法：对所有点对 $(i,j)$ 计算一遍距离：

- 总共 $\binom{n}{2} = O(n^2)$ 个点对；
- 每次距离计算是 $O(1)$；
- 总复杂度：$O(n^2)$。

点很多时会比较慢。

------

### 3. 分治算法（Divide and Conquer，大致思路）

PPT 只写了一句“divide-and-conquer: $O(n\log n)$”，这里用通俗版概括一下：

1. **预处理**：按 $x$ 坐标把所有点排序一次。
2. **分治**：
   - 把点集按中间的 $x$ 坐标分成左右两半 $L$ 和 $R$；
   - 递归求出左半最近距离 $d_L$，右半最近距离 $d_R$；
   - 令 $d=\min(d_L,d_R)$。
3. **合并**（关键步骤）：
   - 只需考虑“一点在左、一点在右”的最近点对；
   - 可以证明：如果这样的最近点对距离小于 $d$，那么这两点必定都在垂直分界线两侧距离不超过 $d$ 的细长“带状区”里；
   - 把这条带状区内的点按 $y$ 坐标排序，利用几何性质可以证明：
      每个点只需要和它在 $y$ 方向上**常数个后继**比较距离即可（经典结论：最多检查 6~8 个邻居），
      就能保证不会漏掉最近点对。
4. 综合起来，每层递归是 $O(n)$ 工作，递归深度是 $O(\log n)$，所以总复杂度是：
$$
 T(n)=O(n\log n)
$$

------

## 最后小结：这一章的算法武器库

这章其实是在给你展示“计算几何里常用的几种算法技巧”：

1. **叉积（cross product）**
   - 用符号判断方向：顺/逆时针、左转/右转、是否共线；
   - 做线段相交判定 (`SEGMENTS-INTERSECT`) 的基础。
2. **扫描线（sweep line）+ 平衡树**
   - 解“是否存在相交线段对？”
   - 思路：按 $x$ 排序端点，用扫描线维护当前线段的垂直顺序，只检查相邻线段。
   - 复杂度：$O(n\log n)$。
3. **栈 + 左转测试**：Graham 扫描凸包
   - 先按极角排序，再用栈维护“外轮廓”，遇到非左转就弹出栈顶。
   - 复杂度：$O(n\log n)$。
4. **包裹（gift wrapping）**：Jarvis 凸包
   - 模拟用纸包裹点集，每次找“最逆时针”的下一点。
   - 复杂度：$O(nh)$，适合 $h$ 很小的情况。
5. **分治**：最近点对
   - 左右递归 + 带状区检查少量邻居。
   - 复杂度：$O(n\log n)$。

理解了这些算法，你再看 PPT 上的英文，只需要把英文当成符号和图例，对照这里的中文理解即可。



# a11

这章主要讲了三类东西：

1. **多项式和卷积的基本概念与算法**
2. **离散傅里叶变换 DFT**（把“系数形式”变成“点值形式”）
3. **快速傅里叶变换 FFT**（用分治把 DFT 从 $,\Theta(n^2),$ 加速到 $,\Theta(n\log n)$，从而把多项式乘法也加速到 $,\Theta(n\log n)$） 

我按 PPT 的顺序，用中文、只讲**算法相关内容和典型例子**，所有公式都用 `$` / `$$` 包起来。

---

## 一、导入：多项式乘法和 FFT

设有两个多项式（最高次数都 $<n$）：

$$
A(x)=\sum_{j=0}^{n-1} a_j x^j,\qquad
B(x)=\sum_{j=0}^{n-1} b_j x^j
$$

* **相加**：
  $$C(x)=A(x)+B(x)=\sum_{j=0}^{n-1} c_j x^j,\quad c_j=a_j+b_j$$
  只要把对应系数相加一遍，时间 $\Theta(n)$。

* **直接相乘（“学校里的竖式乘法”）**：

  $$C(x)=A(x)B(x)=\sum_{j=0}^{2n-2} c_j x^j$$

  每个 $c_j$ 都是卷积：

  $$
  c_j=\sum_{k=0}^{j} a_k,b_{j-k}
  $$

  要让每个 $a_k$ 和每个 $b_\ell$ 都相乘一次，所以总共大约 $n^2$ 次乘法，时间 $\Theta(n^2)$。

本章的核心问题就是：

> 能不能更快地做多项式乘法？

答案：用 **FFT** 可以做到 **$\Theta(n\log n)$**。

---

## 二、30.1 多项式表示与卷积

### 1. 系数表示（coefficient representation）

就是刚才的写法：

$$
A(x)=\sum_{j=0}^{n-1} a_j x^j
$$

把系数按次序排成一个向量：

$$
\mathbf{a}=(a_0,a_1,\dots,a_{n-1})^T
$$

典型算法：

1. **多项式求值**（给定 $x_0$，算 $A(x_0)$）
   用 Horner 法则（合并括号从里往外算）：

   $$
   A(x_0)=a_0+x_0(a_1+x_0(a_2+\dots+x_0 a_{n-1})\dots)
   $$

   只要从高次往低次扫一遍，时间 $\Theta(n)$。

2. **加法**：前面已说，直接对每一位做 $c_j=a_j+b_j$，$\Theta(n)$。

3. **乘法与卷积**
   PPT 把“系数卷积”单独拿出来说：设

   $$
   \mathbf{a}=(a_0,\dots,a_{n-1}),\quad
   \mathbf{b}=(b_0,\dots,b_{n-1})
   $$

   定义卷积

   $$
   \mathbf{c}=\mathbf{a}\odot\mathbf{b},\quad
   c_j=\sum_{k=0}^{j} a_k b_{j-k},\ j=0,\dots,2n-2
   $$

   这恰好就是多项式乘法 $C(x)=A(x)B(x)$ 的系数。
   直接算卷积需要 $\Theta(n^2)$。

> **典型应用 1：多项式乘法**

* 输入：两个长度为 $n$ 的系数数组 `a[0..n-1]`、`b[0..n-1]`
* 直接算法（不用 FFT）：

  ```text
  for j = 0..2n-2:
      c[j] = 0
      for k = 0..j:
          c[j] += a[k] * b[j-k]
  ```
* 输出：`c[0..2n-2]`，对应 $C(x)=A(x)B(x)$。
* 时间：$\Theta(n^2)$。

---

### 2. 点值表示（point-value representation）

另一种表示法是：选 $n$ 个互不相同的点 $x_0,\dots,x_{n-1}$，记

$$
y_k = A(x_k),\quad k=0,\dots,n-1
$$

多项式就表示成一堆点：

$$
{(x_0,y_0),\ (x_1,y_1),\dots,(x_{n-1},y_{n-1})}
$$

这就是“点值表示”。

**重要结论（唯一性定理）**：
若 $x_0,\dots,x_{n-1}$ 两两不同，那么存在唯一的“次数 $<n$ 的多项式” $A(x)$，使得

$$
A(x_k)=y_k,\quad k=0,\dots,n-1
$$

即：**点值表示与系数表示一一对应**。

> **典型应用 2：插值**

* 输入：$n$ 个点 $(x_k,y_k)$，$x_k$ 各不相同
* 问题：找多项式 $A(x)$（次数 $<n$）满足 $A(x_k)=y_k$
* 一种公式（拉格朗日插值）是：

  $$
  A(x)=\sum_{k=0}^{n-1} y_k L_k(x),\quad
  L_k(x)=\prod_{j\ne k}\frac{x-x_j}{x_k-x_j}
  $$

  算出所有 $L_k(x)$ 的系数，就能得到 $A(x)$ 的系数向量。
  这个算法复杂度是 $\Theta(n^2)$。

（PPT 只是提一下这个结论，真正的重点在后面用 FFT 做更快的“插值”。）

---

### 3. 点值形式下的加法和乘法

点值形式有个非常舒服的性质：

* 若 $C(x)=A(x)+B(x)$，并且
  $$
  A(x_k)=y_k,\quad B(x_k)=z_k
  $$
  那么
  $$
  C(x_k)=y_k+z_k
  $$
  所以只要把每个点的函数值相加：

  $$
  (x_k,,y_k) + (x_k,,z_k) \Rightarrow (x_k,,y_k+z_k)
  $$

  只需 $\Theta(n)$。

* 若 $C(x)=A(x)B(x)$，则

  $$
  C(x_k)=A(x_k)B(x_k)=y_k z_k
  $$

  所以在**同一批点**上，只要把函数值相乘：

  $$
  (x_k,,y_k),\ (x_k,,z_k)\Rightarrow (x_k,,y_k z_k)
  $$

  仍然只要 $\Theta(n)$！

问题：多项式乘积 $C(x)$ 的次数可达 $2n-2$，**需要至少 $2n$ 个点值**才能唯一确定 $C(x)$。因此：

* 要做点值相乘，必须先把 $A,B$ 从 $n$ 个点扩展（zero-padding 系数、再求值）到 $2n$ 个点。

---

### 4. 用点值表示实现“快速多项式乘法”

PPT 里的大图给出了完整流程：

1. **输入**：$A,B$ 的系数表示：
   $$
   \mathbf{a}=(a_0,\dots,a_{n-1}),\quad
   \mathbf{b}=(b_0,\dots,b_{n-1})
   $$
   
2. **把每个多项式扩展到长度 $2n$ 的系数**（后面补 $0$）：

   $$
   (a_0,\dots,a_{n-1},0,\dots,0)
   $$

3. **把扩展后的系数转成在 $2n$ 个点上的点值**：
   这一步就是“多点求值（evaluation）”，如果用 FFT，只要 $\Theta(n\log n)$。

4. **在点值形式下，逐点相乘**：

   $$
   (x_k,,y_k) \cdot (x_k,,z_k) \Rightarrow (x_k,,y_k z_k)
   $$

   时间仅 $\Theta(n)$。

5. **对乘完之后的 $2n$ 个点做插值（interpolation）**，还原为 $C(x)$ 的系数。
   这一步用反向的 FFT（逆 DFT）也只要 $\Theta(n\log n)$。

综合下来，多项式乘法的时间是

$$
\Theta(n\log n)+\Theta(n)+\Theta(n\log n)=\Theta(n\log n)
$$

这就是定理 30.2 的结论。

> **典型应用 3：用 FFT 做多项式乘法 / 卷积**

* 输入：两个长度 $n$ 的实数或复数序列 $\mathbf{a},\mathbf{b}$
* 把它们视为多项式系数，零填充到长度 $2n$
* 算法步骤：

  1. 用长度 $2n$ 的 FFT 算 $\text{DFT}*{2n}(\mathbf{a}),\ \text{DFT}*{2n}(\mathbf{b})$；
  2. 逐点相乘：$\mathbf{c}'_k=\mathbf{A}_k\mathbf{B}_k$；
  3. 对 $\mathbf{c}'$ 做长度 $2n$ 的逆 FFT，得到卷积系数 $\mathbf{c}$；
* 输出：$\mathbf{c}$，就是卷积 $\mathbf{a}\odot\mathbf{b}$，也就是多项式 $A(x)B(x)$ 的系数。

---

## 三、30.2 复数单位根、DFT 和 FFT

### 1. 复数单位根（roots of unity）

设

$$
\omega_n = e^{2\pi i/n}
$$

称 $\omega_n$ 为“**主 $n$ 次单位根**”。所有的 $n$ 次单位根是

$$
1,\ \omega_n,\ \omega_n^2,\ \dots,\ \omega_n^{n-1}
$$

它们刚好在复平面的单位圆上均匀分布。欧拉公式是：

$$
e^{iu} = \cos u + i\sin u
$$

所以

$$
\omega_n^k = e^{2\pi i k/n} = \cos\frac{2\pi k}{n}+i\sin\frac{2\pi k}{n}
$$

这部分 PPT 讲了几个性质（只给结论）：

1. **有限群性质**
   $$
   \omega_n^n = 1,\quad
   \omega_n^{j+k} = \omega_n^j \omega_n^k
   $$
   
2. **若 $n$ 为偶数，则**

   $$
   \omega_n^{n/2} = -1
   $$

   （在单位圆上转半圈就是 $-1$）

3. **“等分引理”（Halving lemma）**

   当 $n$ 为偶数时，集合

   $$
   {\omega_n^{2k}\mid k=0,\dots,n/2-1}
   $$

   正好是所有 $(n/2)$ 次单位根。

4. **“求和引理”（Summation lemma）**

   若 $k$ 不是 $n$ 的倍数，则

   $$
   \sum_{j=0}^{n-1} \omega_n^{kj} = 0
   $$

   这个性质会用在证明逆 DFT 的公式正确。

---

### 2. DFT：在单位根处的多项式求值

现在选取特殊的 $n$ 个点：

$$
x_k = \omega_n^k,\quad k=0,\dots,n-1
$$

对多项式

$$
A(x)=\sum_{j=0}^{n-1} a_j x^j
$$

定义 **离散傅里叶变换 DFT** 为序列 $y_0,\dots,y_{n-1}$：

$$
y_k = A(\omega_n^k) = \sum_{j=0}^{n-1} a_j \omega_n^{kj}
$$

记作

$$
\mathbf{y}= \operatorname{DFT}_n(\mathbf{a})
$$

这样，**DFT 就是把“系数表示”变成“在单位根处的点值表示”**。

如果直接按照定义算，每个 $y_k$ 都需要 $n$ 次乘加，总共 $n^2$，时间 $\Theta(n^2)$——太慢，这就是要用 FFT 加速的对象。

> **典型应用 4：离散信号转频域**

* 把每个采样点 $s(t_i)$ 看成 $a_i$，形成序列

  $$
  \mathbf{a}=(a_0,\dots,a_{n-1})
  $$

* 做 DFT 得到

  $$
  \mathbf{y}=\operatorname{DFT}_n(\mathbf{a})
  $$

  这就是频谱 $S(\omega)$ 的离散版本，告诉你信号里分别含有多少“不同频率”的成分。

---

### 3. FFT：用分治快速计算 DFT

FFT 使用了一个关键的分解式：

把多项式 $A(x)$ 的偶数项和奇数项分开：

$$
A^{[0]}(x)=a_0+a_2 x+a_4 x^2+\dots+a_{n-2} x^{n/2-1}
$$

$$
A^{[1]}(x)=a_1+a_3 x+a_5 x^2+\dots+a_{n-1} x^{n/2-1}
$$

那么

$$
A(x)=A^{[0]}(x^2)+x,A^{[1]}(x^2)
$$

这一步只是重排。

接着要在所有 $x_k=\omega_n^k$ 上求值。注意：

$$
x_k^2=(\omega_n^k)^2=\omega_n^{2k}=\omega_{n/2}^k
$$

利用“等分引理”，$x_k^2$ 其实就是 $n/2$ 次单位根。于是：

1. 先对长度为 $n/2$ 的序列“偶数系数”做 DFT，得到

   $$
   y_k^{[0]}=A^{[0]}(\omega_{n/2}^k)
   $$

2. 再对“奇数系数”做 DFT，得到

   $$
   y_k^{[1]}=A^{[1]}(\omega_{n/2}^k)
   $$

3. 再利用

   $$
   A(\omega_n^k)=A^{[0]}(\omega_{n/2}^k)+\omega_n^k,A^{[1]}(\omega_{n/2}^k)
   $$

   $$
   A(\omega_n^{k+n/2})=
   A^{[0]}(\omega_{n/2}^k)-\omega_n^k,A^{[1]}(\omega_{n/2}^k)
   $$

   把两个长度为 $n/2$ 的 DFT 合成一个长度为 $n$ 的 DFT。

这就是 FFT 的核心递推关系。伪代码结构大致是：

```text
RECURSIVE-FFT(a):
    n = len(a)   // n 是 2 的幂
    if n == 1: return a
    a_even = (a[0], a[2], a[4], ...)
    a_odd  = (a[1], a[3], a[5], ...)
    y_even = RECURSIVE-FFT(a_even)
    y_odd  = RECURSIVE-FFT(a_odd)
    ω = e^{2πi/n}
    ω_k = 1
    for k = 0..n/2-1:
        t = ω_k * y_odd[k]
        y[k]       = y_even[k] + t
        y[k+n/2]   = y_even[k] - t
        ω_k = ω_k * ω
    return y
```

* 每一次递归，把问题大小减半（$n\to n/2$），做两次 FFT；
* 合并阶段只需一趟循环，每层 $O(n)$；
* 总复杂度是 $\Theta(n\log n)$。

其中 $\omega_k$ 在合并时既被加上又被减去，这个乘数 $\omega_k$ 常称为 **twiddle factor（旋转因子）**。

> **典型应用 5：用 FFT 计算 DFT（示意版）**

比如 $n=8$，输入 $(a_0,\dots,a_7)$，递归分解如下：

1. 拆成偶数位置 $(a_0,a_2,a_4,a_6)$ 和奇数位置 $(a_1,a_3,a_5,a_7)$；
2. 再对子序列长度 $4$ 继续拆成长度 $2$，直到长度 $1$；
3. 自底向上，使用上述合并公式求出 $y_0,\dots,y_7$。

---

### 4. 逆 DFT（inverse DFT）与卷积定理

有了 DFT，自然要能“反过来”：给出 $y_k$ 还原 $a_j$。

**逆 DFT 的公式**是：

$$
a_j = \frac{1}{n} \sum_{k=0}^{n-1} y_k \omega_n^{-kj},\quad j=0,\dots,n-1
$$

也可以写成矩阵形式，说“DFT 的矩阵 $V_n$ 的逆矩阵是 $V_n^{-1}$，其元素为 $\omega_n^{-kj}/n$”。从算法角度理解就够了：

> **如何用 FFT 实现逆 DFT？**

* 把所有的 $\omega_n$ 换成 $\omega_n^{-1}$ 做一次 FFT；
* 然后把结果每一项都除以 $n$；
* 时间也是 $\Theta(n\log n)$。

---

**卷积定理**（Convolution theorem）告诉你：

设 $,\mathbf{a},\mathbf{b},$ 为长度 $n$ 的序列，把它们补零到长度 $2n$，则

$$
\mathbf{a}\odot\mathbf{b}
=\operatorname{DFT}*{2n}^{-1}
\Big(
\operatorname{DFT}*{2n}(\mathbf{a})
\cdot
\operatorname{DFT}_{2n}(\mathbf{b})
\Big)
$$

这里 “$\cdot$” 是分量乘法，“$\odot$” 是卷积。

这就是“用 FFT 做卷积 / 多项式乘法”的理论基础，对应刚才的典型应用 3。

---

## 四、30.3 高效 FFT 实现（蝶形、迭代、并行）

这一节主要是**实现细节优化**，思想你掌握一下即可。

### 1. 蝶形运算（butterfly）

从递归 FFT 的合并步骤可以抽象出一个基本操作：

给定一对输入 $(u,v)$ 和一个“旋转因子” $\omega$，输出一对：

$$
(u+ \omega v,\ u-\omega v)
$$

把这个图形画出来就是一个 “X” 形状，所以叫 **蝶形运算**。

> **典型应用 6：2 点 DFT 的蝶形**

当 $n=2$ 时，DFT 就是：

$$
\begin{pmatrix}y_0 \ y_1\end{pmatrix} \\
= \\
 
\begin{pmatrix}
1 & 1 \\
1 & -1 
\end{pmatrix}
\begin{pmatrix}a_0\ a_1\end{pmatrix}
$$

这恰好是一次蝶形：$\omega=-1$，输出 $(a_0+a_1, a_0-a_1)$。

整个 FFT 可以看成是很多层、很多个蝶形堆叠起来。

---

### 2. 迭代 FFT 与 Bit-reversal

递归版容易理解，但在实际实现中会转成**迭代版** FFT：

1. 先对输入序列进行一次 **bit-reversal 重排**：

   * 把下标用二进制写出来，
   * 把二进制位反过来，
   * 以这个新下标重新排列数据。

   例如 $n=8$：

   * 下标 $3$ 是二进制 $011$，翻转得到 $110$，即 $6$；
   * 所以原来 $a_3$ 的位置上的数会被放到新数组的下标 $6$ 上。

2. 然后从“块长 $2$ 的蝶形”开始，每一层把块长翻倍，直到块长 $n$；

3. 在每一层，对每个块做对应的蝶形运算（用不同的旋转因子 $\omega$）。

迭代式的伪代码大致是：

```text
ITERATIVE-FFT(a):
    BIT-REVERSE-COPY(a, A)   // 重排
    for s = 1 .. log2 n:
        m = 2^s
        ω_m = e^{2πi/m}
        for k = 0 .. n-1 step m:
            ω = 1
            for j = 0 .. m/2 - 1:
                t = ω * A[k+j+m/2]
                u = A[k+j]
                A[k+j]       = u + t
                A[k+j+m/2]   = u - t
                ω = ω * ω_m
    return A
```

时间复杂度仍然是 $\Theta(n\log n)$。

> **典型应用 7：手算一个小例子**

比如 $n=4$，输入 $(a_0,a_1,a_2,a_3)$：

1. 对下标 $0,1,2,3$ 做 bit-reversal（2 位二进制）：

   * $00\to 00$（0）
   * $01\to 10$（2）
   * $10\to 01$（1）
   * $11\to 11$（3）
     排成 $(a_0,a_2,a_1,a_3)$；
2. 第一层（块长 2）：对 $(a_0,a_2)$、$(a_1,a_3)$ 做 2 点蝶形；
3. 第二层（块长 4）：再对上面结果做 4 点蝶形，得到最终 DFT 值。

---

### 3. 并行 FFT 与其它

PPT 后面还画了 **并行 FFT 电路图**：所有蝶形运算在每一层可以并行执行，这样在硬件或多核环境下可以非常快。实际的并行算法就不展开了，思想就是“同一层的蝶形互不依赖，可以一起算”。

---

## 五、典型应用总结

最后 PPT 简单列了 FFT 的几个重要应用：

1. **多项式运算**：快速乘法、快速求卷积。
2. **大整数乘法**：把大整数写成“某个基数 $B$ 的展开”，系数就是数字，做一次多项式乘法就能得到结果（再处理进位）。
3. **信号处理**：音频、图像、视频等的频谱分析、滤波、压缩。
4. 其它数值计算相关的场景（比如解一些差分方程等）。

---

如果你愿意，我可以再针对某一块（例如“多项式乘法 + FFT 的具体算例”或者“DFT/FFT 的手算例子”）单独写一份**步骤特别详细的中文推导与演算过程**，方便你照着练习。



# a12

下面我按 PPT 的顺序，用中文把这一章（Chapter 32：String Matching）讲清楚：讲了哪些**字符串匹配算法**、它们解决什么**典型问题/案例**、以及各自的**核心做法**（复杂度只给结论）。

------

## 0. 这一章要解决的问题：什么叫“匹配/位移 shift”

- 给定文本串 $T[1..n]$ 和模式串 $P[1..m]$（$m\le n$），字母表是有限集合 $\Sigma$，且 $P_i,T_i\in\Sigma$。
- 说“$P$ 在 $T$ 中以位移 $s$ 出现”，意思是：$0\le s\le n-m$ 且
  $$T[s+1..s+m] = P[1..m]\quad(\text{等价于 }T[s+j]=P[j],,1\le j\le m).$$
- 我们要找的是**所有有效位移 valid shifts**：所有满足上式的 $s$。

PPT 还铺垫了符号：

- $\Sigma^*$：由 $\Sigma$ 里的字符组成的所有有限串；$\varepsilon$ 是空串。

- 前缀/后缀：$\omega \sqsubset x$ 表示前缀，$\omega \sqsupset x$ 表示后缀。

- 一个关键小引理（Overlapping-suffix lemma）：如果 $x\sqsupset z$ 且 $y\sqsupset z$，那么较短的后缀会“包含在”较长后缀里（长度相等则相等）。

  > 直观上：两个串都以同一个 $z$ 结尾，它们末尾对齐后，短的那段必然也是长的后缀。

------

## 1) 32.1 朴素匹配（Naive string-matching）

### 算法做什么

它把模式串当“模板”，从左到右枚举每个可能位移 $s=0..n-m$，每次检查：
$$P[1..m] \stackrel{?}{=} T[s+1..s+m].$$

伪代码（PPT给的）核心就是两层含义：

- 外层：遍历所有 $s$；
- 内层：逐字符比对（PPT说第4行“隐式循环”）。

### 复杂度结论（只给结论）

- 最坏匹配时间：$\Theta((n-m+1)m)$（每个 $s$ 最多比对 $m$ 个字符）。这一点和 Rabin–Karp 的最坏时间写法一致（PPT在 RK 那页直接给了这个量级）。

### 典型案例 & 解法（PPT里的思考题）

1. **找“所有匹配”但要求子串不重叠**
   PPT例子：$T=\text{aaaaa},,P=\text{aa}$，希望匹配偏移是 $0$ 和 $2$（而不是 $0,1,2,3$）。
   **做法**：当你在位移 $s$ 找到一次完整匹配后，不让下一次从 $s+1$ 开始，而是直接跳到 $s+m$（跳过刚匹配的那段），继续找。
2. **实现 `strrstr`（最后一次匹配的位置）**
   **两种常见做法：**

- 从左到右照常扫描，每次匹配就更新 `last_pos=s`，最终返回最后记录。
- 或者从右到左枚举 $s=n-m,n-m-1,\dots,0$，第一次匹配就返回。

> PPT 同时提到 C 里的 `strstr(text, pattern)` 作为文本编辑/查找的典型接口场景。

------

## 2) 32.2 Rabin–Karp（RK）算法：用“指纹/哈希”加速筛选

### 核心思想

把长度为 $m$ 的串看成一个 $d$ 进制数（或编码后的数），对模式串算一个值 $p(P)$，对每个对齐窗口 $T[s+1..s+m]$ 算 $p(T_{s+m})$：

- 若 $p(P)=p(T_{s+m})$ 则认为匹配；或更常用：
- 先比模：若 $(p(P)\bmod q)=(p(T_{s+m})\bmod q)$，再做一次“真匹配”检查 $P==T[s+1..s+m]$ 来排除冲突。

PPT举例：字母 ${a,b,c,d}\mapsto{0,1,2,3}$，算出 $p(P)=99$（展示了按位权展开）。

### 复杂度结论（PPT给的）

- 预处理时间：$\Theta(m)$。
- 最坏运行时间：$\Theta((n-m+1)m)$（哈希冲突多时，退化到频繁“真比对”）。

### 典型案例（PPT提到）

- RK 在实践中表现好，并可推广到**二维模式匹配**等相关问题。

  > 直观上：二维里也可以先用哈希快速过滤候选位置，再做精确验证。

------

## 3) 中场对比：Naive vs RK vs FA vs KMP（PPT的对比页）

PPT用一张对比图强调“遇到不匹配时怎么快速移动 $P$”：

- Naive：右移 +1，再从头比。
- RK：靠 $p(P)$（指纹）过滤候选。
- FA（有限自动机）：预处理出 $\delta$ 转移表，不匹配时直接按表跳。
- KMP：预处理出前缀函数 $\pi$，不匹配时按 $\pi$ 跳。

（同一页还有一张“预处理/匹配时间”的表格，后面我会分别在 FA/KMP 小节给出 PPT明确写出来的复杂度结论。）

------

## 4) 32.3 有限自动机（Finite Automaton, FA）做字符串匹配

### 4.1 FA 基本概念（PPT先讲自动机是什么）

一个有限自动机 $M$ 是五元组：
$$M=(Q,q_0,A,\Sigma,\delta),$$
读入字符时按 $\delta(q,a)$ 从状态 $q$ 跳到新状态。

PPT还定义了“终态函数” $\Phi$（final-state function）：

- $\Phi(\varepsilon)=q_0$
- $\Phi(\omega a)=\delta(\Phi(\omega),a)$

### 4.2 用 FA 做匹配：状态代表“当前匹配了多少”

PPT的核心解释是：扫描文本 $T$ 的前缀 $T_i$ 时，自动机状态就是“$P$ 的前缀在 $T_i$ 末尾能匹配到多长”。

为此引入**后缀函数**（suffix function）$\sigma$：

> $\sigma(x)$ = “$x$ 的后缀中，最长的那段，恰好也是 $P$ 的前缀”的长度
> 形式化：
> $$\sigma(x)=\max{k:;P_k \sqsupset x}.$$

### 4.3 最关键的定义：如何定义转移函数 $\delta$

PPT给出的定义是：
$$\delta(q,a)=\sigma(P_q a).$$

并且 PPT用一个“不变式”解释为什么这定义合理：
自动机在扫完 $T_i$ 时的状态 $\Phi(T_i)$，保持等于 $\sigma(T_i)$；下一字符是 $a$ 时就满足
$$\Phi(T_{i+1})=\delta(\Phi(T_i),a)=\sigma(P_q a)=\sigma(T_i a)=\sigma(T_{i+1}).$$

### 4.4 匹配算法（扫描时只查表）

PPT的 `FINITE-AUTOMATON-MATCHER` 就是：

- 初始化 $q=0$
- 逐个字符读 $T[i]$，令 $q\leftarrow\delta(q,T[i])$
- 若 $q==m$，说明刚读完一个匹配，输出位移 $i-m$。

**复杂度结论（PPT明确写的）**：匹配时间是 $\Theta(n)$。

PPT也强调了代价：预处理（构造 $\delta$）在字母表 $\Sigma$ 很大时会很大（例如中文字符集）。

### 4.5 典型案例：模式 $P=\text{ababaca}$ 的自动机构造/转移

PPT用图解释：比如 $\delta(5,b)=4$，原因是读到状态 $5$ 再输入 $b$，相当于看 $P_5b=\text{ababab}$，它的最长“既是 $P$ 前缀又是后缀”的长度是 $4$，所以跳到 4。

同一个例子里，自动机在文本 $T=\text{abababacaba}$ 上运行，能找到一次匹配（PPT说匹配在位置 9 结束）。

### 4.6 如何计算 $\delta$（PPT给了构造伪代码）

PPT的 `COMPUTE-TRANSITION-FUNCTION(P, Σ)` 思想是：对每个状态 $q$ 和每个字符 $a\in\Sigma$，找最大的 $k$ 使得
$$P_k \sqsupset (P_q a).$$
伪代码体现为从 $k=\min(m,q+1)$ 往下试，直到满足为止，然后置 $\delta(q,a)=k$。

------

## 5) 32.4 KMP（Knuth–Morris–Pratt）：不用整张 $\delta$ 表，也能线性匹配

PPT在进入 KMP 前，先用一页对比解释了“FA vs KMP”：

- FA：下一个字符 $a$ 来了，直接查 $\delta(q,a)$。
- KMP：只预处理一个函数 $\pi$；当发生不匹配时，反复令 $q\leftarrow \pi[q]$，再检查是否能继续匹配（相当于把 $P$ 移到“下一个最可能的位置”）。

### 5.1 前缀函数 $\pi$ 的定义（PPT给的公式）

对模式串 $P$，定义前缀函数：
$$\pi:{1,2,\dots,m}\to{0,1,\dots,m-1},\qquad
\pi[q]=\max{k:;k<q\ \text{且}\ P_k\sqsupset P_q}.$$

这句话翻成中文就是：

> 在 $P[1..q]$ 的所有“真前缀”里，找一个最长的，它同时也是 $P[1..q]$ 的后缀；长度就是 $\pi[q]$。

### 5.2 典型“失配”场景（PPT举的）

PPT举例说：已经匹配到 $q=5$（即 $P_5 \sqsupset T_{s+5}$），但下一字符不等：$T[s+6]\ne P[6]$。
这时 KMP 不会像 naive 那样把 $P$ 只右移 1 再重头比，而是用 $\pi$ 直接跳到一个更合理的 $k$，继续尝试。

### 5.3 复杂度结论（PPT明确写的）

- KMP 匹配时间：$\Theta(n)$。
- 计算并存储 $\pi[1..m]$ 的预处理时间：$\Theta(m)$。

PPT还给了一个示例：对 $P=\text{ababaca}$，按 $q=1,2,3,\dots$ 逐步算出 $\pi[q]$（展示了如何从 $\pi[q-1]$ 推到 $\pi[q]$）。

------

## 6) PPT开头给的“应用/典型场景”（你理解算法时可对号入座）

这一章开头直接说了两类典型应用：

1. **文本编辑/查找**：在 text-editing programs 里找 pattern（例如 `strstr`），高效算法能提升编辑器响应性。
2. **生物信息**：在 DNA 序列里搜索特定模式。

把这些需求和四种算法对上：

- 想实现最简单可靠：Naive（很多库函数的朴素版本/基线）
- 想“先快速筛掉大部分不可能位置”：Rabin–Karp（哈希指纹过滤，必要时再精确比对）
- 想做到“扫描 $T$ 时每字符常数处理”：FA / KMP 都给出 $\Theta(n)$ 的匹配（区别在于 FA 用整张 $\delta$，KMP 用较小的 $\pi$ 来绕开大字母表预处理）

------

如果你愿意，我也可以继续按 PPT 的图（尤其是 **Figure 32.7 的自动机图** 和 **KMP 的失配回退流程图**）把“状态怎么跳、为什么跳到那个数”用同一个例子 $P=\text{ababaca}$、$T=\text{abababacaba}$ 逐步手算一遍（不讲复杂度推导，只讲跳转逻辑）。



# a13

下面按这章 PPT（第 17 章：Amortized Analysis / 平摊分析）的逻辑，用中文把整章内容“从头到尾讲一遍”。你看完应该就能掌握：平摊分析是什么、为什么要用、三种经典方法怎么做，以及三个核心例子（栈、二进制计数器、动态表扩容）和两个应用（凸包 Graham-Scan、KMP）。

------

## 1. 这章想解决什么问题：单次操作很贵，但“平均下来”很便宜

很多数据结构/算法会出现这种现象：

- 大多数操作很快（比如 $O(1)$），
- 但偶尔某一次操作特别贵（比如扩容要复制一堆元素，可能 $O(n)$）。

**平摊分析（Amortized Analysis）\**关心的是：
对一串操作序列，把“总耗时”均摊到每次操作上，证明\**每次操作的平均成本**很小——而且这个结论是**最坏情况下也成立**的（不靠概率）。

> 它和“平均情况分析（average-case）”不同：平均情况通常要假设输入分布/概率；平摊分析不需要概率，结论对任何操作序列都保证成立。

------

## 2. 本章三大方法总览

PPT 把平摊分析分成 3 种经典套路：

1. **聚集分析（Aggregate analysis）**
2. **记账法（Accounting method）**
3. **势能法（Potential method）**

它们最终都在证明同一件事：

> 长序列总成本 $\Rightarrow$ 每次操作的平摊成本很小。

------

## 3. 17.1 聚集分析：先算总成本，再除以 $n$

### 3.1 核心定义

如果能证明：任意 $n$ 次操作序列的总最坏时间是 $T(n)$，
那么每次操作的平摊成本就是：

$$\text{amortized cost} = \frac{T(n)}{n}$$



------

### 3.2 例子 1：栈的 PUSH / POP / MULTIPOP

#### 操作定义

- **PUSH(S,x)**：入栈，$O(1)$
- **POP(S)**：出栈，$O(1)$（空栈会报错）
- **MULTIPOP(S,k)**：连续弹出 $k$ 个；若栈里只有 $s$ 个，就弹出全部 $s$ 个
  所以单次最坏时间：$O(\min(s,k))$，可能一次弹很多。

#### 为什么总成本是 $O(n)$？

考虑从空栈开始的一串长度为 $n$ 的操作序列（每一步是 PUSH/POP/MULTIPOP 之一）。
关键观察：

- **每个元素被 PUSH 进栈一次之后，最多只能被 POP 掉一次**（不管是普通 POP 还是 MULTIPOP 内部的 POP）。
- 因此，“实际发生的 POP 次数” $\le$ “发生的 PUSH 次数” $\le n$。

所以：整个序列中，所有出栈动作加起来最多 $n$ 次，总时间 $O(n)$。
于是平摊到每次操作：

$$\frac{O(n)}{n}=O(1)$$

这说明：虽然某次 MULTIPOP 可能是 $O(n)$，但平均下来每步依然是 $O(1)$。

------

### 3.3 例子 2：$k$ 位二进制计数器 INCREMENT

我们用位数组 $A[0..k-1]$ 表示计数器，最低位在 $A[0]$。计数器表示的数：

$$x=\sum_{i=0}^{k-1} A[i]\cdot 2^i$$

每次 **INCREMENT(A)**（加 1）会做：从低位开始把连续的 1 翻成 0，直到遇到第一个 0，把它翻成 1。

#### 单次操作最坏很贵

如果当前是 $111\cdots 111$，那要翻转很多位，单次最坏 $O(k)$。
如果做 $n$ 次增量，粗略上界是 $O(nk)$，但不紧。

#### 聚集分析：统计“翻转次数”

观察每一位翻转的频率：

- $A[0]$ 每次都翻：翻转 $n$ 次
- $A[1]$ 每两次翻一次：约 $n/2$ 次
- $A[2]$ 每四次翻一次：约 $n/4$ 次
- 一般地，第 $i$ 位翻转次数 $\approx n/2^i$

总翻转次数：

$$\sum_{i=0}^{k-1}\left\lfloor \frac{n}{2^i}\right\rfloor < \sum_{i=0}^{\infty}\frac{n}{2^i}=2n$$

所以 $n$ 次 INCREMENT 的总成本 $O(n)$，平摊每次就是 $O(1)$。

------

### 3.4 聚集分析的应用：凸包 Graham-Scan 的 while 循环为何总共 $O(n)$

Graham-Scan 的核心结构是“用栈维护凸包边界”，循环里会：

- 每个点 $p_i$ **PUSH 一次**
- 在 while 中可能反复 **POP**，直到转向正确（左转）

聚集分析的关键句：

- 每个点最多被 POP 一次（因为一旦弹出就不再回来），
- 因此 while 循环里的 POP 总次数 $\le$ PUSH 总次数 $\le n$，所以 while 总开销 $O(n)$。

整体复杂度：排序 $O(n\log n)$ + 扫描 $O(n)$。

------

## 4. 17.2 记账法：给操作“收费”，多收的存成信用

记账法的思想很像“预存余额”：

- 第 $i$ 次操作实际成本：$c_i$
- 我们人为规定它的平摊收费：$\hat{c}_i$
- 如果 $\hat{c}_i > c_i$，多出来的部分作为**信用（credit）**存起来；
- 以后遇到某次操作 $c_i$ 很大但 $\hat{c}_i$ 不大，就用之前存的信用支付差额。

要求是：任意时刻信用不能为负，这样就能保证：

$$\sum_{i=1}^n c_i \le \sum_{i=1}^n \hat{c}_i$$

也就是：总实际成本被总平摊收费上界住。

------

### 4.1 栈操作的记账法（一个经典分配）

一种常见的收费方式（PPT 给的思路）：

- PUSH：收 2
  - 1 用来付本次 PUSH 的实际成本
  - 1 作为信用贴在“被压入的元素”身上，未来用于支付它被 POP 的成本
- POP：收 0（因为弹出时用元素身上的信用支付）
- MULTIPOP：收 0（每弹一个元素就用它的信用支付）

于是 $n$ 次操作总收费 $O(n)$，所以总实际成本也是 $O(n)$。

------

### 4.2 二进制计数器的记账法：每次把 0 置 1 收 2

PPT 的策略是：

- 每当某个位从 0 变 1：收 2
  - 1 用于支付这次翻转
  - 1 留在该位上作为信用（因为未来它从 1 变 0 时要花 1）
- 每当某个位从 1 变 0：收 0（直接用该位上存的 1 信用支付）

这样每次 INCREMENT 的平摊收费最多 2，因此 $n$ 次总收费 $O(n)$，总实际成本也 $O(n)$。

------

### 4.3 记账法的应用：Graham-Scan 与 KMP

**Graham-Scan：**

- 每个点 PUSH 一次，给 PUSH 收 2：1 付 PUSH，1 存为该点的信用
- POP 收 0：弹点时用点上的信用支付 POP
  因此 POP 再多也付得起，总体仍然线性扫描。

**KMP：**
PPT 用“$q$（或 $k$）增加/减少”的视角讲：

- 当 $q$ 增加（匹配推进）时，收 2（1 付实际成本，1 存信用）
- 当 $q$ 减少（失配回退 $q=\pi[q]$）时，收 0，用之前累积的信用支付
  直观上：$q$ 不可能无限次减少，因为它最多减少到 0，而之前每次增加都攒了信用，所以总回退次数被总增加次数上界住，从而整体是线性。

------

## 5. 17.3 势能法：把“预付的工作”存在数据结构的势能里

势能法可以看作“把信用集中记在整个数据结构上”，不是分散记在每个元素上。

### 5.1 定义

- 第 $i$ 次操作实际成本：$c_i$
- 操作前数据结构：$D_{i-1}$，操作后：$D_i$
- 势能函数：$\Phi(D)$，给每个状态一个非负“势能值”

定义第 $i$ 次操作的平摊成本：

$$\hat{c}*i = c_i + \Phi(D_i)-\Phi(D*{i-1})$$

总平摊成本：

$$\sum_{i=1}^n \hat{c}*i
= \sum*{i=1}^n c_i + \Phi(D_n)-\Phi(D_0)$$

通常令 $\Phi(D_0)=0$ 且保证 $\Phi(D_i)\ge 0$，就得到：

$$\sum_{i=1}^n c_i \le \sum_{i=1}^n \hat{c}_i$$



------

### 5.2 势能法分析栈：令势能等于栈中元素个数

定义：

$$\Phi(D_i)=\text{stack size}$$

- **PUSH**：实际 $c_i=1$，栈大小 +1，所以 $\Delta\Phi=1$
  $$\hat{c}_i = 1+1=2$$
- **POP**：实际 $c_i=1$，栈大小 -1，所以 $\Delta\Phi=-1$
  $$\hat{c}_i = 1-1=0$$
- **MULTIPOP**：弹出 $k'=\min(k,s)$ 个
  实际 $c_i=k'$，势能减少 $k'$：$\Delta\Phi=-k'$
  $$\hat{c}_i = k' - k' = 0$$

因此每步平摊成本常数（PUSH 2，其余 0），总计 $O(n)$。

------

### 5.3 势能法分析二进制计数器：令势能等于 1 的个数

定义第 $i$ 次操作后，计数器中 1 的数量为 $b_i$，势能：

$$\Phi(D_i)=b_i$$

假设第 $i$ 次 INCREMENT 把 $t_i$ 个 1 翻成 0，并把最多 1 个 0 翻成 1。
所以实际成本：

$$c_i \le t_i + 1$$

势能变化（1 的个数减少 $t_i$，再增加 1）满足：

$$b_i \le b_{i-1}-t_i+1 \quad\Rightarrow\quad \Phi(D_i)-\Phi(D_{i-1}) \le 1-t_i$$

于是平摊成本：

$$\hat{c}*i
= c_i + \Phi(D_i)-\Phi(D*{i-1})
\le (t_i+1) + (1-t_i) = 2$$

所以每次 INCREMENT 平摊成本 $\le 2$。

------

## 6. 17.4 动态表（Dynamic tables）：扩容插入为什么平摊 $O(1)$

这部分是本章最经典的“偶尔扩容很贵”的例子：
表底层是数组；插入时如果满了就**扩容**（常用策略：容量翻倍），并把旧元素全部拷贝到新表。

### 6.1 负载因子（load factor）

对非空表 $T$，负载因子：

$$\alpha(T)=\frac{T.num}{T.size}$$

其中：

- $T.num$：当前元素个数
- $T.size$：当前容量（槽位数）

翻倍扩容且只有插入时，PPT 指出负载因子至少 $\ge 1/2$，浪费空间不会超过一半。

------

### 6.2 扩容插入的实际代价长什么样

插入第 $i$ 个元素时：

- 若不触发扩容：只要放进去，$c_i=1$
- 若触发扩容：要复制已有元素再插入，新成本大约是 $i$ 量级（因为要搬运很多元素）

所以单次最坏 $O(n)$，粗略总上界可到 $O(n^2)$，但这不紧。

------

### 6.3 三种方法都能证明：**每次插入平摊成本是常数**

#### (1) 聚集分析结论（直觉）

扩容发生在容量为 $1,2,4,8,\dots$ 这些时刻。
当从 $2^k$ 扩到 $2^{k+1}$ 时，会复制 $2^k$ 个元素。
总复制量是几何级数：

$$1+2+4+\cdots+2^{\lfloor \log n\rfloor} < 2n$$

再加上每次插入本身的 1，总体 $O(n)$，所以平摊 $O(1)$。

#### (2) 记账法：每次插入收 3

PPT 给的策略：**每次插入收 3 元**

- 1 元支付本次插入
- 2 元存为信用
  这些信用累积起来，足够未来某次扩容时支付“搬运旧元素”的成本
  结论：每次插入平摊成本为 3。

#### (3) 势能法：选一个合适的势能函数

PPT 选的势能函数是：

$$\Phi(T)=2T.num - T.size$$

并给出两个关键状态：

- **刚扩容后**：$T.num=T.size/2 \Rightarrow \Phi(T)=0$
- **扩容前一刻**：$T.num=T.size \Rightarrow \Phi(T)=T.num$

然后分别讨论：

- 不扩容的插入：平摊成本算出来是 3
- 触发扩容的插入：平摊成本也被压到常数（最终也是 3 的量级）

因此结论：**TABLE-INSERT 的平摊成本是常数（PPT 给出为 3）**。

------

### 6.4 扩容+收缩（contraction）

PPT 写“自学”，意思是：删除多了可能需要缩容（例如容量减半），也可以用同样三种方法分析，目标仍是证明插入/删除的平摊都是 $O(1)$。

------

## 7. 本章最后的总结与作业含义

PPT 最后用表格式总结：
三种方法（聚集 / 记账 / 势能）分别在这些例子里怎么用：

- 栈操作
- 二进制计数器
- 动态表扩容/收缩
  以及应用：凸包、KMP 等。

作业方向（PPT 给的练习）大意是：

1. 做完课后所有习题
2. 再找几个新例子（不能再用凸包/KMP）来展示如何做平摊分析
3. 把书里所有用平摊分析的算法找出来，标注用的是哪种方法、结论是什么。

------

如果你愿意，我也可以把这一章最核心的“解题模板”提炼成一页中文速记（比如：什么时候选聚集/记账/势能、势能函数怎么猜、如何写出严谨的不等式）。

# a14

下面是这章《Chapter 34：NP Complete Problems（NP 完全问题）》的**中文详细讲解版**，按 PPT 的逻辑从头到尾串起来；你看完基本就等于把这一章全学了一遍。

------

## 1. 这章想解决什么核心问题？

这章用一堆例子说明：

- **有些问题“算出来很难”，但“验证对不对很容易”**。
- 计算机科学把这类问题系统化后，形成了 **P、NP、NP-Complete（NP 完全）、NP-hard（NP 困难）** 等概念，并提出最大悬案：**$P\stackrel{?}{=}NP$**。

------

## 2. 从“大数分解”说起：算很难，验很快

PPT 先举了一个经典例子：分解大数。比如把一个很大的数写成两个整数相乘：

- 问题形式：给你 $M$，问能否找到 $a,b$ 使得
  $$M=a\times b$$

PPT 用了著名的例子（Cole 在 1903 年分解 $2^{67}-1$）：
$$2^{67}-1 = 761,838,257,287 \times 193,707,721$$
关键点是：**找到这个分解很费劲（150 天），但一旦有人给出 $a,b$，你用乘法一算就能立刻验证对不对。**

### 为什么这重要？

因为现代密码（尤其 RSA）依赖一个直觉：

- **大数相乘容易**（$n=pq$ 很快）
- **把 $n$ 再分解回 $p,q$ 很难**（目前没有已知高效通用算法）
  所以能保证通信安全。

------

## 3. 复杂性：算法复杂 vs 问题复杂（别混淆）

PPT 强调两件事：

### 3.1 算法的复杂性（Algorithm complexity）

指**某一个具体算法**运行要多久，比如排序：

- 冒泡排序：$O(n^2)$
- 快速排序：$O(n\log n)$

### 3.2 问题的复杂性（Problem complexity）

指**解决这个问题的所有算法里，最好的那个算法**的复杂度。
比如“排序问题”本身（在比较模型下）最优可做到 $O(n\log n)$。

------

## 4. 为什么只研究“判定问题（Decision Problem）”？

因为优化问题不好直接讨论“最好算法是什么”，研究起来太麻烦。于是大家把问题统一成**回答只有 Yes/No 的判定问题**。

### 4.1 判定问题定义

**判定问题**：答案只有“是/否”（Yes/No）。

例子：

- 数组里是否有重复元素？
- 是否存在长度至少为 $k$ 的公共子序列？

### 4.2 优化问题怎么变判定问题？

例如最短路：

- 优化版：从 $u$ 到 $v$ 的最短路是多少？
- 判定版：是否存在一条路径使得 $\text{Path}(u,v) < k$？

你不断试不同的 $k$，就能间接定位最短值。

------

## 5. P 类：能在多项式时间内“直接解出来”的判定问题

### 定义（PPT 原意）

P 是能用多项式时间解决的判定问题集合：
如果存在算法时间复杂度为
$$O(n^k)\quad (k\text{ 为常数})$$
就认为在 P 里，直观上属于“容易”。

------

## 6. NP 类：不一定容易“找到解”，但容易“验证解”

### 6.1 NP 的核心定义

NP 是这样一类判定问题：

- 给你一个候选答案（证据/证明/解），你能在多项式时间内验证它是否正确。

所以 NP 强调的是 **Verify（验证）快**，不是 Solve（求解）快。

### 6.2 NP 不是 “Non-Polynomial”

PPT 特别提醒：NP 里的 N 指 **Non-Deterministic（非确定性）**，不是“非多项式”。

它的直观意思是：

- 求解时你可能需要“猜”（非确定性地选一个候选解）
- 但猜完之后，验证很快

### 6.3 非确定性例子

- **大数分解**：$M=?\times ?$（你不知道怎么一步步算出因子，只能“猜因子再验证”）
- **哈密顿回路**：图 $G$ 是否存在一条回路，恰好经过每个顶点一次？（给你一条回路你能快速验，但从零找出来很难）

------

## 7. P vs NP：验证通常比求解更容易

PPT 用一句话总结：

- P：能很快求解
- NP：能很快验证
- 通常“验证解”比“找到解”更容易

并再次用 $2^{67}-1$ 分解说明：**找因子慢，验乘积快。**

------

## 8. RSA：NP 思想的经典应用（公钥密码）

这一部分 PPT 用 RSA 解释：为什么“验证快但求解难”能做安全。

### 8.1 RSA 密钥生成步骤（按 PPT）

1. 随机选两个大素数 $p,q$（$p\neq q$）
2. 计算
   $$n=pq$$
3. 计算欧拉函数
   $$\varphi(n)=(p-1)(q-1)$$
4. 选一个小奇数 $e$，满足
   $$\gcd(e,\varphi(n))=1$$
5. 求 $d$ 为 $e$ 在模 $\varphi(n)$ 下的乘法逆元：
   $$ed\equiv 1\pmod{\varphi(n)}$$
6. 公钥发布：$P=(e,n)$
7. 私钥保密：$S=(d,n)$

### 8.2 加密与解密（PPT 的符号）

- 加密：
  $$C=P(M)=M^e \bmod n$$
- 解密：
  $$M=S(C)=C^d \bmod n$$

### 8.3 为什么一定能解回去（正确性）

PPT 给出推导核心是：
$S(P(M))=(M^e \bmod n)^d \bmod n = M^{ed}\bmod n$
又因为
$ed = 1+k\varphi(n)$
所以
$M^{ed}=M^{1+k\varphi(n)} = M\cdot (M^{\varphi(n)})^k$
利用数论定理（与费马小定理/欧拉定理相关），最终可得到：
$S(P(M))\equiv M\pmod n$
也就是说解密一定还原明文。

### 8.4 为什么安全（但注意 PPT 的一句“未证明”）

PPT 说得很实在：

- 如果分解 $n=pq$ 很容易，那破解 RSA 也容易。
- 反过来“分解难 $\Rightarrow$ 破解难”严格来说**未被数学证明**。
- 但几十年研究下来，**目前没有发现比分解 $n$ 更简单的通用破解方法**，所以现实中认为安全性可靠。

------

## 9. RSA Challenge Numbers：公开挑战“分解大整数”

PPT 列了一些 “RSA-576 / RSA-768 / RSA-1024 / RSA-2048 …” 这样的挑战数：

- 这些数是两个大素数相乘得到的
- 公开悬赏谁能把它分解回去
- 有的已经被分解（如 RSA-768），更大的（如 RSA-2048）仍未分解

它要表达的点是：**大数分解确实非常难，而且随着位数增长难度爆炸式上升。**

------

## 10. NP-Complete（NP 完全）：NP 里“最难的一批”

### 10.1 关系：$P\subseteq NP$

PPT 明确：所有 P 问题都属于 NP（因为能求解就当然能验证）。

### 10.2 最大悬案：$P \stackrel{?}{=} NP$

如果 $P=NP$，意味着：

- **所有能快速验证的题，都能快速求解**
  这会颠覆密码学、优化、AI 推理等大量领域。
  目前主流观点倾向：$P\neq NP$，但没人证明。

### 10.3 NP-Complete 的直观定义

PPT 的意思是：NP-Complete 是 NP 中“最难”的那一类问题：
一个判定问题 $C$ 是 NP 完全（NPC），当且仅当：

1. $C \in NP$（解可以多项式时间验证）
2. 所有 NP 完全问题都能多项式归约到 $C$（等价地说：$C$ 至少和 NP 里最难问题一样难）

它还强调一个重要结论：

- **只要任意一个 NP-Complete 被找到多项式算法，那么所有 NP-Complete 都有多项式算法**（因为可以互相归约）。

### 10.4 NP-hard（NP 困难）

PPT 区分：

- NP-Complete：既在 NP 中，又“最难”
- NP-hard：只要求“所有 NP 问题都能归约到它”，**不要求它在 NP**（所以范围更大）

------

## 11. SAT（布尔可满足性）：第一个被证明的 NP 完全问题

PPT 说 Cook 在 1971 年证明 SAT 是 NP-Complete，此后发现的 NPC 问题已非常多。

### 11.1 基本概念（布尔逻辑）

- 布尔变量：只能取 True/False（或 1/0），如 $p,q,r,s$
- 文字（literal）：变量或其否定，如 $p$、$\neg q$
- 子句（clause）：多个文字的“或”（析取）
  例如：$(p\vee q\vee s)$、$(\neg q\vee r)$

### 11.2 CNF 合取范式（Conjunctive Normal Form）

CNF 是多个子句的“与”（合取）：
$(p\vee q\vee s)\wedge(\neg q\vee r)\wedge(\neg p\vee r)\wedge(\neg r\vee s)$
SAT 问题就是：**是否存在一种真值赋值，使整个 CNF 公式为真**。

------

## 12. 解 SAT 的三种搜索方法（PPT 的重点）

### 方法 1：穷举搜索（Exhaustive Search）

如果有 $n$ 个变量，所有赋值一共 $2^n$ 种，只能全部试：

- 优点：一定正确、一定完整（不会漏解）
- 缺点：除了很小的 $n$，基本不可行

PPT 给了伪代码 ExhSAT：遍历每个候选解，找到就返回 Yes，否则 No。

------

### 方法 2：回溯搜索（Backtracking）

核心思想：

- 把“给变量赋值”的过程看成一棵搜索树
- 一路往下赋值，如果发现“走到死路”（出现空子句）就回退换分支
- 通常比纯穷举快很多，因为能提前剪枝

PPT 的 BtSAT 伪代码逻辑是：

1. 公式空了（都满足了）→ Yes
2. 出现空子句（矛盾）→ No
3. 选一个变量 $x$，先试 $x=1$，不行再试 $x=0$

------

### 方法 3：局部搜索（Local Search）+ GSAT

局部搜索的直觉：

- 先随机给一个赋值
- 然后每一步“翻转一个变量”，让**未满足子句数尽量减少**
- 这类方法通常很快，但**不保证一定找到解**（不完整）

PPT 给了 GSAT（类似 Las Vegas 算法）的框架：

- 外层重复多次随机初始解（Max-tries）
- 内层最多走 Max-moves 步
- 若满足就返回 Yes，否则翻转某变量让不满足子句数最小

------

## 13. 面对 NP-Complete，通常怎么“处理”？

PPT 结论是：到目前为止对 NPC 没有通用多项式算法，所以实践上常用：

- 特殊情况（对某些结构有限制时可能可解）
- 概率分析
- 近似算法
- 启发式算法（比如 GSAT 这类）

------

## 14. 章末“故事/人物”部分在讲什么？

最后几页是补充背景与计算机科学史：

- **冯·诺依曼（von Neumann）**与“存储程序计算机结构”（冯诺依曼结构）：
  四大部件：Memory、ALU、Control Unit、I/O；并强调“指令顺序执行”的思想。
- **图灵机（Turing Machine）**与图灵在二战破解 Enigma 的故事。
- **图灵奖（Turing Award）**：计算机界最高奖项之一，PPT提到奖金与历史，并列举了 Rivest、Knuth、Cook、姚期智等。

这些内容主要是帮助你把“P/NP/NPC”放进更大的计算机科学发展脉络里。

------

如果你愿意，我也可以把这章内容再进一步整理成：
1）**一页“考试背诵版”**（只留定义、关系、关键结论）；
2）**一套“例题讲解版”**（把 SAT、RSA、归约、P/NP/NPC 常见考法串起来）。
